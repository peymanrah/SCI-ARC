#!/usr/bin/env python3
"""
Test Entropy-Guided Refinement Improvement

This script tests whether the new entropy-guided refinement and progressive
residual decay actually improve accuracy per step compared to the baseline.

Tests:
- Small grids (3x3 -> 9x9)
- Medium grids (10x10 -> 20x20) 
- Large grids (20x20+)

Compares:
- use_entropy_refinement=True (NEW) vs False (OLD)
- Step-by-step accuracy progression
"""

import sys
import json
import torch
import torch.nn.functional as F
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from sci_arc.models.rlan import RLAN, RLANConfig


def classify_grid_size(shape):
    """Classify grid by size."""
    h, w = shape
    area = h * w
    if area <= 25:
        return "small"
    elif area <= 100:
        return "medium"
    else:
        return "large"


def pad_grid(grid, size=30):
    """Pad grid to fixed size."""
    grid = np.array(grid)
    h, w = grid.shape
    padded = np.zeros((size, size), dtype=np.int64)
    padded[:h, :w] = grid
    return padded


def run_inference(model, support_in, support_out, query, num_steps=6):
    """Run inference and return per-step predictions."""
    with torch.no_grad():
        outputs = model(
            query,
            train_inputs=support_in,
            train_outputs=support_out,
            return_intermediates=True,
            return_all_steps=True,
        )
        
        all_logits = outputs.get('all_logits', [outputs['logits']])
        centroids = outputs.get('centroids', torch.zeros(1, 7, 2))
        
    return all_logits, centroids


def compute_step_accuracies(all_logits, target):
    """Compute accuracy at each step."""
    th, tw = target.shape
    accuracies = []
    for logits in all_logits:
        pred = logits[0].argmax(dim=0)[:th, :tw].numpy()
        acc = (pred == target).mean()
        accuracies.append(acc)
    return accuracies


def main():
    print("=" * 70)
    print("ENTROPY-GUIDED REFINEMENT TEST")
    print("Testing if new changes improve accuracy per step")
    print("=" * 70)
    
    device = 'cpu'
    
    # Load checkpoint
    ckpt_path = project_root / "checkpoints" / "warmup3.pt"
    print(f"\nLoading checkpoint: {ckpt_path}")
    checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False)
    ckpt_config = checkpoint.get('config', {}).get('model', {})
    
    # Detect dsc_use_complexity_signals
    use_complexity_signals = ckpt_config.get('dsc_use_complexity_signals', None)
    if use_complexity_signals is None:
        stop_pred_key = 'dsc.stop_predictor.0.weight'
        if stop_pred_key in checkpoint['model_state_dict']:
            stop_pred_shape = checkpoint['model_state_dict'][stop_pred_key].shape
            hidden_dim = ckpt_config.get('hidden_dim', 256)
            expected_old = hidden_dim + 1 + hidden_dim
            expected_new = expected_old + 3
            use_complexity_signals = (stop_pred_shape[1] == expected_new)
        else:
            use_complexity_signals = False
    
    # Test configurations
    test_configs = [
        ("Baseline (no entropy refinement)", False),
        ("NEW: Entropy-Guided Refinement", True),
    ]
    
    # Test tasks by grid size
    arc_dir = project_root / "data" / "arc-agi" / "data" / "training"
    
    test_tasks = [
        # Small grids
        ("007bbfb7", "small", "3x3 -> 9x9 tiling"),
        ("0ca9ddb6", "small", "3x3 simple"),
        ("0d3d703e", "small", "3x3 transform"),
        # Medium grids  
        ("025d127b", "medium", "10x10 pattern"),
        ("045e512c", "medium", "9x9 pattern"),
        ("06df4c85", "medium", "11x11 pattern"),
        # Large grids
        ("00d62c1b", "large", "20x20 flood fill"),
        ("0520fde7", "large", "20x20 lines"),
        ("0b148d64", "large", "22x22 grid"),
    ]
    
    results = {size: {name: [] for name, _ in test_configs} for size in ["small", "medium", "large"]}
    
    for config_name, use_entropy in test_configs:
        print(f"\n{'='*60}")
        print(f"Testing: {config_name}")
        print("="*60)
        
        # Create config - note: we can't change use_entropy_refinement after loading
        # weights since entropy_proj and entropy_gate wouldn't have trained weights
        # So we'll test the SAME model but the NEW code path is active automatically
        config = RLANConfig(
            hidden_dim=ckpt_config.get('hidden_dim', 256),
            max_clues=ckpt_config.get('max_clues', 7),
            num_solver_steps=ckpt_config.get('num_solver_steps', 6),
            use_dsc=ckpt_config.get('use_dsc', True),
            use_msre=ckpt_config.get('use_msre', True),
            use_context_encoder=ckpt_config.get('use_context_encoder', True),
            use_hyperlora=ckpt_config.get('use_hyperlora', False),
            use_solver_context=ckpt_config.get('use_solver_context', True),
            dsc_use_complexity_signals=use_complexity_signals,
        )
        
        model = RLAN(config=config)
        
        # Manually set use_entropy_refinement for testing
        if hasattr(model, 'solver'):
            model.solver.use_entropy_refinement = use_entropy
            # Initialize entropy modules if not present
            if use_entropy and model.solver.entropy_proj is None:
                print("  [Note: entropy_proj not initialized in checkpoint - testing code path only]")
        
        model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        model.eval()
        
        for task_id, size_category, desc in test_tasks:
            task_path = arc_dir / f"{task_id}.json"
            if not task_path.exists():
                print(f"  [SKIP] {task_id} not found")
                continue
            
            with open(task_path) as f:
                task = json.load(f)
            
            train = task["train"]
            test = task["test"][0]
            target = np.array(test["output"])
            th, tw = target.shape
            
            # Prepare data
            support_in = torch.tensor(np.stack([pad_grid(ex["input"]) for ex in train] + 
                                               [np.zeros((30, 30))] * (5 - len(train)))).unsqueeze(0).long()
            support_out = torch.tensor(np.stack([pad_grid(ex["output"]) for ex in train] + 
                                                [np.zeros((30, 30))] * (5 - len(train)))).unsqueeze(0).long()
            query = torch.tensor(pad_grid(test["input"])).unsqueeze(0).long()
            
            # Run inference
            all_logits, centroids = run_inference(model, support_in, support_out, query)
            
            # Compute accuracies
            accuracies = compute_step_accuracies(all_logits, target)
            improvement = accuracies[-1] - accuracies[0]
            
            print(f"\n  {task_id} ({size_category}, {th}x{tw}): {desc}")
            print(f"    Steps: {' -> '.join([f'{a:.1%}' for a in accuracies])}")
            print(f"    Improvement: {improvement:+.1%}")
            
            results[size_category][config_name].append({
                'task_id': task_id,
                'accuracies': accuracies,
                'improvement': improvement,
                'grid_size': (th, tw),
            })
    
    # Summary and comparison
    print("\n" + "=" * 70)
    print("SUMMARY: COMPARISON BY GRID SIZE")
    print("=" * 70)
    
    output_dir = project_root / "scripts" / "outputs" / "deep_diagnosis"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    for size in ["small", "medium", "large"]:
        print(f"\n--- {size.upper()} GRIDS ---")
        
        for config_name, _ in test_configs:
            if not results[size][config_name]:
                continue
            
            task_results = results[size][config_name]
            improvements = [r['improvement'] for r in task_results]
            final_accs = [r['accuracies'][-1] for r in task_results]
            
            print(f"  {config_name}:")
            print(f"    Mean improvement: {np.mean(improvements)*100:+.1f}%")
            print(f"    Mean final acc: {np.mean(final_accs)*100:.1f}%")
    
    # Create visualization
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    colors = ['#e74c3c', '#27ae60']  # Red for baseline, green for new
    
    for idx, size in enumerate(["small", "medium", "large"]):
        ax = axes[idx]
        
        for config_idx, (config_name, _) in enumerate(test_configs):
            if not results[size][config_name]:
                continue
            
            # Average accuracy per step across tasks
            all_accs = [r['accuracies'] for r in results[size][config_name]]
            if all_accs:
                num_steps = len(all_accs[0])
                mean_accs = [np.mean([a[s] for a in all_accs]) * 100 for s in range(num_steps)]
                ax.plot(range(1, num_steps+1), mean_accs, 
                       marker='o', color=colors[config_idx], 
                       label=config_name.split(":")[0].strip(),
                       linewidth=2, markersize=8)
        
        ax.set_xlabel('Solver Step')
        ax.set_ylabel('Accuracy (%)')
        ax.set_title(f'{size.capitalize()} Grids')
        ax.grid(True, alpha=0.3)
        ax.legend(loc='lower right')
        ax.set_ylim([0, 100])
    
    plt.tight_layout()
    fig.savefig(output_dir / "entropy_refinement_comparison.png", dpi=150)
    print(f"\n[OK] Saved comparison plot to {output_dir / 'entropy_refinement_comparison.png'}")
    
    # Per-step analysis
    print("\n" + "=" * 70)
    print("PER-STEP ACCURACY BREAKDOWN")
    print("=" * 70)
    
    for size in ["small", "medium", "large"]:
        print(f"\n{size.upper()} GRIDS:")
        for config_name, _ in test_configs:
            if not results[size][config_name]:
                continue
            
            all_accs = [r['accuracies'] for r in results[size][config_name]]
            if all_accs:
                num_steps = len(all_accs[0])
                print(f"\n  {config_name}:")
                for step in range(num_steps):
                    step_accs = [a[step]*100 for a in all_accs]
                    print(f"    Step {step+1}: {np.mean(step_accs):.1f}% (Â±{np.std(step_accs):.1f})")
    
    # Key findings
    print("\n" + "=" * 70)
    print("KEY FINDINGS")
    print("=" * 70)
    
    # Check if entropy refinement helped
    baseline_improvements = []
    entropy_improvements = []
    
    for size in results:
        for r in results[size]["Baseline (no entropy refinement)"]:
            baseline_improvements.append(r['improvement'])
        for r in results[size]["NEW: Entropy-Guided Refinement"]:
            entropy_improvements.append(r['improvement'])
    
    if baseline_improvements and entropy_improvements:
        print(f"\nOverall Mean Improvement:")
        print(f"  Baseline: {np.mean(baseline_improvements)*100:+.1f}%")
        print(f"  Entropy:  {np.mean(entropy_improvements)*100:+.1f}%")
        
        if np.mean(entropy_improvements) > np.mean(baseline_improvements):
            print(f"\n[SUCCESS] Entropy refinement shows IMPROVEMENT!")
            print(f"   Gain: {(np.mean(entropy_improvements) - np.mean(baseline_improvements))*100:+.1f}%")
        else:
            print(f"\n[NOTE] Entropy refinement needs trained weights")
            print("   (entropy_proj and entropy_gate are randomly initialized)")
            print("   Re-train the model to see actual improvements.")
    
    print(f"\n{'='*70}")
    print("NOTE: Since checkpoint doesn't have trained entropy weights,")
    print("the comparison shows code path differences only.")
    print("Full improvement will be visible after training with new code.")
    print("="*70)


if __name__ == "__main__":
    main()
