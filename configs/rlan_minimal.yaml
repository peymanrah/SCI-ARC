# RLAN MINIMAL Configuration
# ===========================
# ABLATION-VALIDATED minimal config for stable training
# 
# VALIDATED ON ABLATION STUDY (Dec 2025):
# - Achieves 100% exact match on simple ARC tasks in 2-5 epochs
# - No NaN issues with proper padding (ignore_index=-100)
# - Core modules only: ContextEncoder + DSC + MSRE
#
# KEY INSIGHTS FROM ABLATION:
# 1. LCR/SPH are NOT needed for learning (can be disabled)
# 2. Simple optimizer (no scheduler) is more stable than OneCycleLR
# 3. weighted_stablemax with ignore_index=-100 prevents NaN
# 4. Per-module LR boosting (10x for DSC/MSRE) can cause instability
#
# PHILOSOPHY:
# - If the architecture is good, simple training should work
# - Complex training dynamics mask architectural issues
# - This config prioritizes stability over speed
#
# ===========================

model:
  type: "rlan"
  hidden_dim: 256
  num_colors: 10
  num_classes: 10  # colors 0-9 (no boundary markers needed)
  max_grid_size: 30
  
  max_clues: 6
  num_predicates: 32  # Increased for better feature diversity
  num_solver_steps: 6
  
  use_act: false       # DISABLED - adds complexity without benefit
  dropout: 0.1
  
  dsc_num_heads: 4
  lcr_num_heads: 4     # Not used (LCR disabled)
  
  msre_encoding_dim: 32
  msre_num_freq: 8
  lcr_num_freq: 8      # Not used (LCR disabled)
  
  # CORE MODULES ONLY (validated by ablation)
  use_context_encoder: true   # REQUIRED - provides task context
  use_dsc: true               # REQUIRED - dynamic spatial cluing
  use_msre: true              # REQUIRED - multi-scale reasoning
  use_lcr: false              # DISABLED - not needed for learning
  use_sph: false              # DISABLED - not needed for learning
  use_learned_pos: false      # DISABLED - Fourier encoding works better

training:
  max_epochs: 200  # Reduced - simple tasks converge in <50 epochs
  
  batch_size: 32   # Smaller batch for stability
  grad_accumulation_steps: 2
  
  learning_rate: 5.0e-4  # Higher LR works with stable optimizer
  weight_decay: 0.01     # Light regularization
  gradient_clip: 1.0     # Conservative clipping
  
  # ============================================================
  # MODULE-SPECIFIC LEARNING RATES
  # ============================================================
  # ABLATION FINDING: Per-module LR multipliers can cause NaN
  # when combined with schedulers. Disabled for stability.
  # The model still learns effectively with uniform LR.
  # ============================================================
  dsc_lr_multiplier: 1.0   # DISABLED - uniform LR is more stable
  msre_lr_multiplier: 1.0  # DISABLED - uniform LR is more stable
  
  # Temperature schedule for DSC attention
  # CRITICAL: Don't go below 0.5 - causes attention collapse
  temperature_start: 1.0
  temperature_end: 0.5
  
  # ============================================================
  # LOSS CONFIGURATION (Validated by Ablation)
  # ============================================================
  # ============================================================
  # LOSS CONFIGURATION - FOCAL WEIGHTED (RECOMMENDED)
  # ============================================================
  # FocalWeightedStablemaxLoss = our weights + focal modulation
  # Combines BG/FG gradient balance with dynamic hard-pixel focusing
  # ============================================================
  loss_mode: 'focal_weighted'  # RECOMMENDED: weights + focal modulation
  
  # BG/FG weight caps (validated by ablation)
  bg_weight_cap: 2.0               # BG weight (ensures ~52% gradient to BG)
  fg_weight_cap: 5.0               # Max FG weight (rarest class = 5.0)
  
  # Focal params
  focal_gamma: 2.0                 # Focusing parameter (higher = more focus on hard)
  focal_alpha: 0.75                # Only used for legacy focal modes
  
  # ============================================================
  # MINIMAL AUXILIARY LOSSES (Ablation Validated)
  # ============================================================
  # Most auxiliary losses are NOT needed for learning.
  # Only deep_supervision helps - regularization can hurt stability.
  # ============================================================
  lambda_entropy: 0.0           # OFF - not needed
  lambda_sparsity: 0.0          # OFF - can cause instability
  lambda_predicate: 0.0         # OFF - SPH disabled
  lambda_curriculum: 0.0        # OFF - no curriculum needed
  lambda_deep_supervision: 0.3  # ON - helps intermediate steps
  lambda_act: 0.0               # OFF - ACT disabled
  
  # Clue regularization (light touch)
  min_clues: 2.0                # Target at least 2 clues
  min_clue_weight: 1.0          # Light penalty (reduced from 5.0)
  ponder_weight: 0.01           # Small cost per clue
  entropy_ponder_weight: 0.0    # No entropy pondering
  
  use_stablemax: true
  
  use_curriculum: false
  curriculum_stages: []
  
  optimizer: "adamw"
  beta1: 0.9
  beta2: 0.95               # TRM uses 0.95 for stability
  
  # ============================================================
  # SCHEDULER (Ablation Finding: CosineAnnealing is more stable)
  # ============================================================
  # OneCycleLR with LR multipliers caused NaN in ablation tests.
  # Cosine annealing is more stable for this architecture.
  # ============================================================
  scheduler: "cosine"       # Changed from onecycle - more stable
  warmup_epochs: 10         # Reduced warmup
  min_lr: 1.0e-6
  
  use_ema: true
  ema_decay: 0.999          # Standard EMA decay

data:
  train_path: "./data/arc-agi/data/training"
  eval_path: "./data/arc-agi/data/evaluation"
  
  max_grid_size: 30
  
  # CRITICAL: Ignore padding in loss (ignore_index=-100)
  # Without this, padding pixels dominate loss and cause NaN
  ignore_padding_in_loss: true
  
  num_workers: 8            # Reduced for stability
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: true
  
  cache_samples: false
  
  augmentation:
    enabled: true
    rotation: true
    flip: true
    transpose: true
    color_permutation: true
    color_permutation_prob: 0.3  # 30% only - preserve color identity
    translational: true

evaluation:
  num_guesses: 2
  use_tta: true
  tta_rotations: [0, 90, 180, 270]
  tta_flips: [false, true]

logging:
  log_every: 1
  save_every: 25
  eval_every: 5
  keep_last_n: 5
  checkpoint_dir: "checkpoints/rlan_minimal"
  log_to_file: true
  track_augmentation: true
  
  use_wandb: false
  wandb_project: "rlan-arc"
  wandb_run_name: null

hardware:
  device: "cuda"
  seed: 42
  deterministic: false

device:
  use_cuda: true
  mixed_precision: true
  dtype: "bfloat16"
  compile: false
