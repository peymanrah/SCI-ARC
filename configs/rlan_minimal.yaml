# RLAN MINIMAL Configuration
# ===========================
# TRUE MINIMAL ablation - ONLY task loss, nothing else
# 
# This tests whether RLAN's ARCHITECTURAL innovations work
# without being confused by complex training dynamics.
#
# PHILOSOPHY:
# - If the architecture is good, simple training should work
# - If complex training is required, the architecture may be flawed
# - TRM achieves 50% with just cross-entropy loss
#
# KEY INSIGHT (from epochs 3-11 analysis):
# Even with padding ignored, class 0 (black) is ~4-8x more common
# than individual FG colors within actual grid content. Pure
# stablemax causes collapse to all-class-0 predictions.
#
# We use weighted_stablemax to give rare FG classes stronger
# gradients. This is still "minimal" (just task loss).
#
# ===========================

model:
  type: "rlan"
  hidden_dim: 256
  num_colors: 10
  num_classes: 11  # 0=boundary, 1-10=colors (TRM-style encoding)
  max_grid_size: 30
  
  max_clues: 6
  num_predicates: 8
  num_solver_steps: 6
  
  use_act: false       # DISABLED - removes ACT complexity
  dropout: 0.1
  
  dsc_num_heads: 4
  lcr_num_heads: 4
  
  msre_encoding_dim: 32
  msre_num_freq: 8
  lcr_num_freq: 8
  
  # CORE MODULES ONLY
  use_context_encoder: true   # Required for task signal
  use_dsc: true               # Core novelty
  use_msre: true              # Core novelty  
  use_lcr: false              # Disabled
  use_sph: false              # Disabled
  use_learned_pos: false

training:
  max_epochs: 500
  
  batch_size: 64
  grad_accumulation_steps: 4
  
  learning_rate: 1.0e-4
  weight_decay: 0.1
  gradient_clip: 5.0
  
  # ============================================================
  # MODULE-SPECIFIC LEARNING RATES (Gradient Compensation)
  # ============================================================
  # CRITICAL INSIGHT (from gradient analysis, epochs 21-32):
  # DSC gradient norm: ~0.01-0.02
  # Solver gradient norm: ~0.8-1.4
  # This 60-100x difference means DSC learns 60-100x slower!
  # 
  # Root cause: Long gradient path + coordinate normalization + Fourier encoding
  # Solution: Higher LR for DSC/MSRE to compensate for gradient dilution
  # ============================================================
  dsc_lr_multiplier: 10.0   # DSC needs 10x LR due to gradient dilution
  msre_lr_multiplier: 10.0  # MSRE also needs higher LR
  
  # Temperature schedule (still needed for DSC attention)
  temperature_start: 1.0
  temperature_end: 0.1
  
  # ============================================================
  # MINIMAL LOSS: Task loss with class weighting
  # ============================================================
  # Even with padding ignored, class 0 (black) is ~4-8x more
  # common than individual FG colors in actual grid content.
  # 
  # Target distribution example: [7.8%, 1.4%, 1.2%, 1.0%, ...]
  # Class 0 dominates, causing mode collapse with pure stablemax.
  #
  # weighted_stablemax uses inverse frequency weighting:
  # - Rare FG colors get stronger gradients
  # - Still "minimal" (just task loss, no auxiliary losses)
  # ============================================================
  loss_mode: 'weighted_stablemax'  # Inverse frequency weighting
  
  # BG/FG weight caps (tune for ARC class distribution)
  bg_weight_cap: 1.0               # Cap BG weight (it's still common)
  fg_weight_cap: 10.0              # Allow strong FG gradients
  
  # Focal loss params (not used with weighted_stablemax)
  focal_gamma: 2.0
  focal_alpha: 0.75
  
  # ============================================================
  # DISABLE ALL AUXILIARY LOSSES
  # ============================================================
  lambda_entropy: 0.0           # OFF - let attention learn naturally
  lambda_sparsity: 0.0          # OFF - let clue count emerge naturally
  lambda_predicate: 0.0         # OFF - no SPH
  lambda_curriculum: 0.0        # OFF - no curriculum
  lambda_deep_supervision: 0.0  # OFF - only final step matters
  lambda_act: 0.0               # OFF - no ACT
  
  # Clue regularization (all OFF)
  min_clues: 0.0                # No minimum
  min_clue_weight: 0.0          # No penalty
  ponder_weight: 0.0            # No pondering cost
  entropy_ponder_weight: 0.0    # No entropy pondering
  
  use_stablemax: true
  
  use_curriculum: false
  curriculum_stages: []
  
  optimizer: "adamw"
  beta1: 0.9
  beta2: 0.95
  scheduler: "onecycle"
  warmup_epochs: 20
  min_lr: 1.0e-6
  
  use_ema: true
  ema_decay: 0.99  # Reduced from 0.995 - EMA was lagging too much behind training

data:
  train_path: "./data/arc-agi/data/training"
  eval_path: "./data/arc-agi/data/evaluation"
  
  max_grid_size: 30
  
  # CRITICAL: Ignore padding in loss (like TRM)
  # Without this, padding pixels (93% of grid) dominate the loss
  # and cause mode collapse to background-only predictions
  ignore_padding_in_loss: true
  
  # TRM-STYLE ENCODING (prevents mode collapse)
  # 1. Shift colors by +1: colors 0-9 → classes 1-10
  # 2. Add boundary markers (class 0) at grid edges
  # This dilutes dominant class (color 0) from 60% → ~43%
  use_trm_encoding: true
  
  num_workers: 24
  pin_memory: true
  prefetch_factor: 4
  persistent_workers: true
  
  cache_samples: false
  
  augmentation:
    enabled: true
    rotation: true
    flip: true
    transpose: true
    color_permutation: true
    color_permutation_prob: 0.3  # 30% only - preserve color identity
    translational: true

evaluation:
  num_guesses: 2
  use_tta: true
  tta_rotations: [0, 90, 180, 270]
  tta_flips: [false, true]

logging:
  log_every: 1
  save_every: 25
  eval_every: 5
  keep_last_n: 5
  checkpoint_dir: "checkpoints/rlan_minimal"
  log_to_file: true
  track_augmentation: true
  
  use_wandb: false
  wandb_project: "rlan-arc"
  wandb_run_name: null

hardware:
  device: "cuda"
  seed: 42
  deterministic: false

device:
  use_cuda: true
  mixed_precision: true
  dtype: "bfloat16"
  compile: false
