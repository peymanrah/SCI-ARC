# RLAN STABLE DEV 512 Configuration
# ===================================
# Fresh training configuration with 512 hidden dimension
# Based on rlan_stable_dev.yaml with adjustments for wider network
#
# CREATED: January 2026
#
# WHY 512 HIDDEN DIM:
# - 2× capacity for richer representations
# - Better expressiveness for complex ARC transformations
# - Still fits in 24GB VRAM with adjusted batch size
#
# ARCHITECTURE NOTES (Jan 2026):
# ==============================
# RLAN is NOT a deep stacked-layer model like GPT/BERT. It uses:
# 1. GridEncoder: Single embed+proj+norm layer (NOT multi-layer transformer)
# 2. ContextEncoder: Cross-attention with pooling (single layer)
# 3. RecursiveSolver: ConvGRU with num_solver_steps iterations (7 by default)
# 4. DSC: Single GRUCell with iterative clue discovery
#
# The "depth" in RLAN comes from SOLVER ITERATIONS, not stacked layers.
# For 512 hidden dim:
# - num_solver_steps=7 is still appropriate (same complexity of reasoning)
# - No need to add more "layers" because architecture is iterative, not deep
# - Wider hidden dim → richer per-step representations
#
# If more capacity is needed later, consider:
# - Increase num_solver_steps (9 or 10) for more iterations
# - Increase max_clues for more spatial anchors
# - These are the "depth" knobs in RLAN, not traditional layer count
#
# CHANGES FROM 256 CONFIG:
# ========================
# 1. hidden_dim: 256 → 512 (2× wider)
# 2. batch_size: 50 → 28 (VRAM constraint with 2× params)
# 3. grad_accumulation_steps: 7 → 12 (maintain ~336 effective batch)
# 4. learning_rate: 5e-4 → 3e-4 (wider nets need lower LR)
# 5. hyperlora_rank: 8 → 16 (scale with hidden dim for expressiveness)
# 6. Attention heads: 4 → 8 (keep head_dim=64 consistent with 256 config)
#
# MEMORY ESTIMATE:
# - 256 hidden: ~20M params, ~14GB peak VRAM
# - 512 hidden: ~80M params, ~22GB peak VRAM (fits in 24GB)
#
# HARDWARE TARGET: RTX 3090 (24GB VRAM), 48 virtual CPUs, 128GB RAM
# ==========================

model:
  type: "rlan"
  hidden_dim: 512           # CHANGED: 2× wider (was 256)
  num_colors: 10
  num_classes: 10  # colors 0-9
  max_grid_size: 30
  
  max_clues: 7              # Same - task complexity, not model width
  num_predicates: 32        # Same - semantic concepts
  num_solver_steps: 7       # Same - iteration count (this is RLAN's "depth")
  
  use_act: false            # DISABLED
  dropout: 0.1              # Same - regularization ratio
  
  # =============================================================
  # BEST-STEP SELECTION (Dec 2025)
  # =============================================================
  use_best_step_selection: false
  
  dsc_num_heads: 8          # CHANGED: 8 heads × 64 dim = 512 (was 4 × 64 = 256)
  lcr_num_heads: 8          # CHANGED: Match DSC (was 4)
  
  msre_encoding_dim: 32     # Same - frequency encoding (separate system)
  msre_num_freq: 8          # Same
  lcr_num_freq: 8           # Same
  
  # CORE MODULES ONLY (ablation validated)
  use_context_encoder: true   # REQUIRED - learns task from demos
  use_dsc: true               # REQUIRED - dynamic spatial cluing
  use_msre: true              # REQUIRED - multi-scale relative encoding
  use_lcr: false              # DISABLED - not needed for learning
  
  # Context injection mode
  use_cross_attention_context: true
  spatial_downsample: 8       # Same - spatial resolution target
  use_sph: false              # DISABLED
  use_learned_pos: false      # DISABLED - sinusoidal works better
  
  # Phase 2.5: Solver Cross-Attention to Support Set (Dec 2025)
  use_solver_context: true
  solver_context_heads: 8     # CHANGED: 8 heads for 512 dim (was 4)
  
  # =============================================================
  # HYPER-LORA: META-LEARNING WEIGHT ADAPTATION
  # =============================================================
  use_hyperlora: true
  hyperlora_rank: 16          # CHANGED: 2× rank for 2× hidden (was 8)
  hyperlora_scaling: 1.0
  hyperlora_dropout: 0.0
  hyperlora_init_scale: 0.005
  hyperlora_max_norm: 1.0

  # =============================================================
  # HIERARCHICAL PRIMITIVE MEMORY (HPM v2)
  # =============================================================
  use_hpm: true
  hpm_start_epoch: 20
  
  # Sparse MoE Routing Configuration
  hpm_top_k: 2
  hpm_balance_weight: 0.01
  
  # Static Bank Configuration
  hpm_primitives_per_bank: 32   # Same - number of primitives (embeddings auto-resize)
  hpm_levels_per_bank: 3
  hpm_use_cross_attention: true
  
  # Dynamic Bank Configuration
  hpm_memory_size: 10000
  hpm_retrieval_k: 5
  
  # Bank Selection
  hpm_use_compositional_bank: true
  hpm_use_pattern_bank: true
  hpm_use_relational_bank: true
  hpm_use_concept_bank: false
  hpm_use_procedural_bank: true
  hpm_use_instance_bank: true
  
  # HPM Buffer Storage
  hpm_buffer_path: "checkpoints/rlan_stable_512/hpm"  # Updated path for 512
  hpm_buffer_save_frequency: 1
  hpm_buffer_stale_days: 7
  hpm_buffer_auto_load: true
  
  # HPM Memory Collection
  hpm_memory_start_epoch: 0
  
  # HPM Solver-Context Coupling
  hpm_solver_context_enabled: true
  hpm_solver_context_start_epoch: 80
  hpm_solver_context_gate_init: 0.0
  hpm_solver_context_gate_warmup_epochs: 20
  hpm_solver_context_max_tokens: 4
  hpm_solver_context_gate_max: 0.3
  hpm_solver_context_logit_clamp: 5.0
  hpm_solver_context_disable_on_instability: true

training:
  max_epochs: 200
  
  # =============================================================
  # MEMORY PLANNING FOR 512 HIDDEN DIM (Jan 2026)
  # =============================================================
  # Parameter scaling: ~4× more params (256² → 512² = 4× in linear layers)
  # Memory scaling: ~2× VRAM per batch item (params + activations)
  #
  # 256 config: batch=50, peak ~14GB
  # 512 config: batch=28, peak ~22GB (targeting 24GB GPU)
  #
  # Effective batch unchanged: 28 × 12 = 336 ≈ 50 × 7 = 350
  # =============================================================
  use_8bit_optimizer: true
  gradient_checkpointing: false  # DISABLED: Known issues with Dict/None args
  use_torch_compile: false
  torch_compile_mode: 'reduce-overhead'
  
  batch_size: 28              # CHANGED: Reduced for 2× params (was 50)
  grad_accumulation_steps: 12 # CHANGED: Increased to maintain effective batch (was 7)
  
  learning_rate: 3.0e-4       # CHANGED: Lower for wider net (was 5e-4)
  weight_decay: 0.01
  gradient_clip: 1.0
  
  # Per-module LR boosting
  dsc_lr_multiplier: 1.0
  msre_lr_multiplier: 1.0
  hyperlora_lr_multiplier: 1.0
  
  # Temperature for DSC attention
  temperature_start: 1.0
  temperature_end: 0.5
  
  # Loss Configuration
  loss_mode: 'focal_weighted'
  bg_weight_cap: 1.0
  fg_weight_cap: 6.0
  focal_gamma: 1.2
  focal_alpha: 0.75
  
  # AUXILIARY LOSSES
  lambda_entropy: 0.01
  lambda_sparsity: 0.5
  lambda_predicate: 0.01
  lambda_curriculum: 0.0
  lambda_deep_supervision: 0.0
  lambda_act: 0.0
  lambda_centroid_diversity: 0.1
  
  # Clue regularization
  min_clues: 1.0
  min_clue_weight: 5.0
  ponder_weight: 0.02
  entropy_ponder_weight: 0.02
  
  use_stablemax: true
  use_curriculum: false
  curriculum_stages: []
  
  optimizer: "adamw"
  beta1: 0.9
  beta2: 0.95
  
  # NO SCHEDULER - constant LR (most stable)
  scheduler: "none"
  warmup_epochs: 0
  min_lr: 3.0e-4  # Match learning_rate
  
  # EMA Configuration
  use_ema: false
  ema_decay: 0.995
  
  # =============================================================
  # STAGED META-LEARNING (Same schedule as 256)
  # =============================================================
  solver_context_start_epoch: 5
  cross_attention_start_epoch: 12
  meta_learning_start_epoch: 8
  
  hyperlora_warmup_epochs: 8
  hyperlora_warmup_start_scale: 0.005
  hyperlora_warmup_end_scale: 0.05
  
  activation_lr_reduction: 0.5
  activation_lr_recovery_epochs: 2
  
  grad_explosion_threshold: 10.0
  grad_explosion_lr_reduction: 0.5
  grad_explosion_cooldown_epochs: 2
  
  # LOO Training
  # NOTE (Jan 2026): Increased divisor from 2.0 to 2.5 for 512 hidden dim.
  # LOO does N forward passes per sample, and 512 activations are 2× heavier.
  # With batch=28 and divisor=2.5: LOO batch ≈ 11, with grad_accum auto-adjusted.
  loo_training:
    enabled: true
    start_epoch: 24
    loss_weight: 0.05
    min_pairs_for_loo: 2
    max_loo_pairs: 4
    batch_reduction_divisor: 2.5    # CHANGED: 2.0 → 2.5 for 512 memory safety
    min_batch_size: 8
    adjust_grad_accumulation: true
  
  # Equivariance Training (DISABLED)
  equivariance_training:
    enabled: false
    start_epoch: 16
    loss_weight: 0.01
    num_augmentations: 4
  
  # Meta Escalation
  meta_escalation:
    enabled: true
    start_epoch: 25
    ramp_epochs: 12
    schedule: linear
    targets:
      hyperlora_delta_scale: 0.20
      equiv_loss_weight: 0.03
      loo_loss_weight: 0.08
    require_stability: true
    max_grad_explosion_events_per_epoch: 2
    max_lr_backoff_events_per_epoch: 2
    max_consecutive_nan_streak_per_epoch: 3
    recovery_enabled: true
    recovery_step_per_window: 0.05
    log_every_epoch: true
    
    late_phase:
      enabled: true
      start_epoch: 50
      max_grad_explosion_events: 0
      max_lr_backoff_events: 0
      max_attention_collapse_events: 0
      decay_targets: false
      target_decay_factor: 0.8
      lr_decay:
        enabled: true
        start_epoch: 50
        end_epoch: 200
        decay_factor: 0.1
        schedule: cosine

data:
  train_path: "./data/arc-agi/data/training"
  eval_path: "./data/arc-agi/data/evaluation"
  
  max_grid_size: 30
  ignore_padding_in_loss: true
  
  # Use all 400 tasks with 50 samples each
  max_tasks: null
  samples_per_task: 50
  stratified_seed: 42
  
  num_workers: 12
  pin_memory: true
  prefetch_factor: 6
  persistent_workers: true
  
  cache_samples: true
  num_cached_samples: 20000
  cache_path: "./cache/rlan_stable_20k_400tasks.pkl"  # Same cache (data unchanged)
  cache_load_percent: 100
  
  bucketed_batching: true
  bucket_boundaries: [10, 15, 20, 25]
  
  augmentation:
    enabled: true
    rotation: true
    flip: true
    transpose: true
    color_permutation: true
    color_permutation_prob: 0.5
    translational: true
    track_augmentation: true

evaluation:
  num_guesses: 2
  use_tta: true
  tta_rotations: [0, 90, 180, 270]
  tta_flips: [false, true]
  
  use_trm_style_eval: true
  num_augmented_views: 8
  num_color_perms: 4
  use_voting: true
  use_inverse_aug: true
  pass_ks: [1, 2, 3]
  max_eval_tasks: 100

monitoring:
  enabled: true
  exact_match_warning: 0.10
  exact_match_critical: 0.20
  entropy_ratio_warning: 2.0
  entropy_ratio_critical: 5.0
  stop_value_warning: 0.15
  stop_value_critical: 0.25
  lora_norm_warn: 2.0
  lora_norm_critical: 5.0
  lora_norm_kill: 10.0
  nan_batches_abort: 20
  tta_consensus_warning: 0.25
  tta_consensus_critical: 0.15
  centroid_spread_warning: 2.0
  centroid_spread_critical: 0.5
  attn_max_collapse_threshold: 0.02
  attention_collapse_consecutive_threshold: 2
  
  collapse_backoff:
    enabled: true
    cooldown_epochs: 3
    delta_scale_factor: 0.5
    lr_factor: 0.5
    restore_rate: 0.2

logging:
  log_every: 1
  save_every: 10
  eval_every: 10
  keep_last_n: 3
  checkpoint_dir: "checkpoints/rlan_stable_512"  # Updated path for 512
  log_to_file: true
  track_augmentation: true
  memory_debug_batches: 5
  
  use_wandb: false
  wandb_project: "rlan-arc"
  wandb_run_name: null

inference:
  temperature: 0.1
  use_best_step_selection: false
  num_steps_override: null
  
  use_tta: true
  num_dihedral: 8
  num_color_perms: 4
  batch_size: 32
  
  loo_sanity_check: true
  loo_threshold: 0.9
  
  acw_enabled: true
  acw_num_views: 8

  meta_learning:
    hyperlora:
      enable: true
      require_trained: true
      fallback_on_failure: true
    hpm:
      enable: true
      require_nonempty_buffers: true
      min_buffer_entries: 1
      use_static_banks: true
      use_dynamic_banks: true
    solver_context:
      enable: true
    cross_attention:
      enable: true

hardware:
  device: "cuda"
  seed: 42
  deterministic: false
  check_nan_inf: true

device:
  use_cuda: true
  mixed_precision: true
  dtype: "bfloat16"
  compile: false
