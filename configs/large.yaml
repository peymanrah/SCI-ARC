# SCI-ARC Large Model Configuration
# ===========================================================================
# PURPOSE: MAXIMUM CAPACITY exploration (research/ablation)
# Use Case: Testing if more capacity helps, ablation studies
# Caution: May overfit on small ARC training set
#
# Parameters: ~15M (2x TRM)
# Training Time: ~48-72 hours
# ===========================================================================

model:
  hidden_dim: 384
  num_colors: 10
  max_grid_size: 30
  
  num_structure_slots: 16
  se_layers: 4                  # Transformer encoder layers
  structure_heads: 12
  use_abstraction: true
  
  max_objects: 24
  content_heads: 12
  
  binding_heads: 12
  
  H_cycles: 4
  L_cycles: 6
  L_layers: 3
  
  dropout: 0.1

training:
  learning_rate: 2.0e-4
  weight_decay: 0.02
  max_epochs: 150
  warmup_epochs: 10
  grad_clip: 1.0
  grad_accumulation_steps: 1
  
  # Batch size for RTX 3090 24GB
  # Large model uses more memory, so we reduce batch size
  batch_size: 128               # Reduced from 192 for larger model
  eval_batch_size: 128
  
  scl_weight: 1.0               # SCL weight (BatchNorm fix makes SCL work)
  ortho_weight: 0.01
  deep_supervision_weight: 0.5
  
  # CISL: Content-Invariant Structure Learning (replaces SCL for few-shot)
  # Note: Config params use 'cicl_' prefix for backward compatibility
  use_cicl: true                # Enable CISL instead of legacy SCL
  cicl_consist_weight: 0.5      # Within-task consistency weight (λ₁)
  cicl_color_inv_weight: 0.5    # Content invariance weight (λ₂)
  cicl_variance_weight: 0.1     # Batch variance weight (λ₃, anti-collapse)
  cicl_target_std: 0.5          # Target std (γ) for variance regularization
  
  # Anti-background-collapse (CRITICAL for ARC's ~85% background grids)
  focal_gamma: 2.0              # Focal Loss gamma (down-weights easy/background pixels)
  use_class_weights: true       # Background weight=0.1, others=1.0
  label_smoothing: 0.1          # Prevents overconfident predictions
  
  scheduler_type: cosine
  min_lr: 1.0e-6
  
  use_amp: true
  use_curriculum: true
  curriculum_stages:
    - 20
    - 60
    - 100

data:
  # Dataset paths
  arc_dir: ./data/arc-agi/data  # Contains training/ and evaluation/ subdirectories
  rearc_dir: null               # Set to ./data/rearc if available
  
  # Data loading
  num_workers: 24
  pin_memory: true
  
  # LARGE MODEL: Use infinite data to prevent overfitting
  cache_samples: false          # Infinite data generation
  cache_augmentations: 8        # Ignored when cache_samples=false
  
  augment: true
  
  # SCL Transform Family Assignment
  use_augment_family: true      # Use augmentation type as transform_family for SCL

evaluation:
  num_attempts: 2
  use_voting: true
  temperature: 0.8              # Slightly lower for more confident predictions

logging:
  log_every: 5
  eval_every: 1
  save_every: 5
  keep_last_n: 5
  log_to_file: true             # Save all terminal output to log file
  use_wandb: true
  wandb_project: sci-arc-large
  checkpoint_dir: ./checkpoints/large

hardware:
  device: cuda
  seed: 42
