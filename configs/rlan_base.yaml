# RLAN Base Configuration - Production Competitive Training
# Optimized for RTX 3090 (24GB VRAM), 48 vCPU, 128GB RAM
# Memory target: <22GB VRAM (no shared memory spillover)
#
# Key Design Decisions (aligned with ARC-AGI competition requirements):
# - Long training (1000 epochs) with infinite augmentation diversity
# - Large effective batch via gradient accumulation
# - AdamW + OneCycleLR (proven effective for ARC tasks)
# - Full 8 dihedral + color permutation augmentation
# - Multi-worker data loading for CPU utilization

model:
  type: "rlan"
  hidden_dim: 256  # ~12M params with ContextEncoder
  num_colors: 10
  num_classes: 11  # 10 colors + background
  max_grid_size: 30
  max_clues: 5
  num_predicates: 8
  num_solver_steps: 6
  use_act: true  # Enable Adaptive Computation Time
  dropout: 0.1
  
  # DSC parameters
  dsc_num_heads: 8
  
  # MSRE parameters
  msre_encoding_dim: 64
  msre_num_freq: 8
  
  # LCR parameters
  lcr_num_freq: 8
  lcr_num_heads: 8

training:
  # Long training for ARC (TRM trains for 5000+ epochs on similar data)
  max_epochs: 1000
  
  # =============================================================
  # BATCH SIZE STRATEGY FOR SINGLE RTX 3090 (24GB VRAM)
  # =============================================================
  # TRM: global_batch=768 across 8 A100 GPUs
  # RLAN target: effective_batch=384 (50% of TRM)
  # 
  # hidden_dim=256 uses ~200MB/sample with AMP
  # Max safe batch: 96 samples (~19GB VRAM)
  # grad_accumulation=4 → effective_batch = 96 × 4 = 384
  # =============================================================
  batch_size: 96
  grad_accumulation_steps: 4
  # effective_batch_size = 96 × 4 = 384
  
  learning_rate: 3.0e-4  # Higher LR for OneCycle (peak LR)
  weight_decay: 0.1      # TRM uses 0.1, not 1e-5
  gradient_clip: 1.0
  
  # Temperature schedule (slower annealing for longer training)
  temperature_start: 5.0
  temperature_end: 0.1
  
  # Loss weights - focal loss dominates
  focal_gamma: 2.0
  focal_alpha: 0.25
  lambda_entropy: 0.01
  lambda_sparsity: 0.005
  lambda_predicate: 0.001
  lambda_curriculum: 0.01
  lambda_deep_supervision: 0.3
  
  # Curriculum phases (scaled for 1000 epochs)
  pretrain_epochs: 100
  finetune_start_epoch: 800
  
  # Optimizer: AdamW is standard and works well
  # Scheduler: OneCycle gives faster convergence than cosine
  optimizer: "adamw"
  scheduler: "onecycle"  # onecycle > cosine for ARC
  warmup_epochs: 20      # 2% warmup
  min_lr: 1.0e-6
  
  # EMA for stable evaluation
  use_ema: true
  ema_decay: 0.999

data:
  # Data paths
  train_path: "./data/arc-agi/data/training"
  eval_path: "./data/arc-agi/data/evaluation"
  
  max_grid_size: 30
  
  # Data loading optimization for 48 vCPU server
  # Use 12 workers (25% of vCPU) for optimal GPU feeding
  # More workers = more parallel augmentation = infinite diversity
  num_workers: 12
  pin_memory: true
  prefetch_factor: 4  # How many batches each worker prefetches
  persistent_workers: true  # Keep workers alive between epochs
  
  # INFINITE DIVERSITY MODE (CRITICAL for ARC generalization!)
  # cache_samples=false means every __getitem__ call generates NEW augmentation
  # With 400 tasks × 8 dihedral × 362880 color perms = billions of unique samples
  cache_samples: false
  
  # Full augmentation pipeline (matching TRM)
  augmentation:
    enabled: true
    rotation: true           # 4 rotations (0°, 90°, 180°, 270°)
    flip: true               # 2 flips (horizontal, vertical)
    transpose: true          # 2 transposes (main diagonal, anti-diagonal)
    color_permutation: true  # 9! = 362,880 color permutations
    # Total: 8 dihedral × 362,880 color = 2,903,040 augmentations per task!

evaluation:
  num_guesses: 2
  use_tta: true
  tta_rotations: [0, 90, 180, 270]
  tta_flips: [false, true]

logging:
  log_every: 1  # Log EVERY batch for visibility (only ~3-4 batches per epoch)
  save_every: 25  # Save checkpoint every 25 epochs (less disk I/O)
  eval_every: 5   # Evaluate every 5 epochs (faster training)
  keep_last_n: 5  # Keep last 5 checkpoints
  checkpoint_dir: "checkpoints/rlan_base"
  log_to_file: true
  
  # WandB (optional)
  use_wandb: false
  wandb_project: "rlan-arc"
  wandb_run_name: null

# Hardware settings
hardware:
  device: "cuda"
  seed: 42
  deterministic: false

device:
  use_cuda: true
  mixed_precision: true   # AMP for memory efficiency
  dtype: "bfloat16"       # bfloat16 more stable than float16 (TRM uses this)
  compile: true            # torch.compile for 20-40% speedup
