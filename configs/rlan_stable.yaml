# RLAN STABLE Configuration
# ==========================
# Production-ready configuration validated by ablation testing
# 
# HARDWARE TARGET: RTX 3090 (24GB VRAM), 48 virtual CPUs, 128GB RAM
#
# USE THIS CONFIG WHEN:
# - Training new models from scratch
# - Production training runs
# - Establishing baseline performance
#
# ABLATION VALIDATION (Dec 2025):
# - Achieves 100% exact match on simple ARC tasks in 2-5 epochs
# - Grid expansion tasks (3x3→9x9) reach 100% in ~130 epochs
# - Clue regularization validated: faster convergence, stable training
# - No NaN issues with proper padding (ignore_index=-100)
#
# CONTEXT INJECTION (Dec 2025):
# - use_cross_attention_context=true with spatial_downsample=8
# - Preserves spatial structure for generalization (unlike FiLM compression)
# - Adaptive pooling: all grids → 8×8 + learned refinement + position re-encoding
# - Memory efficient: 64 tokens/pair vs 900 (14x reduction)
#
# DATA LOADING (Dec 2025):
# - 400K cached samples in CPU RAM (not GPU VRAM!)
# - num_workers=0 when cached (avoids memory duplication)
# - CUDAPrefetcher for async CPU→GPU transfer (no GPU stalls)
#
# SAMPLE COUNT (TRM comparison):
# - TRM: 1000x pre-generated augmentation = 800,000 samples
# - RLAN Phase 1: 400,000 cached (1000 per task)
# - RLAN Phase 2: INFINITE on-the-fly augmentation
#
# KEY PRINCIPLES:
# 1. No scheduler - constant learning rate (most stable)
# 2. Clue regularization ENABLED (prevents clue collapse)
# 3. Core modules: ContextEncoder (CrossAttention+Pooling), DSC, MSRE
# 4. Light auxiliary losses (entropy, sparsity, predicate)
# 5. max_grid_size=30 (official ARC-AGI maximum)
#
# TRAINING WORKFLOW:
# Phase 1: Train with cache_samples=true until convergence (memorization)
# Phase 2: Resume with cache_samples=false for diversity (generalization)
#
# ⚠️ CACHE NOTE: DELETE CACHE after augmentation order fix (Dec 2025)!
# Previous cache had wrong order (dihedral→color), now using TRM order (color→dihedral)
#   rm ./cache/rlan_stable_400k_v2.pkl
#   rm ./cache/rlan_stable_400k_v3.pkl  # v3 with correct TRM-style order
#
# ==========================

model:
  type: "rlan"
  hidden_dim: 256
  num_colors: 10
  num_classes: 10  # colors 0-9
  max_grid_size: 30
  
  max_clues: 7
  num_predicates: 32  # More predicates for diversity
  num_solver_steps: 6   # Reverted from 10 - 6 is stable and fast
  
  use_act: false       # DISABLED
  dropout: 0.1
  
  # =============================================================
  # BEST-STEP SELECTION (Dec 2025)
  # =============================================================
  # When enabled, selects the best prediction from all solver steps:
  # - Training: Uses lowest loss (ground truth available)
  # - Inference: Uses lowest entropy (confidence-based)
  # 
  # This prevents over-iteration from hurting accuracy. If step 3
  # is better than step 6 for a simple task, we use step 3.
  # 
  # Recommended workflow:
  # 1. Train with use_best_step_selection: false (default)
  # 2. At inference, set use_best_step_selection: true to use entropy
  # 3. The model doesn't need retraining - this is inference-only
  #
  # Can also enable during training to track best-step histograms.
  use_best_step_selection: false
  
  dsc_num_heads: 4
  lcr_num_heads: 4     # Not used
  
  msre_encoding_dim: 32
  msre_num_freq: 8
  lcr_num_freq: 8      # Not used
  
  # CORE MODULES ONLY (ablation validated)
  use_context_encoder: true   # REQUIRED - learns task from demos
  use_dsc: true               # REQUIRED - dynamic spatial cluing
  use_msre: true              # REQUIRED - multi-scale relative encoding
  use_lcr: false              # DISABLED - not needed for learning
  
  # Context injection mode (CRITICAL FOR STABILITY!)
  # false = FiLM conditioning (stable, proven, compresses context to vector)
  # true = CrossAttention (experimental, high memory, can cause mode collapse)
  #
  # MIDDLE GROUND: Enable CrossAttention but with spatial downsampling
  # This gives spatial context without the massive memory cost
  use_cross_attention_context: true
  
  # Spatial downsample TARGET SIZE (not divisor!)
  # All support grids (3x3 to 30x30) are pooled to this fixed size
  # 8 = 8x8 output (64 tokens per pair, ~14x reduction from 30x30)
  # Uses learned refinement + position re-encoding after pooling
  spatial_downsample: 8       # 8x8 target - preserves objects while being memory-efficient
  use_sph: false              # DISABLED - not needed for learning
  use_learned_pos: false      # DISABLED - sinusoidal works better
  
  # Phase 2.5: Solver Cross-Attention to Support Set (Dec 2025)
  # ENABLED: Critical for generalization to unseen ARC rules
  # 
  # WHY THIS IS NEEDED:
  # - DSC injection provides GLOBAL context (baked in once at start)
  # - Solver cross-attention provides LOCAL verification at EVERY step
  # - These are COMPLEMENTARY: spatial priors + iterative verification
  #
  # MEMORY COST: ~100MB additional (trivial on 24GB GPU)
  # - Support: (B, N, 256, 8, 8) computed ONCE, reused across 6 steps
  # - Attention: O(900 × 256) per step = ~230K ops = <1ms/step
  #
  # PREVIOUS ISSUE: Mode collapse was caused by hardcoded CrossAttentionInjector
  # not by solver cross-attention. Now with spatial_downsample=8, memory is fine.
  use_solver_context: true    # ENABLED - critical for iterative verification
  solver_context_heads: 4     # Multi-head attention for diverse queries
  
  # =============================================================
  # HYPER-LORA: META-LEARNING WEIGHT ADAPTATION (Dec 2025)
  # =============================================================
  # HyperLoRA predicts task-specific LoRA weight deltas from context.
  # This allows the model to adapt its GRU gates and output head to
  # each specific task, similar to test-time training (TTT) but
  # without gradient updates at inference time.
  #
  # ARCHITECTURE:
  # - Pools context from encoded support features
  # - Predicts low-rank weight deltas (A, B matrices) for:
  #   - GRU reset gate, update gate, candidate gate
  #   - Output head (logits projection)
  # - Applied via additive LoRA: W' = W + A @ B.T
  #
  # TRAINING:
  # - Task loss: Standard cross-entropy on predictions
  # - LOO loss: Leave-one-out generalization (predict held-out pair)
  # - Equivariance loss: Consistent predictions across augmentations
  #
  # INFERENCE:
  # - Sanity check: Verify LoRA works on support set (>90%)
  # - ACW: Augmented Confidence Weighting for voting
  #
  # PRODUCTION DEFAULT: ENABLED for generalization
  # =============================================================
  use_hyperlora: true        # ENABLED - critical for task-specific adaptation
  hyperlora_rank: 8          # LoRA rank (lower = fewer params, less expressive)
  hyperlora_scaling: 1.0     # Scale factor for LoRA outputs
  hyperlora_dropout: 0.0     # Dropout on LoRA activations
  hyperlora_init_scale: 0.1  # INCREASED: Was 0.01, now 0.1 for stronger meta-learning signal

  # =============================================================
  # HIERARCHICAL PRIMITIVE MEMORY (HPM v2) - Dec 2025
  # =============================================================
  # HPM is a multi-bank memory system for universal continual learning.
  # It stores ALL types of useful primitives (not just compositional).
  #
  # v2 IMPROVEMENTS:
  # - Sparse MoE Top-K routing (prevents mode collapse)
  # - Gated residual initialized to 0 (training stability)
  # - Static vs Dynamic bank split (unbounded memory)
  # - Load Balancing Loss (ensures all banks utilized)
  #
  # BANK TYPES:
  # - STATIC BANKS (nn.Parameter, learned during training):
  #   - Compositional: Transformations that compose (rotate, translate)
  #   - Pattern: Holistic patterns/templates (textures, shapes)
  #   - Relational: Spatial/logical relationships (above, inside)
  #   - Concept: Domain knowledge (semantics, categories)
  #
  # - DYNAMIC BANKS (KV-cache, grows with solved tasks):
  #   - Procedural: HyperLoRA latent codes for task procedures
  #   - Instance: ContextEncoder outputs for retrieval-based reasoning
  #
  # MEMORY EFFICIENCY:
  # - Gate starts at 0, so HPM contributes nothing initially
  # - Top-K routing means only k banks queried per sample
  # - Dynamic banks use CPU storage with FAISS for fast retrieval
  #
  # THREE-PHASE TRAINING WORKFLOW:
  # Phase 1: Train baseline RLAN (HPM gate stays ~0)
  # Phase 2: Populate dynamic buffers with solved tasks
  # Phase 3: Fine-tune with HPM retrieval enabled
  #
  # PRODUCTION DEFAULT: DISABLED (enable for continual learning)
  # =============================================================
  use_hpm: false                      # DISABLED - enable for continual learning
  
  # Sparse MoE Routing Configuration
  hpm_top_k: 2                        # Number of banks to route to per sample
  hpm_balance_weight: 0.01            # Load balancing loss weight (prevents mode collapse)
  
  # Static Bank Configuration
  hpm_primitives_per_bank: 16         # Number of primitives per static bank
  hpm_levels_per_bank: 2              # Hierarchical levels within each bank
  hpm_use_cross_attention: true       # Use cross-attention aggregation
  
  # Dynamic Bank Configuration
  hpm_memory_size: 10000              # Max entries in dynamic banks
  hpm_retrieval_k: 5                  # Number of neighbors to retrieve
  
  # Bank Selection (which banks to enable)
  # Static banks (learned primitives):
  hpm_use_compositional_bank: true    # Transformations that compose
  hpm_use_pattern_bank: true          # Holistic patterns/templates
  hpm_use_relational_bank: true       # Spatial/logical relationships
  hpm_use_concept_bank: false         # Domain knowledge (optional)
  
  # Dynamic banks (KV-cache, grows over time):
  hpm_use_procedural_bank: false      # HyperLoRA codes (enable with HyperLoRA)
  hpm_use_instance_bank: false        # ContextEncoder cache (enable for retrieval)

training:
  max_epochs: 200  # Enough for hard tasks (~130 epochs for 3x3→9x9 expansion)
  
  # CONSERVATIVE batch size to stay within 24GB GPU VRAM only
  # 
  # MEMORY ANALYSIS (Dec 2025 with CrossAttention + SolverCrossAttention):
  # - Base model (commit 2a50f2d, no cross-attention): batch=75 used ~22GB
  # - CrossAttentionInjector adds: ~160 MB (query+key+value+FFN activations)
  # - SolverCrossAttention adds: ~400 MB (6 steps × 67 MB each)
  # - Backward pass: ~2× forward = +1.1 GB from cross-attention
  # - TOTAL additional: ~1.7 GB from cross-attention features
  #
  # ADDITIONAL MEMORY (Dec 2025 with HyperLoRA + LOO + Equivariance):
  # - HyperLoRA: ~500 MB (5M params + activations)
  # - LOO training: N forward passes per sample (N=3-5 avg)
  # - Equivariance: 4 augmentations = 4x forward passes
  # - TOTAL: ~4-5 GB additional on top of base model
  #
  # To stay within 24GB with ~2GB headroom:
  # - Reduce batch from 64 to 16 (saves ~12 GB)
  # - Use grad_accumulation=20 for effective batch of 320 (TRM-competitive)
  #
  # MEMORY FIX (Dec 2025): OOM with batch=32 due to:
  # - LOO training: N forward passes (N=3-5) with graph accumulation
  # - Equivariance: 4 augmentation forward passes per sample
  # - Combined: up to 9x memory multiplier in worst case!
  # 
  # UPDATE (Dec 2025): With staged meta-learning, LOO/Equiv are OFF for
  # epochs 1-3, so we can use larger batches. Increased from 16 to 28.
  # At 16GB usage with batch=16, we have 8GB headroom on 24GB GPU.
  #
  batch_size: 28    # Increased from 16 (8GB headroom available)
  grad_accumulation_steps: 12  # effective_batch_size = 28 × 12 = 336
  
  learning_rate: 5.0e-4  # Works well in ablation
  weight_decay: 0.01     # Light regularization
  gradient_clip: 1.0     # Conservative
  
  # Per-module LR boosting
  # NOTE: Set to 1.0 for stable training. Higher values may cause NaN.
  # HyperLoRA benefits from higher LR to learn meta-weight prediction faster.
  # MIDDLE GROUND (Dec 2025): Reduced from 10.0 to 3.0 for smoother staged training
  dsc_lr_multiplier: 1.0
  msre_lr_multiplier: 1.0
  hyperlora_lr_multiplier: 3.0   # 3x base LR (was 10x, caused instability)
  
  # Temperature for DSC attention
  temperature_start: 1.0
  temperature_end: 0.5    # Don't go below 0.5
  
  # =============================================================
  # LOSS CONFIGURATION - PURE STABLEMAX (TRM PROVEN)
  # =============================================================
  # TRM uses PURE stablemax_cross_entropy with NO class weighting.
  # They handle class imbalance through:
  #   1. Color permutation on every sample (shuffles which color is rare)
  #   2. Large pre-generated cache (1000x augmentation per task)
  #   3. Long training (100K epochs)
  #
  # For RLAN, we adopt the same approach:
  #   - Pure stablemax (no weighting, no focal)
  #   - 100% color permutation (shuffles class distribution)
  #   - 400K cached samples (500 per task)
  #
  # If RLAN's architecture is superior, it should learn faster!
  # =============================================================
  loss_mode: 'stablemax'  # TRM-proven: pure cross-entropy, no weighting
  bg_weight_cap: 2.0      # Not used with stablemax mode
  fg_weight_cap: 5.0      # Not used with stablemax mode
  
  focal_gamma: 2.0        # Not used with stablemax mode
  focal_alpha: 0.75       # Not used with stablemax mode
  
  # AUXILIARY LOSSES (minimized for stability)
  lambda_entropy: 0.01          # Light attention sharpness
  lambda_sparsity: 0.5          # Required for clue regularization
  lambda_predicate: 0.01        # Light predicate diversity
  lambda_curriculum: 0.0        # OFF
  lambda_deep_supervision: 0.0  # OFF - not needed with weighted_stablemax
  lambda_act: 0.0               # OFF
  
  # Clue regularization (ENABLED - validated stable Dec 2025)
  # Helps model learn appropriate clue count per task
  min_clues: 1.0             # Allow 1-clue simple tasks (was 2.5)
  min_clue_weight: 5.0       # Prevents collapse to 0 clues
  ponder_weight: 0.02        # Small cost per clue (encourages efficiency)
  entropy_ponder_weight: 0.02  # Encourages sharp attention
  
  use_stablemax: true
  
  use_curriculum: false
  curriculum_stages: []
  
  optimizer: "adamw"
  beta1: 0.9
  beta2: 0.95
  
  # NO SCHEDULER - constant LR (most stable)
  scheduler: "none"
  warmup_epochs: 0
  min_lr: 5.0e-4  # Same as learning_rate
  
  # =============================================================
  # EMA CONFIGURATION - DISABLED FOR 20-EPOCH TRAINING
  # =============================================================
  # Analysis showed EMA with decay=0.999 creates 1000-step lag.
  # For 20-epoch training (~26K steps), EMA never catches up.
  # Disabled based on user guidance: "EMA doesn't benefit 20 epochs"
  # =============================================================
  use_ema: false
  ema_decay: 0.999  # Not used when use_ema=false
  
  # =============================================================
  # STAGED META-LEARNING (Dec 2025 - BG Collapse Fix)
  # =============================================================
  # PROBLEM: Enabling LOO + Equivariance from epoch 1 destabilizes
  # early learning, causing the model to collapse to all-background.
  #
  # SOLUTION: Train base model first (pure task loss), then activate
  # meta-learning losses after the model learns basic FG/BG distinction.
  #
  # This parameter controls when LOO and Equivariance losses activate.
  # The model architecture (HyperLoRA) is always present, but the
  # auxiliary meta-learning losses are delayed.
  # =============================================================
  meta_learning_start_epoch: 3  # Activate LOO/Equivariance after epoch 3
  
  # =============================================================
  # LOO TRAINING: LEAVE-ONE-OUT META-LEARNING LOSS
  # =============================================================
  # Teaches the model to generalize from N-1 examples to the Nth.
  # Only active when use_hyperlora: true in model section.
  # Only active after meta_learning_start_epoch (staged training).
  #
  # For each training task with N pairs:
  #   1. Hold out one pair as the "test"
  #   2. Pool context from remaining N-1 pairs
  #   3. Predict LoRA weights from this context
  #   4. Use LoRA to predict held-out pair
  #   5. Backprop loss on held-out prediction
  #
  # This explicitly trains the HyperLoRA to GENERALIZE.
  # =============================================================
  loo_training:
    enabled: true
    loss_weight: 0.2       # MIDDLE GROUND: 20% of task loss (was 0.5, caused instability)
    min_pairs_for_loo: 2   # Need at least 2 pairs to hold one out
  
  # =============================================================
  # AUGMENTATION EQUIVARIANCE LOSS
  # =============================================================
  # Encourages consistent LoRA predictions across augmented views.
  # If a task is rotated 90°, the predicted LoRA weights should
  # have similar magnitudes (the "rule" is the same, just rotated).
  # Only active after meta_learning_start_epoch (staged training).
  #
  # Loss: L2 on Frobenius norm difference of LoRA matrices.
  # =============================================================
  equivariance_training:
    enabled: true
    loss_weight: 0.05      # MIDDLE GROUND: Lighter regularization (was 0.1)
    num_augmentations: 4   # Number of random augmentations per task
  
  # =============================================================
  # META-LEARNING STRATEGY SUMMARY (COMPLEMENTARY LOSSES)
  # =============================================================
  # The three meta-learning losses work together without conflict:
  #
  # 1. TASK LOSS (weight=1.0): Standard cross-entropy on predictions
  #    - Objective: Predict correct output grids
  #    - Target: Direct task performance
  #
  # 2. LOO LOSS (weight=0.5): Leave-one-out generalization
  #    - Objective: Predict Nth pair from N-1 context
  #    - Target: Few-shot generalization within a task
  #    - Complementary: Doesn't conflict with task loss - both want correct predictions
  #                     LOO adds EXPLICIT generalization signal during training
  #
  # 3. EQUIVARIANCE LOSS (weight=0.1): Augmentation consistency
  #    - Objective: Same LoRA weights for rotated/flipped versions
  #    - Target: Transform-invariant representations
  #    - Complementary: Acts as REGULARIZER - prevents overfitting to specific orientations
  #                     Light weight (0.1) ensures it doesn't dominate task learning
  #
  # MATHEMATICAL RELATIONSHIP:
  #   L_total = L_task + 0.5 * L_loo + 0.1 * L_equiv
  #
  # WHY THIS IS OPTIMAL (NOT OVERKILL):
  # - Task loss drives direct performance
  # - LOO loss adds explicit generalization pressure (meta-learning signal)
  # - Equivariance loss adds regularization (prevents orientation bias)
  # - Weights are calibrated so task loss dominates, meta-losses support
  #
  # HYPERLORA LR MULTIPLIER (10x):
  # - HyperLoRA needs faster learning to adapt its meta-prediction
  # - Base model has stable pretrained weights, HyperLoRA learns from scratch
  # - 10x ensures HyperLoRA doesn't become a bottleneck
  # =============================================================

data:
  train_path: "./data/arc-agi/data/training"
  eval_path: "./data/arc-agi/data/evaluation"
  
  max_grid_size: 30
  
  # CRITICAL: Ignore padding in loss
  ignore_padding_in_loss: true
  
  # Data loading configuration
  # NOTE: When cache_samples=true, num_workers is automatically set to 0
  # to avoid duplicating the 400K cache across worker processes.
  # GPU stalls are prevented by CUDAPrefetcher (async CPU→GPU transfer).
  num_workers: 12      # Used when cache_samples=false (on-the-fly augmentation)
  pin_memory: true     # Enables async GPU transfer (always beneficial)
  prefetch_factor: 6   # Used when num_workers > 0
  persistent_workers: true
  
  # ==========================================================================
  # CACHING + AUGMENTATION STRATEGY (TRM-INSPIRED)
  # ==========================================================================
  # TRM uses 1000x pre-generated augmentation per task with:
  #   - 8 dihedral transforms
  #   - Random color permutation (color 0 fixed, 1-9 shuffled)
  #   - Translational offset
  #
  # We do the same with 400K cached samples:
  #   - Each sample has RANDOM augmentation baked in
  #   - color_permutation_prob: 1.0 = EVERY sample color-permuted (like TRM!)
  #   - This naturally balances class distribution over training
  #
  # CRITICAL: If you change augmentation settings, DELETE the cache!
  #   rm ./cache/rlan_stable_400k.pkl
  # ==========================================================================
  
  cache_samples: true
  num_cached_samples: 400000  # 500 per task (TRM uses 1000x)
  cache_path: "./cache/rlan_stable_400k_v3.pkl"  # v3: TRM-style aug order (color→dihedral)
  
  augmentation:
    enabled: true
    rotation: true
    flip: true
    transpose: true
    color_permutation: true
    color_permutation_prob: 1.0  # TRM STYLE: ALWAYS apply (was 0.3, now 1.0)
    translational: true
    track_augmentation: true  # REQUIRED for inverse aug at eval

# =============================================================
# EVALUATION - TRM-STYLE WITH INVERSE AUGMENTATION
# =============================================================
# Critical for measuring true generalization:
# 1. Apply augmentations to test inputs
# 2. Get model predictions
# 3. REVERSE augmentations to canonical space
# 4. Compare with original ground truth
# 5. Use voting across predictions for robustness
# =============================================================
evaluation:
  num_guesses: 2
  use_tta: true
  tta_rotations: [0, 90, 180, 270]
  tta_flips: [false, true]
  
  # TRM-style evaluation settings
  use_trm_style_eval: true
  num_augmented_views: 8  # 8 dihedral transforms (D4 group)
  num_color_perms: 4      # 4 color permutations per dihedral = 32 total views
  use_voting: true        # Aggregate predictions
  use_inverse_aug: true   # CRITICAL: Undo augmentations before comparison
  pass_ks: [1, 2, 3]      # Report Pass@K: is answer in top K voted predictions?
  max_eval_tasks: 100     # Number of eval tasks to run TTA on (100 for full eval)

# =============================================================
# GAP MONITORING - EARLY WARNING FOR GENERALIZATION ISSUES
# =============================================================
monitoring:
  enabled: true
  exact_match_warning: 0.10   # Warn if train-eval gap > 10%
  exact_match_critical: 0.20  # Critical if gap > 20%
  entropy_ratio_warning: 2.0  # Warn if eval entropy > 2x train
  entropy_ratio_critical: 5.0 # Critical if > 5x
  stop_value_warning: 0.15    # Warn if stop value gap > 0.15
  stop_value_critical: 0.25   # Critical if > 0.25

logging:
  log_every: 1
  save_every: 10
  eval_every: 1  # Changed from 5 to detect train/eval gap early
  keep_last_n: 3
  checkpoint_dir: "checkpoints/rlan_stable"
  log_to_file: true
  track_augmentation: true  # Enable to verify augmentation diversity
  
  use_wandb: false
  wandb_project: "rlan-arc"
  wandb_run_name: null

# =============================================================
# INFERENCE SETTINGS (Dec 2025)
# =============================================================
# These settings can be overridden via command-line arguments:
#   --use-best-step, --no-best-step, --num-steps <N>
#
# Example commands:
#   # Use entropy-based best step + 10 solver iterations
#   python evaluate_rlan.py --checkpoint best.pt --use-best-step --num-steps 10
#
#   # Compare 6 vs 10 steps
#   python evaluate_rlan.py --checkpoint best.pt --num-steps 6
#   python evaluate_rlan.py --checkpoint best.pt --num-steps 10
#
inference:
  # Temperature for softmax (lower = sharper predictions)
  # Should match training end temperature for consistency
  temperature: 0.1
  
  # Best-step selection: pick step with lowest entropy (most confident)
  # - false: Always use last step (default, matches training)
  # - true: Use entropy-based selection (may improve accuracy)
  use_best_step_selection: false
  
  # Solver steps override (null = use model's trained default)
  # Can test with more steps than trained (e.g., train=6, infer=10)
  num_steps_override: null
  
  # Test-Time Augmentation (matches training TTA for consistent results)
  # With 8 dihedral x 4 color perms = 32 views, same as evaluate_trm_style()
  use_tta: true
  num_dihedral: 8      # Full D4 group (identity + 3 rotations + 4 reflections)
  num_color_perms: 4   # Color permutations per dihedral (keeps color 0 fixed)
  
  # Batch size for evaluation (can be larger than training)
  batch_size: 32
  
  # =============================================================
  # HYPER-LORA INFERENCE SETTINGS
  # =============================================================
  # LOO Sanity Check: Verify predicted LoRA weights work on support set
  # If accuracy on training pairs is below threshold, fall back to
  # standard forward pass without LoRA (safety mechanism).
  # =============================================================
  loo_sanity_check: true
  loo_threshold: 0.9    # Require 90% accuracy on support set
  
  # =============================================================
  # ACW: AUGMENTED CONFIDENCE WEIGHTING
  # =============================================================
  # Instead of simple voting, weight each augmented prediction by
  # how consistent it is with other augmentations. More consistent
  # predictions get higher weight in the final vote.
  #
  # Consistency = 1 - mean(pairwise_disagreement_with_other_views)
  # =============================================================
  acw_enabled: true
  acw_num_views: 8     # Number of augmented views to consider

hardware:
  device: "cuda"
  seed: 42
  deterministic: false  # Set true for reproducibility (slower)
  check_nan_inf: true   # Enable NaN/Inf detection in training loop

device:
  use_cuda: true
  mixed_precision: true
  dtype: "bfloat16"
  compile: false
