# RLAN STABLE Configuration
# ==========================
# Production-ready configuration validated by ablation testing
# 
# HARDWARE TARGET: RTX 3090 (24GB VRAM), 48 virtual CPUs, 128GB RAM
#
# USE THIS CONFIG WHEN:
# - Training new models from scratch
# - Production training runs
# - Establishing baseline performance
#
# ABLATION VALIDATION (Dec 2025):
# - Achieves 100% exact match on simple ARC tasks in 2-5 epochs
# - Grid expansion tasks (3x3→9x9) reach 100% in ~130 epochs
# - Clue regularization validated: faster convergence, stable training
# - No NaN issues with proper padding (ignore_index=-100)
#
# CONTEXT INJECTION (Dec 2025):
# - use_cross_attention_context=true with spatial_downsample=8
# - Preserves spatial structure for generalization (unlike FiLM compression)
# - Adaptive pooling: all grids → 8×8 + learned refinement + position re-encoding
# - Memory efficient: 64 tokens/pair vs 900 (14x reduction)
#
# DATA LOADING (Dec 2025):
# - 400K cached samples in CPU RAM (not GPU VRAM!)
# - num_workers=0 when cached (avoids memory duplication)
# - CUDAPrefetcher for async CPU→GPU transfer (no GPU stalls)
#
# SAMPLE COUNT (TRM comparison):
# - TRM: 1000x pre-generated augmentation = 800,000 samples
# - RLAN Phase 1: 400,000 cached (1000 per task)
# - RLAN Phase 2: INFINITE on-the-fly augmentation
#
# KEY PRINCIPLES:
# 1. No scheduler - constant learning rate (most stable)
# 2. Clue regularization ENABLED (prevents clue collapse)
# 3. Core modules: ContextEncoder (CrossAttention+Pooling), DSC, MSRE
# 4. Light auxiliary losses (entropy, sparsity, predicate)
# 5. max_grid_size=30 (official ARC-AGI maximum)
#
# TRAINING WORKFLOW:
# Phase 1: Train with cache_samples=true until convergence (memorization)
# Phase 2: Resume with cache_samples=false for diversity (generalization)
#
# ⚠️ CACHE NOTE: DELETE CACHE after augmentation order fix (Dec 2025)!
# Previous cache had wrong order (dihedral→color), now using TRM order (color→dihedral)
#   rm ./cache/rlan_stable_400k_v2.pkl
#   rm ./cache/rlan_stable_400k_v3.pkl  # v3 with correct TRM-style order
#
# ==========================

model:
  type: "rlan"
  hidden_dim: 256
  num_colors: 10
  num_classes: 10  # colors 0-9
  max_grid_size: 30
  
  max_clues: 7
  num_predicates: 32  # More predicates for diversity
  num_solver_steps: 6   # Reverted from 10 - 6 is stable and fast
  
  use_act: false       # DISABLED
  dropout: 0.1
  
  dsc_num_heads: 4
  lcr_num_heads: 4     # Not used
  
  msre_encoding_dim: 32
  msre_num_freq: 8
  lcr_num_freq: 8      # Not used
  
  # CORE MODULES ONLY (ablation validated)
  use_context_encoder: true   # REQUIRED - learns task from demos
  use_dsc: true               # REQUIRED - dynamic spatial cluing
  use_msre: true              # REQUIRED - multi-scale relative encoding
  use_lcr: false              # DISABLED - not needed for learning
  
  # Context injection mode (CRITICAL FOR STABILITY!)
  # false = FiLM conditioning (stable, proven, compresses context to vector)
  # true = CrossAttention (experimental, high memory, can cause mode collapse)
  #
  # MIDDLE GROUND: Enable CrossAttention but with spatial downsampling
  # This gives spatial context without the massive memory cost
  use_cross_attention_context: true
  
  # Spatial downsample TARGET SIZE (not divisor!)
  # All support grids (3x3 to 30x30) are pooled to this fixed size
  # 8 = 8x8 output (64 tokens per pair, ~14x reduction from 30x30)
  # Uses learned refinement + position re-encoding after pooling
  spatial_downsample: 8       # 8x8 target - preserves objects while being memory-efficient
  use_sph: false              # DISABLED - not needed for learning
  use_learned_pos: false      # DISABLED - sinusoidal works better
  
  # Phase 2.5: Solver Cross-Attention to Support Set (Dec 2025)
  # ENABLED: Critical for generalization to unseen ARC rules
  # 
  # WHY THIS IS NEEDED:
  # - DSC injection provides GLOBAL context (baked in once at start)
  # - Solver cross-attention provides LOCAL verification at EVERY step
  # - These are COMPLEMENTARY: spatial priors + iterative verification
  #
  # MEMORY COST: ~100MB additional (trivial on 24GB GPU)
  # - Support: (B, N, 256, 8, 8) computed ONCE, reused across 6 steps
  # - Attention: O(900 × 256) per step = ~230K ops = <1ms/step
  #
  # PREVIOUS ISSUE: Mode collapse was caused by hardcoded CrossAttentionInjector
  # not by solver cross-attention. Now with spatial_downsample=8, memory is fine.
  use_solver_context: true    # ENABLED - critical for iterative verification
  solver_context_heads: 4     # Multi-head attention for diverse queries

training:
  max_epochs: 200  # Enough for hard tasks (~130 epochs for 3x3→9x9 expansion)
  
  # MAXIMIZED for 24GB GPU with solver cross-attention enabled
  # Peak VRAM ≈ 19.8 GB (leaves ~4 GB headroom)
  # Larger real batch > gradient accumulation for:
  #   - Faster training (1 forward pass vs multiple)
  #   - Better GPU utilization
  #   - More diverse dropout/augmentation per step
  batch_size: 288   # Max safe for 24GB with solver cross-attention
  grad_accumulation_steps: 1  # No accumulation needed (real batch = effective batch)
  
  learning_rate: 5.0e-4  # Works well in ablation
  weight_decay: 0.01     # Light regularization
  gradient_clip: 1.0     # Conservative
  
  # NO per-module LR boosting (ablation showed it can cause NaN)
  dsc_lr_multiplier: 1.0
  msre_lr_multiplier: 1.0
  
  # Temperature for DSC attention
  temperature_start: 1.0
  temperature_end: 0.5    # Don't go below 0.5
  
  # =============================================================
  # LOSS CONFIGURATION - PURE STABLEMAX (TRM PROVEN)
  # =============================================================
  # TRM uses PURE stablemax_cross_entropy with NO class weighting.
  # They handle class imbalance through:
  #   1. Color permutation on every sample (shuffles which color is rare)
  #   2. Large pre-generated cache (1000x augmentation per task)
  #   3. Long training (100K epochs)
  #
  # For RLAN, we adopt the same approach:
  #   - Pure stablemax (no weighting, no focal)
  #   - 100% color permutation (shuffles class distribution)
  #   - 400K cached samples (500 per task)
  #
  # If RLAN's architecture is superior, it should learn faster!
  # =============================================================
  loss_mode: 'stablemax'  # TRM-proven: pure cross-entropy, no weighting
  bg_weight_cap: 2.0      # Not used with stablemax mode
  fg_weight_cap: 5.0      # Not used with stablemax mode
  
  focal_gamma: 2.0        # Not used with stablemax mode
  focal_alpha: 0.75       # Not used with stablemax mode
  
  # AUXILIARY LOSSES (minimized for stability)
  lambda_entropy: 0.01          # Light attention sharpness
  lambda_sparsity: 0.5          # Required for clue regularization
  lambda_predicate: 0.01        # Light predicate diversity
  lambda_curriculum: 0.0        # OFF
  lambda_deep_supervision: 0.0  # OFF - not needed with weighted_stablemax
  lambda_act: 0.0               # OFF
  
  # Clue regularization (ENABLED - validated stable Dec 2025)
  # Helps model learn appropriate clue count per task
  min_clues: 1.0             # Allow 1-clue simple tasks (was 2.5)
  min_clue_weight: 5.0       # Prevents collapse to 0 clues
  ponder_weight: 0.02        # Small cost per clue (encourages efficiency)
  entropy_ponder_weight: 0.02  # Encourages sharp attention
  
  use_stablemax: true
  
  use_curriculum: false
  curriculum_stages: []
  
  optimizer: "adamw"
  beta1: 0.9
  beta2: 0.95
  
  # NO SCHEDULER - constant LR (most stable)
  scheduler: "none"
  warmup_epochs: 0
  min_lr: 5.0e-4  # Same as learning_rate
  
  # =============================================================
  # EMA CONFIGURATION - DISABLED FOR 20-EPOCH TRAINING
  # =============================================================
  # Analysis showed EMA with decay=0.999 creates 1000-step lag.
  # For 20-epoch training (~26K steps), EMA never catches up.
  # Disabled based on user guidance: "EMA doesn't benefit 20 epochs"
  # =============================================================
  use_ema: false
  ema_decay: 0.999  # Not used when use_ema=false

data:
  train_path: "./data/arc-agi/data/training"
  eval_path: "./data/arc-agi/data/evaluation"
  
  max_grid_size: 30
  
  # CRITICAL: Ignore padding in loss
  ignore_padding_in_loss: true
  
  # Data loading configuration
  # NOTE: When cache_samples=true, num_workers is automatically set to 0
  # to avoid duplicating the 400K cache across worker processes.
  # GPU stalls are prevented by CUDAPrefetcher (async CPU→GPU transfer).
  num_workers: 12      # Used when cache_samples=false (on-the-fly augmentation)
  pin_memory: true     # Enables async GPU transfer (always beneficial)
  prefetch_factor: 6   # Used when num_workers > 0
  persistent_workers: true
  
  # ==========================================================================
  # CACHING + AUGMENTATION STRATEGY (TRM-INSPIRED)
  # ==========================================================================
  # TRM uses 1000x pre-generated augmentation per task with:
  #   - 8 dihedral transforms
  #   - Random color permutation (color 0 fixed, 1-9 shuffled)
  #   - Translational offset
  #
  # We do the same with 400K cached samples:
  #   - Each sample has RANDOM augmentation baked in
  #   - color_permutation_prob: 1.0 = EVERY sample color-permuted (like TRM!)
  #   - This naturally balances class distribution over training
  #
  # CRITICAL: If you change augmentation settings, DELETE the cache!
  #   rm ./cache/rlan_stable_400k.pkl
  # ==========================================================================
  
  cache_samples: true
  num_cached_samples: 400000  # 500 per task (TRM uses 1000x)
  cache_path: "./cache/rlan_stable_400k_v3.pkl"  # v3: TRM-style aug order (color→dihedral)
  
  augmentation:
    enabled: true
    rotation: true
    flip: true
    transpose: true
    color_permutation: true
    color_permutation_prob: 1.0  # TRM STYLE: ALWAYS apply (was 0.3, now 1.0)
    translational: true
    track_augmentation: true  # REQUIRED for inverse aug at eval

# =============================================================
# EVALUATION - TRM-STYLE WITH INVERSE AUGMENTATION
# =============================================================
# Critical for measuring true generalization:
# 1. Apply augmentations to test inputs
# 2. Get model predictions
# 3. REVERSE augmentations to canonical space
# 4. Compare with original ground truth
# 5. Use voting across predictions for robustness
# =============================================================
evaluation:
  num_guesses: 2
  use_tta: true
  tta_rotations: [0, 90, 180, 270]
  tta_flips: [false, true]
  
  # TRM-style evaluation settings
  use_trm_style_eval: true
  num_augmented_views: 8  # 8 dihedral transforms (D4 group)
  num_color_perms: 4      # 4 color permutations per dihedral = 32 total views
  use_voting: true        # Aggregate predictions
  use_inverse_aug: true   # CRITICAL: Undo augmentations before comparison
  pass_ks: [1, 2, 5, 10]  # Report Pass@K metrics like TRM
  max_eval_tasks: 100     # Number of eval tasks to run TTA on (100 for full eval)

# =============================================================
# GAP MONITORING - EARLY WARNING FOR GENERALIZATION ISSUES
# =============================================================
monitoring:
  enabled: true
  exact_match_warning: 0.10   # Warn if train-eval gap > 10%
  exact_match_critical: 0.20  # Critical if gap > 20%
  entropy_ratio_warning: 2.0  # Warn if eval entropy > 2x train
  entropy_ratio_critical: 5.0 # Critical if > 5x
  stop_value_warning: 0.15    # Warn if stop value gap > 0.15
  stop_value_critical: 0.25   # Critical if > 0.25

logging:
  log_every: 1
  save_every: 10
  eval_every: 1  # Changed from 5 to detect train/eval gap early
  keep_last_n: 3
  checkpoint_dir: "checkpoints/rlan_stable"
  log_to_file: true
  track_augmentation: true  # Enable to verify augmentation diversity
  
  use_wandb: false
  wandb_project: "rlan-arc"
  wandb_run_name: null

hardware:
  device: "cuda"
  seed: 42
  deterministic: false

device:
  use_cuda: true
  mixed_precision: true
  dtype: "bfloat16"
  compile: false
