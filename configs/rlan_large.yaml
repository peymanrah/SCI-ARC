# RLAN Large Configuration (~51M params)
# =======================================
# Full capacity RLAN for capacity scaling experiments
# 
# WARNING: NOT a fair comparison with TRM (~7M params)
# Use rlan_fair.yaml for parameter-matched comparison
#
# Purpose: Explore if more capacity helps ARC
# - Does 51M RLAN outperform 7M RLAN?
# - Is there a capacity ceiling for ARC?
#
# Module Scaling Rules:
#   num_heads = hidden_dim / 64 = 512 / 64 = 8
#   max_clues, num_predicates = task-specific (fixed)
#   msre/lcr params = positional encoding (fixed)
#
# Optimized for RTX 3090 (24GB VRAM), single GPU training

model:
  type: "rlan"
  hidden_dim: 512   # Full capacity (~51M params)
  num_colors: 10
  num_classes: 11   # 10 colors + background
  max_grid_size: 30
  
  # Task-specific (NOT capacity-dependent) - same as other configs
  max_clues: 6       # Ceiling slightly above ARC max (~5) for generalization
  num_predicates: 8  # Covers most spatial relationships
  num_solver_steps: 6
  
  use_act: true  # Enable Adaptive Computation Time
  dropout: 0.1
  
  # Attention heads: hidden_dim / 64 = 512 / 64 = 8
  dsc_num_heads: 8
  lcr_num_heads: 8
  
  # Positional encoding (fixed, task-independent)
  msre_encoding_dim: 64
  msre_num_freq: 8
  lcr_num_freq: 8

training:
  max_epochs: 1000
  
  # =============================================================
  # BATCH SIZE STRATEGY FOR SINGLE RTX 3090 (24GB VRAM)
  # =============================================================
  # hidden_dim=512: ~500MB/sample with AMP + grads
  # Batch: 36 samples (~18GB VRAM, 6GB headroom for 51M model)
  # grad_accumulation=8 → effective_batch = 36 × 8 = 288
  # 
  # Slightly more headroom needed for larger model (EMA = 51M shadow)
  # =============================================================
  batch_size: 36
  grad_accumulation_steps: 8
  # effective_batch_size = 36 × 8 = 288
  
  learning_rate: 1.0e-4
  weight_decay: 0.1
  gradient_clip: 1.0
  
  # Temperature schedule
  temperature_start: 5.0
  temperature_end: 0.1
  
  # Loss weights - focal loss dominates
  focal_gamma: 2.0
  focal_alpha: 0.25
  lambda_entropy: 0.01
  lambda_sparsity: 0.005
  lambda_predicate: 0.001
  lambda_curriculum: 0.01
  lambda_deep_supervision: 0.3
  
  # =============================================================
  # 3-STAGE CURRICULUM LEARNING (easy → medium → hard → all)
  # =============================================================
  # Difficulty based on grid size and number of examples:
  #   Stage 1 (easy): small grids (≤10×10), 3+ training examples
  #   Stage 2 (medium): medium grids (≤20×20), 2+ examples
  #   Stage 3 (hard): large grids, any examples
  #   Stage 4: all data mixed (full training)
  # =============================================================
  use_curriculum: true
  curriculum_stages:
    - 100   # Stage 1 (epochs 0-99): easy tasks only
    - 300   # Stage 2 (epochs 100-299): + medium tasks
    - 600   # Stage 3 (epochs 300-599): + hard tasks
            # Stage 4 (epochs 600+): all data mixed
  
  # Optimizer & Scheduler
  optimizer: "adamw"
  scheduler: "onecycle"
  warmup_epochs: 20
  min_lr: 1.0e-6
  
  # EMA for stable evaluation
  use_ema: true
  ema_decay: 0.999

data:
  train_path: "./data/arc-agi/data/training"
  eval_path: "./data/arc-agi/data/evaluation"
  
  max_grid_size: 30
  
  # Data loading optimization
  num_workers: 12
  pin_memory: true
  prefetch_factor: 4
  persistent_workers: true
  
  # Infinite diversity mode
  cache_samples: false
  
  # Full augmentation pipeline
  augmentation:
    enabled: true
    rotation: true
    flip: true
    transpose: true
    color_permutation: true

evaluation:
  num_guesses: 2
  use_tta: true
  tta_rotations: [0, 90, 180, 270]
  tta_flips: [false, true]

logging:
  log_every: 1
  save_every: 25
  eval_every: 5
  keep_last_n: 5
  checkpoint_dir: "checkpoints/rlan_large"
  log_to_file: true
  
  use_wandb: false
  wandb_project: "rlan-arc"
  wandb_run_name: null

# Hardware settings
hardware:
  device: "cuda"
  seed: 42
  deterministic: false

device:
  use_cuda: true
  mixed_precision: true   # AMP required for 51M model
  dtype: "bfloat16"
  compile: false
