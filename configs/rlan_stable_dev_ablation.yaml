# RLAN STABLE DEV ABLATION Configuration
# =======================================
# Purpose: Simplify RLAN to a "core-generalizing" baseline for ablation study.
# The goal is to fix eval entropy collapse and establish reliable train→eval generalization
# BEFORE re-adding complex meta-learning components.
#
# Target Milestones:
# - Canonical train exact match (100 tasks): >= 70%
# - Eval exact match (TTA, 100 tasks): 20-30%
# - Eval/Train DSC entropy ratio: <= 2.0
#
# Key Design Principles:
# 1. DISABLE all memory/meta components until base solver generalizes
# 2. USE best-step selection to handle solver over-iteration automatically
# 3. KEEP RLAN novelty: clue-anchored relative coordinate reasoning (DSC + MSRE + Context)
# 4. ENABLE new ablation modules: ART (Anchor Robustness Training) + ARPS (Anchor-Relative Program Search)
#
# NEW MODULES (Jan 2026 Ablation Study):
# - anchor_robustness: Forces model to solve using alternate/perturbed anchors for consistency
# - arps_dsl_search: Neural-guided DSL program search in anchor-relative coordinate frame
#
# USAGE:
#   python scripts/train_rlan.py configs/rlan_stable_dev_ablation.yaml
#
# ==============================================================================

model:
  type: "rlan"
  hidden_dim: 256
  num_colors: 10
  num_classes: 10
  max_grid_size: 30

  # Solver steps: 6 is a good balance between capacity and efficiency
  # With use_best_step_selection=true, over-iteration is mitigated automatically
  # (logs showed earlier steps often best, but best-step selection handles this)
  num_solver_steps: 6            # BALANCED - 6 steps with best-step selection
  use_best_step_selection: true  # KEEP ON - critical for stopping at best step

  # CORE RLAN THEORY MODULES (keep these - they are the novelty)
  use_context_encoder: true      # Learns task from demos - REQUIRED
  use_dsc: true                  # Dynamic Saliency Controller - CORE NOVELTY
  use_msre: true                 # Multi-Scale Relative Encoding - CORE NOVELTY

  # DSC parameters
  max_clues: 7
  num_predicates: 32
  dsc_num_heads: 4

  # MSRE parameters
  msre_encoding_dim: 32
  msre_num_freq: 8

  # DISABLE non-critical modules for ablation baseline
  use_lcr: false                 # Latent Counting Registers - add back if needed
  use_sph: false                 # Symbolic Predicate Heads - add back for conditional tasks
  use_learned_pos: false         # Sinusoidal works better
  use_act: false                 # Adaptive Computation Time - keep simple

  # Context injection - keep solver-context for verification
  use_cross_attention_context: true
  spatial_downsample: 8
  use_solver_context: true       # ENABLED - helps verify/refine predictions
  solver_context_heads: 4

  # DISABLE meta-learning + memory for the ablation baseline
  use_hyperlora: false           # was true - re-enable only if baseline works
  use_hpm: false                 # was true - memory is premature optimization

  dropout: 0.1

  # ===========================================================================
  # NEW: ANCHOR ROBUSTNESS TRAINING (ART) - Jan 2026 Ablation
  # ===========================================================================
  # Forces the model to produce consistent predictions under alternate anchors.
  # This directly targets the observed failure mode: eval attention collapse.
  #
  # Theory: If DSC selects anchor A at train time but anchor B at eval time,
  # predictions should be consistent after inverse reprojection. ART trains
  # for this consistency explicitly.
  # ===========================================================================
  anchor_robustness:
    enabled: true
    num_alt_anchors: 1           # Number of alternate anchors per sample (1-2)
    anchor_jitter_px: 2          # Max pixels to jitter anchor centroid
    use_top_k_anchors: true      # Use top-K attention peaks as alternates
    consistency_loss_type: "kl"  # 'kl' or 'l2' for comparing predictions
    consistency_weight: 0.02     # Weight of consistency loss

  # ===========================================================================
  # NEW: ANCHOR-RELATIVE PROGRAM SEARCH (ARPS) - Jan 2026 Ablation
  # ===========================================================================
  # Neural-guided DSL search in anchor-relative coordinate frame.
  # Key insight: Programs expressed in anchor-relative coords generalize better.
  #
  # The neural head proposes programs, the symbolic executor verifies them
  # against training demos. This gives deterministic guarantees the pure
  # neural approach lacks.
  # ===========================================================================
  arps_dsl_search:
    enabled: true
    use_as_auxiliary: true       # Add to neural head, don't replace
    max_program_length: 12       # INCREASED from 8 for complex multi-step tasks
    beam_size: 64                # INCREASED from 32 for better exploration
    top_k_proposals: 8           # INCREASED from 4 for more candidate verification
    
    # DSL Primitives (anchor-relative)
    primitives:
      - "select_color"           # Select object by color
      - "select_connected"       # Select connected component
      - "translate"              # Move by (dx, dy) relative to anchor
      - "reflect_x"              # Reflect across anchor x-axis
      - "reflect_y"              # Reflect across anchor y-axis
      - "rotate_90"              # Rotate 90° around anchor
      - "rotate_180"             # Rotate 180° around anchor
      - "paint"                  # Change color
      - "copy_paste"             # Copy region to offset
      - "tile"                   # Repeat pattern
      - "crop"                   # Crop to region
      - "fill"                   # Fill connected region
    
    # Verification settings
    require_demo_exact_match: true  # Programs must solve ALL demos
    use_mdl_ranking: true           # Prefer shorter programs (MDL prior)
    
    # Training: Imitation learning from search
    search_during_training: true    # Run search to find pseudo-labels
    imitation_weight: 0.1           # Weight of imitation loss

training:
  # ABLATION EPOCHS: Achievable in one GPU cycle (~4-6 hours on RTX 3090)
  # With 50 samples/task × 400 tasks = 20K samples, batch 360 → ~55 iters/epoch
  # 80 epochs × 55 iters = 4400 iterations
  max_epochs: 80                 # REDUCED from 120 for achievable single GPU cycle

  # Optimizer - keep stable settings
  use_8bit_optimizer: true
  gradient_checkpointing: false
  use_torch_compile: false

  batch_size: 40
  grad_accumulation_steps: 9     # Effective batch = 360

  learning_rate: 5.0e-4
  weight_decay: 0.01
  gradient_clip: 1.0

  optimizer: "adamw"
  beta1: 0.9
  beta2: 0.95

  # Scheduler - constant LR is most stable
  scheduler: "none"
  warmup_epochs: 0
  min_lr: 5.0e-4

  # Temperature for DSC
  temperature_start: 1.0
  temperature_end: 0.5

  # Loss configuration
  loss_mode: "focal_weighted"
  focal_gamma: 1.2
  focal_alpha: 0.75
  bg_weight_cap: 1.0
  fg_weight_cap: 6.0

  # REDUCED sparsity pressure to allow more flexible clue discovery
  lambda_entropy: 0.01
  lambda_sparsity: 0.2           # was 0.5 - too restrictive
  lambda_predicate: 0.0          # SPH disabled
  lambda_curriculum: 0.0
  lambda_deep_supervision: 0.0
  lambda_act: 0.0
  lambda_centroid_diversity: 0.8  # INCREASED from 0.3 - critical to prevent clue collapse

  # Clue regularization - reduced to allow task-adaptive clue counts
  min_clues: 1.0
  min_clue_weight: 2.0           # was 5.0
  ponder_weight: 0.01            # was 0.02
  entropy_ponder_weight: 0.01    # was 0.02

  clue_variance_weight: 1.0
  clue_target_variance: 0.5

  stop_saturation_weight: 0.5
  stop_saturation_threshold: 5.0

  use_stablemax: true

  # NO curriculum for ablation - keep training simple
  use_curriculum: false
  curriculum_stages: []

  # EMA disabled for simplicity
  use_ema: false
  ema_decay: 0.995

  # ===========================================================================
  # DISABLE ALL META-LEARNING FOR ABLATION
  # ===========================================================================
  solver_context_start_epoch: 1   # Enable from start (it's stable)
  cross_attention_start_epoch: 1  # Enable from start
  meta_learning_start_epoch: 999  # Never activate HyperLoRA

  # LOO Training - DISABLED
  loo_training:
    enabled: false

  # Equivariance Training - SIMPLIFIED
  equivariance_training:
    enabled: false               # Weight-level equiv disabled

  # Output Equivariance - ENABLED early for TTA consistency
  output_equivariance_training:
    enabled: true
    start_epoch: 3               # Enable early
    loss_weight: 0.01            # Light weight
    num_augmentations: 2
    loss_type: "kl"
    mask_to_target: true

  group_marginalized_nll:
    enabled: false

  # ===========================================================================
  # REALISTIC PHASED TRAINING FOR AGI-ARC GENERALIZATION
  # ===========================================================================
  # Phase A (epochs 1-60): Heavy augmentation for diversity and generalization
  # Phase B (epochs 61-80): Reduced augmentation for fine-tuning
  # 
  # TIMELINE (with 240K pool, 6000 iters/epoch):
  # - Phase A: 60 epochs × 6000 iters = 360,000 training steps
  # - Phase B: 20 epochs × 6000 iters = 120,000 fine-tuning steps
  # - Total: 80 epochs × 6000 iters = 480,000 gradient updates
  # - Estimated time: 12-16 hours on RTX 3090/4090
  # ===========================================================================
  phased_training:
    enabled: true

    # Phase A: Heavy augmentation for learning invariances (generalization phase)
    phase_a:
      end_epoch: 60              # First 60 epochs with full augmentation
      batch_size: 40
      augmentation:
        rotation: true           # ENABLED - dihedral group for invariance
        flip: true               # ENABLED - dihedral group for invariance
        transpose: true          # ENABLED - dihedral group for invariance
        color_permutation: true  # ENABLED - color invariance
        translational: true      # ENABLED - translation invariance
      disable_hyperlora: true
      disable_loo: true
      disable_equivariance: false  # Allow output equiv
      disable_hpm: true
      max_tasks: null

    # Phase B: Reduced augmentation for fine-tuning (memorization phase)
    # Goal: Learn task-specific details that augmentation obscures
    phase_b:
      start_epoch: 61
      end_epoch: 80
      batch_size: 40             # Keep same batch size
      augmentation:
        rotation: true           # Keep dihedral (critical for TTA)
        flip: true               # Keep dihedral
        transpose: true          # Keep dihedral
        color_permutation: false # DISABLE - learn exact colors
        translational: false     # DISABLE - learn exact positions
      disable_hyperlora: true
      disable_loo: true
      disable_equivariance: false
      disable_hpm: true

    # Phase C: Not used in ablation
    phase_c:
      start_epoch: 999999
      batch_size: 24
      augmentation:
        rotation: false
        flip: false
        transpose: false
        color_permutation: false
        translational: false
      disable_hyperlora: true
      disable_loo: true
      disable_equivariance: false
      disable_hpm: true

    phase_readiness:
      use_metric_gating: false   # Disable gating complexity

  meta_escalation:
    enabled: false

data:
  train_path: "./data/arc-agi/data/training"
  eval_path: "./data/arc-agi/data/evaluation"
  max_grid_size: 30
  ignore_padding_in_loss: true

  # Use merged training set
  use_merged_training: true
  merged_training_path: "./data/merged_training"
  merged_dev_path: "./data/merged_training"
  merged_manifest_validation: true

  # ABLATION TRAINING SETTINGS (Jan 2026)
  # NOTE: With cache_samples_mode=rolling, samples_per_task is IGNORED.
  # Rolling cache uses: pool_size (240K) ÷ num_tasks (602) = 399 samples/task
  # The samples_per_task below is only used if cache_samples_mode='static'
  max_tasks: null
  samples_per_task: 50           # Only used with cache_samples_mode='static'
  stratified_seed: 42

  # ===========================================================================
  # OPTIMIZED DATA LOADING FOR 48 vCPU + 128GB RAM
  # ===========================================================================
  # With 48 vCPU: use ~80% for data loading (leave headroom for training)
  # With 128GB RAM: aggressive prefetching and caching is safe
  # ===========================================================================
  num_workers: 32                # was 24 - leverage more vCPUs
  pin_memory: true
  prefetch_factor: 8             # was 4 - 2x more prefetch with 128GB RAM
  persistent_workers: true
  
  # Enable sample caching with 128GB RAM
  cache_samples: true            # was false - use RAM to cache decoded samples
  cache_load_percent: 100        # Load 100% of samples into RAM cache

  # ===========================================================================
  # ROLLING REFRESH CACHE (Jan 2026)
  # ===========================================================================
  # Two-tier cache for epoch-by-epoch augmentation diversity:
  # - Tier A (decode cache): Stable decoded task representations
  # - Tier B (augmented pool): Rolling refresh with new augmentations each epoch
  #
  # Key feature: CPU generates epoch e+1 while GPU trains epoch e (async prefetch)
  # ===========================================================================
  cache_samples_mode: rolling    # 'static' | 'rolling' - use rolling for diversity
  
  rolling_cache:
    # POOL SIZE REASONING (Jan 2026):
    # - 240K samples ÷ 602 merged tasks = 399 samples/task (good diversity)
    # - 240K ÷ batch 40 = 6,000 iters/epoch (manageable training time)
    # - 80 epochs × 6K = 480K gradient updates (~12-16 hours on RTX 3090)
    # - 30% refresh = 72K new augmentations/epoch (massive diversity)
    # GPU UTILIZATION: CUDAPrefetcher overlaps CPU→GPU transfer with GPU compute
    pool_size: 240000            # BALANCED - 399 samples/task, 6K iters/epoch
    refresh_fraction: 0.30       # 30% refresh = 72K new samples per epoch
    anti_repeat_window: 5        # 5 epochs before augmentation can repeat
    prefetch_workers: 16         # 48 vCPU / 3 = 16 for async gen
    coverage_scheduling: true    # Balance dihedral/color/translation coverage
    force_augmentation: true     # FORCE augmentation even when global is off
    seed: 42                     # Base seed for reproducibility

  # AUGMENTATION ENABLED (Jan 2026 fix - was disabled causing overfitting)
  # Rolling cache needs augmentation to generate diverse samples!
  augmentation:
    enabled: true              # ENABLED - required for generalization
    rotation: true             # Dihedral group
    flip: true                 # Dihedral group
    transpose: true            # Dihedral group
    color_permutation: true    # Color invariance
    color_permutation_prob: 0.3
    translational: true        # Translation invariance
    track_augmentation: true

evaluation:
  num_guesses: 2
  
  # TTA with REDUCED complexity for ablation
  use_tta: true
  use_trm_style_eval: true
  num_augmented_views: 8         # Dihedral only
  num_color_perms: 1             # was 4 - disable color perm search
  use_voting: true
  use_inverse_aug: true
  pass_ks: [1, 2, 3]
  max_eval_tasks: 100

  canonical_train_eval:
    enabled: true
    max_tasks: 100

monitoring:
  enabled: true
  exact_match_warning: 0.10
  exact_match_critical: 0.20
  entropy_ratio_warning: 2.0
  entropy_ratio_critical: 5.0
  stop_value_warning: 0.15
  stop_value_critical: 0.25

  lora_norm_warn: 2.0
  lora_norm_critical: 5.0
  lora_norm_kill: 10.0

  nan_batches_abort: 20

  tta_consensus_warning: 0.25
  tta_consensus_critical: 0.15

  centroid_spread_warning: 2.0
  centroid_spread_critical: 0.5

  attn_max_collapse_threshold: 0.02
  attention_collapse_consecutive_threshold: 2

  collapse_backoff:
    enabled: true
    cooldown_epochs: 3
    delta_scale_factor: 0.5
    lr_factor: 0.5
    restore_rate: 0.2

logging:
  log_every: 1
  save_every: 10
  eval_every: 5                  # More frequent eval for ablation monitoring
  keep_last_n: 3
  checkpoint_dir: "checkpoints/rlan_ablation"  # Separate checkpoint dir
  log_to_file: true
  track_augmentation: true
  memory_debug_batches: 5

  use_wandb: false
  wandb_project: "rlan-arc-ablation"
  wandb_run_name: null

inference:
  temperature: 0.1
  use_best_step_selection: true  # Match training
  num_steps_override: null

  use_tta: true
  num_dihedral: 8
  num_color_perms: 1             # Match eval

  batch_size: 64                 # Increased: 128GB RAM handles larger inference batches

  loo_sanity_check: false        # HyperLoRA disabled
  loo_threshold: 0.9

  acw_enabled: false

  meta_learning:
    hyperlora:
      enable: false
    hpm:
      enable: false
    solver_context:
      enable: true
    cross_attention:
      enable: true

hardware:
  device: "cuda"
  seed: 42
  deterministic: true            # Enable for reproducible ablation
  check_nan_inf: true
  num_threads: 16                # PyTorch CPU parallelism (48 vCPU / 3 = ~16)

device:
  use_cuda: true
  mixed_precision: true
  dtype: "bfloat16"
  compile: false
