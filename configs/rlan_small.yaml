# RLAN Small Configuration - Fast Iteration (~2M params)
# For debugging, hyperparameter search, quick experiments
#
# Module Scaling Rules:
#   num_heads = hidden_dim / 64 (standard transformer scaling)
#   max_clues, num_predicates = task-specific (fixed)
#   msre/lcr params = positional encoding (fixed)
#
# PRODUCTION: RTX 3090 (24GB VRAM), 48 vCPU, 128GB RAM

model:
  type: "rlan"
  hidden_dim: 128
  num_colors: 10
  num_classes: 10   # colors 0-9 (no boundary markers needed)
  max_grid_size: 30
  
  # Task-specific (NOT capacity-dependent)
  max_clues: 6       # Ceiling slightly above ARC max (~5) for generalization
  num_predicates: 8  # Covers most spatial relationships
  num_solver_steps: 6
  
  use_act: false  # ACT less useful for small model
  dropout: 0.1
  
  # Attention heads: hidden_dim / 64 = 128 / 64 = 2
  dsc_num_heads: 2
  lcr_num_heads: 2
  
  # Positional encoding (fixed, task-independent)
  msre_encoding_dim: 32  # Sufficient for relative positions
  msre_num_freq: 8       # 2^0 to 2^7 covers 30x30 grids
  lcr_num_freq: 8        # Covers counts 0-30
  
  # Module ablation flags (all enabled by default)
  use_context_encoder: true
  use_dsc: true
  use_msre: true
  use_lcr: true
  use_sph: true

training:
  max_epochs: 500  # More epochs for small model to converge
  
  # =============================================================
  # BATCH SIZE STRATEGY FOR SINGLE RTX 3090 (24GB VRAM)
  # =============================================================
  # TRM: global_batch=768 across 8 A100 GPUs
  # RLAN target: effective_batch=320 (4GB headroom)
  # 
  # hidden_dim=128: ~125MB/sample with AMP + grads
  # Batch: 160 samples (~20GB VRAM, 4GB headroom)
  # grad_accumulation=2 → effective_batch = 160 × 2 = 320
  # =============================================================
  batch_size: 160
  grad_accumulation_steps: 2
  # effective_batch_size = 160 × 2 = 320
  
  learning_rate: 1.0e-4
  weight_decay: 0.1  # Match TRM for regularization
  gradient_clip: 1.0
  
  # Temperature schedule for Gumbel-softmax
  temperature_start: 5.0
  temperature_end: 0.1
  
  # Loss configuration - FOCAL WEIGHTED (RECOMMENDED)
  # FocalWeightedStablemaxLoss = our weights + focal modulation
  loss_mode: 'focal_weighted'
  bg_weight_cap: 2.0
  fg_weight_cap: 5.0
  
  # Focal/loss params
  focal_gamma: 2.0
  focal_alpha: 0.25
  lambda_entropy: 0.01     # REDUCED from 0.1 to match fair config
  lambda_sparsity: 0.001   # REDUCED from 0.05 to prevent DSC collapse
  lambda_predicate: 0.001  # REDUCED from 0.01 to match fair config
  lambda_curriculum: 0.01  # REDUCED from 0.1 to match fair config
  lambda_deep_supervision: 0.3  # REDUCED from 0.5 to match fair config
  
  # TRM numerical stability technique
  use_stablemax: true
  
  # =============================================================
  # CURRICULUM LEARNING: DISABLED (following TRM)
  # =============================================================
  # TRM trains on ALL tasks from epoch 1, relies on augmentation.
  # On-the-fly infinite augmentation is BETTER than curriculum.
  # =============================================================
  use_curriculum: false
  curriculum_stages: []
  
  # Optimizer and scheduler (TRM settings)
  optimizer: "adamw"
  beta1: 0.9      # Adam momentum (standard)
  beta2: 0.95     # TRM uses 0.95 instead of default 0.999 for stability
  scheduler: "onecycle"  # onecycle > cosine for ARC
  warmup_epochs: 10
  min_lr: 1.0e-6
  
  # EMA for stable evaluation
  use_ema: true
  ema_decay: 0.999

data:
  # PRODUCTION data paths - ARC-AGI directory structure
  # Alternative: Use combined JSON files (see comments below)
  train_path: "./data/arc-agi/data/training"
  eval_path: "./data/arc-agi/data/evaluation"
  # For combined JSON files, use:
  # train_path: "data/arc-agi_training_combined.json"
  # eval_path: "data/arc-agi_evaluation_combined.json"
  
  max_grid_size: 30
  
  # Data loading optimization for 48 vCPU server
  # Use 24 workers (50% of vCPU) for on-the-fly augmentation
  num_workers: 24
  pin_memory: true
  prefetch_factor: 4
  persistent_workers: true
  
  # Caching strategy (CRITICAL for competitive training)
  # cache_samples=true: Pre-cache all samples in memory (GPU becomes bottleneck)
  #   - Use for testing/debugging to eliminate data loading variance
  #   - Limited diversity (only cache_augmentations versions per task)
  # cache_samples=false: Infinite augmented samples (maximum diversity)
  #   - Use for competitive training to maximize generalization
  #   - Each epoch sees different augmentations
  cache_samples: false  # Set to true for testing, false for competitive
  cache_augmentations: 8  # Number of pre-generated augmentations when caching
  
  augmentation:
    enabled: true
    rotation: true
    flip: true
    transpose: true
    color_permutation: true  # 9! = 362,880 permutations
    translational: true      # TRM-style random offset in 30×30 canvas

evaluation:
  num_guesses: 2
  use_tta: true
  tta_rotations: [0, 90, 180, 270]
  tta_flips: [false, true]

logging:
  log_every: 10  # Log every N steps
  save_every: 10  # Save checkpoint every N epochs
  eval_every: 1  # Evaluate every N epochs
  keep_last_n: 5  # Keep last N checkpoints
  checkpoint_dir: "checkpoints/rlan_small"
  log_to_file: true  # Save all output to log file
  
  # Augmentation diversity tracking (CRITICAL for debugging)
  # Enables per-epoch logging of transformation distribution
  # Verifies on-the-fly augmentation is truly random and uniform
  track_augmentation: true
  
  # Weights & Biases (optional)
  use_wandb: false
  wandb_project: "rlan-arc"
  wandb_run_name: null  # Auto-generated if null

# Hardware settings (RTX 3090 24GB VRAM)
hardware:
  device: "cuda"
  seed: 42
  deterministic: false  # Set true for reproducibility (slower)

device:
  use_cuda: true
  mixed_precision: true  # REQUIRED for efficient RTX 3090 training
  compile: false  # torch.compile (PyTorch 2.0+)
