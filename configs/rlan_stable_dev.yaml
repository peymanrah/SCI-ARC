# RLAN STABLE DEV Configuration
# ==============================
# Development/iteration configuration for quick testing
# Uses 50 stratified tasks with 50K samples (~625 iters/epoch)
#
# USE THIS CONFIG FOR:
# - Quick iteration and debugging
# - Testing code changes
# - Stratified representative testing
#
# For production training (400K samples), use: rlan_stable_prod.yaml
#
# HARDWARE TARGET: RTX 3090 (24GB VRAM), 48 virtual CPUs, 128GB RAM
#
# USE THIS CONFIG WHEN:
# - Training new models from scratch
# - Production training runs
# - Establishing baseline performance
#
# ABLATION VALIDATION (Dec 2025):
# - Achieves 100% exact match on simple ARC tasks in 2-5 epochs
# - Grid expansion tasks (3x3→9x9) reach 100% in ~130 epochs
# - Clue regularization validated: faster convergence, stable training
# - No NaN issues with proper padding (ignore_index=-100)
#
# CONTEXT INJECTION (Dec 2025):
# - use_cross_attention_context=true with spatial_downsample=8
# - Preserves spatial structure for generalization (unlike FiLM compression)
# - Adaptive pooling: all grids → 8×8 + learned refinement + position re-encoding
# - Memory efficient: 64 tokens/pair vs 900 (14x reduction)
#
# DATA LOADING (Dec 2025):
# - 400K cached samples in CPU RAM (not GPU VRAM!)
# - num_workers=0 when cached (avoids memory duplication)
# - CUDAPrefetcher for async CPU→GPU transfer (no GPU stalls)
#
# SAMPLE COUNT (TRM comparison):
# - TRM: 1000x pre-generated augmentation = 800,000 samples
# - RLAN Phase 1: 400,000 cached (1000 per task)
# - RLAN Phase 2: INFINITE on-the-fly augmentation
#
# KEY PRINCIPLES:
# 1. No scheduler - constant learning rate (most stable)
# 2. Clue regularization ENABLED (prevents clue collapse)
# 3. Core modules: ContextEncoder (CrossAttention+Pooling), DSC, MSRE
# 4. Light auxiliary losses (entropy, sparsity, predicate)
# 5. max_grid_size=30 (official ARC-AGI maximum)
#
# TRAINING WORKFLOW:
# Phase 1: Train with cache_samples=true until convergence (memorization)
# Phase 2: Resume with cache_samples=false for diversity (generalization)
#
# CACHE AUTO-VALIDATION: Cache version and augmentation settings are
# stored in metadata. Stale caches are auto-deleted and regenerated.
# No manual deletion needed - just change settings and run training!
#
# ==========================

model:
  type: "rlan"
  hidden_dim: 256
  num_colors: 10
  num_classes: 10  # colors 0-9
  max_grid_size: 30
  
  max_clues: 7
  num_predicates: 32  # More predicates for diversity
  num_solver_steps: 7   # Increased from 5 for more iteration capacity (user request Jan 2026)
  
  use_act: false       # DISABLED
  dropout: 0.1
  
  # =============================================================
  # BEST-STEP SELECTION (Dec 2025)
  # =============================================================
  # When enabled, selects the best prediction from all solver steps:
  # - Training: Uses lowest loss (ground truth available)
  # - Inference: Uses lowest entropy (confidence-based)
  # 
  # This prevents over-iteration from hurting accuracy. If step 3
  # is better than step 6 for a simple task, we use step 3.
  # 
  # Recommended workflow:
  # 1. Train with use_best_step_selection: false (default)
  # 2. At inference, set use_best_step_selection: true to use entropy
  # 3. The model doesn't need retraining - this is inference-only
  #
  # Can also enable during training to track best-step histograms.
  use_best_step_selection: false
  
  dsc_num_heads: 4
  lcr_num_heads: 4     # Not used
  
  msre_encoding_dim: 32
  msre_num_freq: 8
  lcr_num_freq: 8      # Not used
  
  # CORE MODULES ONLY (ablation validated)
  use_context_encoder: true   # REQUIRED - learns task from demos
  use_dsc: true               # REQUIRED - dynamic spatial cluing
  use_msre: true              # REQUIRED - multi-scale relative encoding
  use_lcr: false              # DISABLED - not needed for learning
  
  # Context injection mode (CRITICAL FOR STABILITY!)
  # false = FiLM conditioning (stable, proven, compresses context to vector)
  # true = CrossAttention (experimental, high memory, can cause mode collapse)
  #
  # MIDDLE GROUND: Enable CrossAttention but with spatial downsampling
  # This gives spatial context without the massive memory cost
  use_cross_attention_context: true
  
  # Spatial downsample TARGET SIZE (not divisor!)
  # All support grids (3x3 to 30x30) are pooled to this fixed size
  # 8 = 8x8 output (64 tokens per pair, ~14x reduction from 30x30)
  # Uses learned refinement + position re-encoding after pooling
  spatial_downsample: 8       # 8x8 target - preserves objects while being memory-efficient
  use_sph: false              # DISABLED - not needed for learning
  use_learned_pos: false      # DISABLED - sinusoidal works better
  
  # Phase 2.5: Solver Cross-Attention to Support Set (Dec 2025)
  # ENABLED: Critical for generalization to unseen ARC rules
  # 
  # WHY THIS IS NEEDED:
  # - DSC injection provides GLOBAL context (baked in once at start)
  # - Solver cross-attention provides LOCAL verification at EVERY step
  # - These are COMPLEMENTARY: spatial priors + iterative verification
  #
  # MEMORY COST: ~100MB additional (trivial on 24GB GPU)
  # - Support: (B, N, 256, 8, 8) computed ONCE, reused across 6 steps
  # - Attention: O(900 × 256) per step = ~230K ops = <1ms/step
  #
  # PREVIOUS ISSUE: Mode collapse was caused by hardcoded CrossAttentionInjector
  # not by solver cross-attention. Now with spatial_downsample=8, memory is fine.
  use_solver_context: true    # ENABLED - critical for iterative verification
  solver_context_heads: 4     # Multi-head attention for diverse queries
  
  # =============================================================
  # HYPER-LORA: META-LEARNING WEIGHT ADAPTATION (Dec 2025)
  # =============================================================
  # HyperLoRA predicts task-specific LoRA weight deltas from context.
  # This allows the model to adapt its GRU gates and output head to
  # each specific task, similar to test-time training (TTT) but
  # without gradient updates at inference time.
  #
  # ARCHITECTURE:
  # - Pools context from encoded support features
  # - Predicts low-rank weight deltas (A, B matrices) for:
  #   - GRU reset gate, update gate, candidate gate
  #   - Output head (logits projection)
  # - Applied via additive LoRA: W' = W + A @ B.T
  #
  # TRAINING:
  # - Task loss: Standard cross-entropy on predictions
  # - LOO loss: Leave-one-out generalization (predict held-out pair)
  # - Equivariance loss: Consistent predictions across augmentations
  #
  # INFERENCE:
  # - Sanity check: Verify LoRA works on support set (>90%)
  # - ACW: Augmented Confidence Weighting for voting
  #
  # PRODUCTION DEFAULT: ENABLED for generalization
  # =============================================================
  use_hyperlora: true        # ENABLED - critical for task-specific adaptation
  hyperlora_rank: 8          # LoRA rank (lower = fewer params, less expressive)
  hyperlora_scaling: 1.0     # Scale factor for LoRA outputs
  hyperlora_dropout: 0.0     # Dropout on LoRA activations
  hyperlora_init_scale: 0.005  # NEAR-ZERO: Start as identity, warmup to 0.1 (was 0.1)
  hyperlora_max_norm: 1.0    # STABILITY FIX: Clamp LoRA delta L2 norm (prevents collapse)

  # =============================================================
  # HIERARCHICAL PRIMITIVE MEMORY (HPM v2) - Dec 2025
  # =============================================================
  # HPM is a multi-bank memory system for universal continual learning.
  # It stores ALL types of useful primitives (not just compositional).
  #
  # v2 IMPROVEMENTS:
  # - Sparse MoE Top-K routing (prevents mode collapse)
  # - Gated residual initialized to 0 (training stability)
  # - Static vs Dynamic bank split (unbounded memory)
  # - Load Balancing Loss (ensures all banks utilized)
  #
  # BANK TYPES:
  # - STATIC BANKS (nn.Parameter, learned during training):
  #   - Compositional: Transformations that compose (rotate, translate)
  #   - Pattern: Holistic patterns/templates (textures, shapes)
  #   - Relational: Spatial/logical relationships (above, inside)
  #   - Concept: Domain knowledge (semantics, categories)
  #
  # - DYNAMIC BANKS (KV-cache, grows with solved tasks):
  #   - Procedural: HyperLoRA latent codes for task procedures
  #   - Instance: ContextEncoder outputs for retrieval-based reasoning
  #
  # MEMORY EFFICIENCY:
  # - Gate starts at 0, so HPM contributes nothing initially
  # - Top-K routing means only k banks queried per sample
  # - Dynamic banks use CPU storage with FAISS for fast retrieval
  #
  # THREE-PHASE TRAINING WORKFLOW:
  # Phase 1: Train baseline RLAN (HPM inactive)
  # Phase 2: Activate HPM (gate learns to open gradually)
  # Phase 3: Fine-tune with HPM retrieval enabled
  #
  # SCIENTIFICALLY OPTIMIZED STAGING (Dec 2025):
  # Module activation schedule (SYNCED for stability):
  #   Epoch 0-4: Base model only (ContextEncoder + FiLM fallback)
  #   Epoch 5: HyperLoRA + SolverCrossAttention + CrossAttentionInjector (SYNCED!)
  #            → Key insight: HyperLoRA NEEDS CrossAttention features to work
  #            → Syncing prevents "zombie" HyperLoRA learning from garbage features
  #   Epoch 16: Equivariance loss (regularization after HyperLoRA warmup)
  #   Epoch 20: HPM (capacity upgrade, ~2GB memory, NO gradient shock - gated residual starts at 0)
  #   Epoch 24: LOO loss (heavy, delayed to after HPM stabilizes)
  #
  # HPM WARMUP (Dec 2025): HPM uses gated residual initialized to 0.
  # This means HPM contribution starts at 0 and grows during training - NO additional warmup needed.
  # LOO is the heavy one (N forward passes per sample). Keep batch_size reasonable.
  # =============================================================
  use_hpm: true                       # ENABLED: HPM with gated residual (safe warmup built-in)
  hpm_start_epoch: 20                 # Activate HPM at epoch 20 (after equivariance @ 16, before LOO @ 24)
  
  # Sparse MoE Routing Configuration
  hpm_top_k: 2                        # Number of banks to route to per sample
  hpm_balance_weight: 0.01            # Load balancing loss weight (prevents mode collapse)
  
  # Static Bank Configuration (AGGRESSIVE for ARC expressiveness)
  # Total primitives = 3 banks × 32 primitives × 3 levels = 288 primitives
  hpm_primitives_per_bank: 32         # Number of primitives per static bank (was 16)
  hpm_levels_per_bank: 3              # Hierarchical levels within each bank (was 2)
  hpm_use_cross_attention: true       # Use cross-attention aggregation
  
  # Dynamic Bank Configuration
  hpm_memory_size: 10000              # Max entries in dynamic banks (enough for 10x ARC)
  hpm_retrieval_k: 5                  # Number of neighbors to retrieve
  
  # Bank Selection (which banks to enable)
  # Static banks (learned primitives):
  hpm_use_compositional_bank: true    # Transformations that compose
  hpm_use_pattern_bank: true          # Holistic patterns/templates
  hpm_use_relational_bank: true       # Spatial/logical relationships
  hpm_use_concept_bank: false         # Domain knowledge (optional)
  
  # Dynamic banks (KV-cache, populated on solved tasks):
  # ENABLED: These grow during training as tasks are solved correctly
  hpm_use_procedural_bank: true       # HyperLoRA codes (requires HyperLoRA enabled)
  hpm_use_instance_bank: true         # ContextEncoder cache (retrieval-augmented reasoning)
  
  # =============================================================
  # HPM BUFFER STORAGE (Jan 2026)
  # =============================================================
  # Central path for HPM dynamic buffers. This is the SOURCE OF TRUTH
  # for training and inference to ensure consistent buffer management.
  #
  # Default: checkpoints/{config_name}/hpm/
  # 
  # During training:
  # - Buffers are saved here after each checkpoint
  # - Separate files: instance_buffer.pt, procedural_buffer.pt
  # - Includes timestamps for staleness detection
  #
  # During inference:
  # - Pass --hpm-buffer-path or use this default
  # - Health check warns if buffers are stale (>7 days old)
  # - Auto-validates buffer format and dimensions
  #
  # OVERWRITE BEHAVIOR:
  # - Training with same YAML OVERWRITES existing buffers
  # - To preserve old buffers, copy them before retraining
  # - Buffers include epoch number for versioning
  # =============================================================
  hpm_buffer_path: "checkpoints/rlan_stable_256/hpm"  # 256-dim config path
  hpm_buffer_save_frequency: 1       # Save buffers every N epochs (1=every checkpoint)
  hpm_buffer_stale_days: 7           # Warn if buffers older than N days at inference
  hpm_buffer_auto_load: true         # Auto-load from path on resume/inference
  
  # =============================================================
  # HPM MEMORY COLLECTION (Jan 2026)
  # =============================================================
  # Allow buffer population before HPM is activated (staged memory build-up).
  # This ensures buffers have content BEFORE solver-context coupling activates.
  hpm_memory_start_epoch: 0  # Start collecting solved task memories from epoch 0
  
  # =============================================================
  # HPM SOLVER-CONTEXT COUPLING (Jan 2026)
  # =============================================================
  # Injects retrieved HPM embeddings as extra tokens into solver 
  # cross-attention. This directly helps iterative correction loop
  # leverage memory for faster exact match.
  #
  # STABILITY FIX (Jan 2026): Moved to epoch 80 (was 45) to ensure:
  # 1. HPM buffers have sufficient entries (exact matches accumulated)
  # 2. HyperLoRA and base model are stable before adding more signal
  # 3. Collapse window (epochs 41-61) is safely passed
  #
  # MINIMAL DESIGN: Uses existing solver-context path + gated warmup
  # to avoid architecture changes and gradient disruption.
  # =============================================================
  hpm_solver_context_enabled: true     # Enable HPM→solver coupling
  hpm_solver_context_start_epoch: 80   # DELAYED: Activate well after collapse window + buffer buildup
  hpm_solver_context_gate_init: 0.0    # Start with zero contribution
  hpm_solver_context_gate_warmup_epochs: 20  # Slower ramp (epoch 80-100)
  hpm_solver_context_max_tokens: 4     # Reduced from 8 for stability
  hpm_solver_context_gate_max: 0.3     # Reduced from 0.5 (more conservative)
  hpm_solver_context_logit_clamp: 5.0  # Tighter clamp (was 10.0)
  hpm_solver_context_disable_on_instability: true  # Auto-disable if causes explosion

training:
  max_epochs: 200  # Enough for hard tasks (~130 epochs for 3x3→9x9 expansion)
  
  # CONSERVATIVE batch size to stay within 24GB GPU VRAM only
  # 
  # MEMORY ANALYSIS (Dec 2025 with CrossAttention + SolverCrossAttention):
  # - Base model (commit 2a50f2d, no cross-attention): batch=75 used ~22GB
  # - CrossAttentionInjector adds: ~160 MB (query+key+value+FFN activations)
  # - SolverCrossAttention adds: ~400 MB (6 steps × 67 MB each)
  # - Backward pass: ~2× forward = +1.1 GB from cross-attention
  # - TOTAL additional: ~1.7 GB from cross-attention features
  #
  # ADDITIONAL MEMORY (Dec 2025 with HyperLoRA + LOO + Equivariance):
  # - HyperLoRA: ~500 MB (5M params + activations)
  # - LOO training: N forward passes per sample (N=3-5 avg)
  # - Equivariance: 4 augmentations = 4x forward passes
  # - TOTAL: ~4-5 GB additional on top of base model
  #
  # To stay within 24GB with ~2GB headroom:
  # - Reduce batch from 64 to 16 (saves ~12 GB)
  # - Use grad_accumulation=20 for effective batch of 320 (TRM-competitive)
  #
  # MEMORY FIX (Dec 2025): OOM with batch=32 due to:
  # - LOO training: N forward passes (N=3-5) with graph accumulation
  # - Equivariance: 4 augmentation forward passes per sample
  # - Combined: up to 9x memory multiplier in worst case!
  # 
  # UPDATE (Dec 2025): With staged meta-learning, LOO/Equiv are OFF for
  # epochs 1-3, so we can use larger batches. Increased from 16 to 28.
  # At 16GB usage with batch=16, we have 8GB headroom on 24GB GPU.
  #
  # GRADIENT CHECKPOINTING ISSUE (Dec 2025):
  # Gradient checkpointing has bugs with Dict and None arguments in PyTorch.
  # DISABLED until proper fix is implemented. Use conservative batch size instead.
  #
  # BATCH SIZE STABILITY (Dec 2025):
  # The old stable config (2a50f2d) used batch=75, grad_accum=4 WITHOUT collapse.
  # The difference was NOT batch size, but:
  # 1. No HyperLoRA forward pass noise in early epochs
  # 2. No SolverCrossAttention forward pass noise in early epochs
  # 3. Simpler architecture = cleaner gradients
  #
  # =====================================================================
  # MEMORY PLANNING FOR STAGED ACTIVATION
  # =====================================================================
  # At epoch 3 (meta_learning_start_epoch), additional modules activate:
  #
  # EPOCHS 0-2 (Staged - minimal memory):
  #   - FiLM context injection: ~1MB (just γ*features+β)
  #   - Solver without cross-attention: baseline
  #   - HyperLoRA deltas = 0 (module exists but doesn't contribute)
  #
  # EPOCH 3+ (Active - additional memory per batch):
  #   - CrossAttentionInjector: +17MB (Q/K/V attention over spatial context)
  #   - SolverCrossAttention: +100MB (6 steps × 17MB per step)
  #   - HyperLoRA deltas: +5MB (small delta matrices)
  #   - TOTAL INCREASE: ~120MB per forward pass
  #
  # With memory optimizations (8-bit AdamW + gradient checkpointing), batch_size=90 is safe.
  # Previous limit was 70 without optimizations. Now we have ~40% memory reduction.
  #
  # =============================================================
  # MEMORY OPTIMIZATIONS (Dec 2025)
  # =============================================================
  # These optimizations reduce VRAM usage and speed up training.
  # All are backward-compatible with fallback to default behavior.
  #
  # 1. 8-bit AdamW: Reduces optimizer state memory by ~75%
  #    - Requires: pip install bitsandbytes
  #    - Fallback: Standard AdamW if bitsandbytes not installed
  #
  # 2. Gradient Checkpointing: Trades compute for memory
  #    - Re-computes activations during backward (slower but less memory)
  #    - Saves 20-50% of activation memory
  #    - SMART UNPACK/REPACK: Works with HyperLoRA via tensor unpacking
  #    - Memory savings apply to ALL epochs (not just early training)
  #
  # 3. torch.compile: JIT compilation for faster kernels
  #    - Requires PyTorch 2.0+
  #    - 10-30% speedup on compatible hardware
  #
  # 4. Flash Attention (SDPA): Automatic in PyTorch 2.0+
  #    - nn.MultiheadAttention uses SDPA by default
  #    - O(N) memory instead of O(N²) for attention
  # =============================================================
  use_8bit_optimizer: true    # ~75% optimizer memory savings (fallback to AdamW if bitsandbytes missing)
  gradient_checkpointing: false  # DISABLED: Known issues with Dict/None args in this codebase (Dec 2025)
                                 # Enable only after validating with 1-2 epoch smoke test
  use_torch_compile: false    # DISABLED - can cause issues with dynamic shapes in ARC grids
  torch_compile_mode: 'reduce-overhead'  # Options: 'default', 'reduce-overhead', 'max-autotune'
  
  # MEMORY FIX (Dec 2025): Reduced from 80 to 64 due to +5GB VRAM spill after HyperLoRA activation
  # Peak was ~29GB (24+5), now targeting ~23GB peak with 20% batch reduction
  # Effective batch unchanged: 50×7 = 350 (same as previous 80×4 = 320)
  # UPDATE (Dec 2025): Reduced to 50 with grad_accum=7 for prod parity (logs from epoch 34 used this)
  batch_size: 50    # Reduced for HyperLoRA activation headroom (was 64)
  grad_accumulation_steps: 7  # Increased to maintain effective_batch_size = 50 × 7 = 350
  
  learning_rate: 5.0e-4  # Works well in ablation
  weight_decay: 0.01     # Light regularization
  gradient_clip: 1.0     # Conservative
  
  # Per-module LR boosting
  # NOTE: Set to 1.0 for stable training. Higher values may cause NaN.
  # HyperLoRA uses same LR as base model (1.0x) for stability during warmup.
  # Higher multipliers can be used after warmup is complete.
  dsc_lr_multiplier: 1.0
  msre_lr_multiplier: 1.0
  hyperlora_lr_multiplier: 1.0   # CONSERVATIVE: Same as base (was 3.0, caused instability)
  
  # Temperature for DSC attention
  temperature_start: 1.0
  temperature_end: 0.5    # Don't go below 0.5
  
  # =============================================================
  # LOSS CONFIGURATION - FOCAL WEIGHTED (PHASE 1 FIX FOR BG COLLAPSE)
  # =============================================================
  # ISSUE DIAGNOSED (Dec 2025): At epoch 6 activation shock, model collapsed
  # to 91% BG accuracy with 3.3% FG accuracy due to:
  #   1. Gradient explosion (64.5x over clip threshold)
  #   2. Simultaneous module activation (HyperLoRA + SolverCrossAttn + CrossAttnInjector)
  #   3. Loss not distinguishing FG from BG sufficiently
  #
  # FIX: Use focal_weighted loss with conservative caps:
  #   - focal_gamma=1.5 (gentle focus, not too aggressive)
  #   - bg_weight_cap=1.5 (slightly boost BG but don't dominate)
  #   - fg_weight_cap=4.0 (strong FG signal to prevent collapse)
  #
  # This maintains strong gradients for minority FG classes while preventing
  # the mode collapse that occurred with pure stablemax.
  # =============================================================
  loss_mode: 'focal_weighted'  # FIX: Prevents BG collapse (was stablemax)
  bg_weight_cap: 1.0      # Lower BG pressure to avoid collapse
  fg_weight_cap: 6.0      # Stronger FG signal to counter BG-heavy pixel imbalance
  
  focal_gamma: 1.2        # Slightly gentler focal to reduce early instability
  focal_alpha: 0.75       # Not used with focal_weighted
  
  # AUXILIARY LOSSES (minimized for stability)
  lambda_entropy: 0.01          # Light attention sharpness
  lambda_sparsity: 0.5          # Required for clue regularization
  lambda_predicate: 0.01        # Light predicate diversity
  lambda_curriculum: 0.0        # OFF
  lambda_deep_supervision: 0.0  # OFF - not needed with weighted_stablemax
  lambda_act: 0.0               # OFF
  lambda_centroid_diversity: 0.3  # Strengthened per TRAINING_FIXES_DEC2025.md (was 0.1, caused collapse)
  
  # Clue regularization (ENABLED - validated stable Dec 2025)
  # Helps model learn appropriate clue count per task
  min_clues: 1.0             # Allow 1-clue simple tasks (was 2.5)
  min_clue_weight: 5.0       # Prevents collapse to 0 clues
  ponder_weight: 0.02        # Small cost per clue (encourages efficiency)
  entropy_ponder_weight: 0.02  # Encourages sharp attention
  
  # NEW (Jan 2025): Variance regularization to prevent clue count collapse
  # When std of clues_used < target_variance, applies penalty
  clue_variance_weight: 1.0   # Weight for variance regularization
  clue_target_variance: 0.5   # Target std for clue count across batch
  
  # NEW (Jan 2025): Stop logit saturation guard to prevent gradient vanishing
  # When |stop_logit| > threshold, sigmoid saturates and gradients vanish
  stop_saturation_weight: 0.5   # Weight for saturation penalty (penalizes extreme logits)
  stop_saturation_threshold: 5.0  # Threshold (sigmoid(-5)≈0.007, sigmoid(5)≈0.993)
  
  use_stablemax: true
  
  use_curriculum: false
  curriculum_stages: []
  
  optimizer: "adamw"
  beta1: 0.9
  beta2: 0.95
  
  # NO SCHEDULER - constant LR (most stable)
  scheduler: "none"
  warmup_epochs: 0
  min_lr: 5.0e-4  # Same as learning_rate
  
  # =============================================================
  # EMA CONFIGURATION (Dec 2025)
  # =============================================================
  # For 200-epoch training (~50K+ steps), EMA CAN be beneficial.
  # However, decay=0.999 creates ~1000-step lag which may hide
  # recent learning. Consider decay=0.995 (200-step window) or
  # 0.99 (100-step window) for faster convergence tracking.
  # Currently DISABLED to simplify debugging during development.
  # Enable for production runs where evaluation stability matters.
  # =============================================================
  use_ema: false
  ema_decay: 0.995  # Faster-tracking decay (was 0.999) for use when enabled
  
  # =============================================================
  # STAGED META-LEARNING (Dec 2025 - SCIENTIFICALLY ORDERED ACTIVATION)
  # =============================================================
  # ROOT CAUSE ANALYSIS (Dec 2025 training log):
  # At epoch 6, ALL three modules activated SIMULTANEOUSLY causing:
  #   - 64.5x gradient explosion (threshold=1.0, actual=64.5)
  #   - Forward-path discontinuity (outputs suddenly different)
  #   - BG collapse: FG acc dropped to 3.3%, BG stayed at 91%
  #
  # SOLUTION: Staggered activation with scientifically-ordered dependencies:
  #
  # PHASE 2 (Epoch 5): Context Path FIRST
  #   - CrossAttentionInjector + SolverCrossAttention activate TOGETHER
  #   - These provide meaningful spatial context features
  #   - Temporarily reduce LR by 2x for 2 epochs to smooth transition
  #
  # PHASE 3 (Epoch 8): HyperLoRA AFTER context path is stable
  #   - Now has meaningful features to predict LoRA deltas from
  #   - Uses near-zero init (hyperlora_init_scale=0.005) to start as identity
  #   - Gradually warms up to full scale over hyperlora_warmup_epochs
  #
  # PHASE 4 (Epoch 12): Equivariance after HyperLoRA stabilizes
  #   - Light regularization (weight=0.01 initially, not 0.05)
  #
  # PHASE 5 (Epoch 18): LOO LAST after everything stable
  #   - Heaviest memory/compute (N forward passes per sample)
  #   - Gentle weight (0.05 initially, not 0.1)
  #
  # BACKOFF RULES: Implemented in train_rlan.py
  #   - If grad_norm > 10x clip threshold → reduce LR by 2x for next epoch
  #   - If 3 consecutive NaN batches → halve newest meta-loss weight
  # =============================================================
  
  # Context path activates FIRST (Phase 2)
  solver_context_start_epoch: 5   # SolverCrossAttention
  cross_attention_start_epoch: 12 # CrossAttentionInjector (DELAYED; FiLM fallback is stabler early)
  
  # HyperLoRA activates AFTER context path is stable (Phase 3)
  meta_learning_start_epoch: 8    # HyperLoRA (3 epochs after context path)
  
  # HyperLoRA warmup: start near-zero and ramp up over N epochs
  hyperlora_warmup_epochs: 8      # Slower warmup to reduce activation shock
  hyperlora_warmup_start_scale: 0.005  # Near-identity at activation
  hyperlora_warmup_end_scale: 0.05     # Lower end-scale to reduce BG-bias amplification risk
  
  # LR reduction at activation epochs (prevents gradient shock)
  activation_lr_reduction: 0.5    # Multiply LR by 0.5 at activation epochs
  activation_lr_recovery_epochs: 2  # Recover to full LR over 2 epochs
  
  # Gradient explosion backoff (safety mechanism)
  grad_explosion_threshold: 10.0  # If grad_norm > threshold * clip, trigger backoff
  grad_explosion_lr_reduction: 0.5  # Multiply LR by this factor on explosion
  grad_explosion_cooldown_epochs: 2  # Wait this many epochs before restoring LR
  
  # =============================================================
  # STAGED LOSS ACTIVATION (Dec 2025 - SCIENTIFICALLY ORDERED)
  # =============================================================
  # Enable losses at DIFFERENT epochs based on training needs:
  # - Equivariance: Epoch 12 (AFTER HyperLoRA stabilizes at 8+4=12)
  # - LOO: Epoch 18 (LAST - heaviest, requires stable everything)
  #
  # This addresses the 60x eval entropy explosion by enforcing:
  # 1. Dihedral invariance after HyperLoRA warmup (fixes 13% consensus)
  # 2. Few-shot generalization last (fixes train→eval gap)
  # =============================================================
  
  # =============================================================
  # LOO TRAINING: LEAVE-ONE-OUT META-LEARNING LOSS (PHASE 5 - LAST)
  # =============================================================
  # Teaches the model to generalize from N-1 examples to the Nth.
  # Only active when use_hyperlora: true in model section.
  #
  # DELAYED ACTIVATION (Dec 2025): Activates at epoch 18
  # This is AFTER:
  #   - Context path stable (epoch 5+)
  #   - HyperLoRA warmup complete (epoch 8+4=12)
  #   - Equivariance regularizing (epoch 12+)
  #
  # MEMORY FIX: Uses reduced batch size automatically.
  # =============================================================
  loo_training:
    enabled: true          # ENABLED: Auto-activates at start_epoch
    start_epoch: 24        # PHASE 5: LOO activates LAST (after HyperLoRA warmup + equivariance)
    loss_weight: 0.05      # GENTLER start (was 0.1) - prevents gradient shock
    min_pairs_for_loo: 2   # Need at least 2 pairs to hold one out
    max_loo_pairs: 4       # CAP at 4 passes to prevent memory explosion
    
    # ADAPTIVE BATCH SIZE REDUCTION (Dec 2025)
    # When LOO activates, batch size is reduced to fit N forward passes in VRAM.
    # Formula: loo_batch = floor(base_batch / batch_reduction_divisor)
    #
    # OBSERVED (24GB GPU): 10GB peak at LOO activation with batch=18
    # This means we have 14GB headroom → can safely use ~2x larger batch.
    #
    # Divisor=2.0: 80/2 = 40 batch (vs old formula: 80/4-2 = 18)
    # adjust_grad_accumulation=true: auto-increase grad_accum to maintain
    # effective batch size (80×4=320 → 40×8=320)
    batch_reduction_divisor: 2.0    # Divide base batch by this (was ~4.4 implicitly)
    min_batch_size: 8               # Floor for safety (was 4)
    adjust_grad_accumulation: true  # Auto-adjust grad_accum to maintain effective batch
  
  # =============================================================
  # AUGMENTATION EQUIVARIANCE LOSS (ENABLED - Jan 2026)
  # =============================================================
  # CRITICAL FOR TTA CONSENSUS (Bug Fix Dec 2025):
  # Equivariance loss ensures HyperLoRA predicts consistent weights for
  # rotated/flipped versions of the same task, improving TTA voting.
  #
  # Without this, TTA consensus was stuck at 0% because model predicted
  # different weights for each augmented view.
  #
  # STE FIX APPLIED: DSC now uses proper identity gradient (raw - raw.detach()
  # + squashed.detach()) so gradients flow correctly through equivariance.
  # =============================================================
  equivariance_training:
    enabled: true          # ENABLED: Critical for TTA consensus (bug fix)
    start_epoch: 16        # PHASE 4: After HyperLoRA warmup (8 + 8 = 16)
    loss_weight: 0.01      # GENTLER start (was 0.05)
    num_augmentations: 4   # 256-dim can handle 4 (512 uses 2 for memory)
  
  # =============================================================
  # META-LEARNING STRATEGY SUMMARY (COMPLEMENTARY LOSSES)
  # =============================================================
  # The three meta-learning losses work together without conflict:
  #
  # 1. TASK LOSS (weight=1.0): Standard cross-entropy on predictions
  #    - Objective: Predict correct output grids
  #    - Target: Direct task performance
  #
  # 2. LOO LOSS (weight=0.2): Leave-one-out generalization
  #    - Objective: Predict Nth pair from N-1 context
  #    - Target: Few-shot generalization within a task
  #    - Complementary: Doesn't conflict with task loss - both want correct predictions
  #                     LOO adds EXPLICIT generalization signal during training
  #
  # 3. EQUIVARIANCE LOSS (weight=0.05): Augmentation consistency
  #    - Objective: Same LoRA weights for rotated/flipped versions
  #    - Target: Transform-invariant representations
  #    - Complementary: Acts as REGULARIZER - prevents overfitting to specific orientations
  #                     Light weight (0.05) ensures it doesn't dominate task learning
  #
  # MATHEMATICAL RELATIONSHIP:
  #   L_total = L_task + 0.2 * L_loo + 0.05 * L_equiv
  #
  # WHY THIS IS OPTIMAL (NOT OVERKILL):
  # - Task loss drives direct performance
  # - LOO loss adds explicit generalization pressure (meta-learning signal)
  # - Equivariance loss adds regularization (prevents orientation bias)
  # - Weights are calibrated so task loss dominates, meta-losses support
  #
  # HYPERLORA LR MULTIPLIER (10x):
  # - HyperLoRA needs faster learning to adapt its meta-prediction
  # - Base model has stable pretrained weights, HyperLoRA learns from scratch
  # - 10x ensures HyperLoRA doesn't become a bottleneck
  # =============================================================

  # =============================================================
  # META ESCALATION: LATE-PHASE STRENGTH INCREASE (Dec 2025)
  # =============================================================
  # After the baseline training is stable (epochs 0-24), we can safely
  # increase meta-learning signal strength to improve AGI-ARC generalization.
  #
  # PROBLEM SOLVED:
  # Current weights (HyperLoRA=0.1, equiv=0.01, loo=0.05) are "safe but capped":
  #   ∇L = ∇L_task + λ_equiv*∇L_equiv + λ_loo*∇L_loo
  # If λ values stay too small, meta terms never become impactful enough
  # to carve the invariances/generalization needed for never-seen rules.
  #
  # SOLUTION:
  # Stability-gated late-phase escalation:
  # - Only increase weights after training is demonstrably stable
  # - Automatic pause on any instability (NaN, grad explosion)
  # - Asymmetric backoff/recovery: fast backoff, slow recovery
  #
  # ONE-RUN PROOF: This creates a within-run "A/B test":
  # - Phase A (epochs 0-24): baseline behavior (meta signals weak, stable)
  # - Phase B (epochs 25-36): gated ramp (meta signals increase if stable)
  # - Phase C (epochs 37+): hold at target (observe AGI-ARC improvement)
  # =============================================================
  meta_escalation:
    enabled: true                    # Set to false to disable (baseline behavior)
    
    # =============================================================
    # META LOSS CAPPING (Jan 2026 FIX)
    # =============================================================
    # Caps the combined (LOO + equiv + HPM) loss contribution to prevent
    # meta-learning from overwhelming the primary task loss.
    # Training logs showed 40.9% meta contribution, causing weak task learning.
    # Target: Keep meta ≤25% of total loss.
    meta_loss_cap_ratio: 0.25        # Max ratio of meta-loss to total loss
    meta_loss_cap_enabled: true      # Set to false to disable capping
    
    # When to start increasing meta-learning strength
    start_epoch: 25                  # After LOO is stable (18 + 7 epochs)
    ramp_epochs: 12                  # Gradual increase over 12 epochs
    schedule: linear                 # linear | cosine
    
    # Target values (conservative but impactful)
    # These are what we ramp TOWARD, not what we jump to
    # UPDATE (Dec 2025): Reduced targets for stability based on epoch 34 logs
    targets:
      hyperlora_delta_scale: 0.20    # Current: 0.1 → Target: 0.2 (2x increase, was 0.3)
      equiv_loss_weight: 0.03        # Current: 0.01 → Target: 0.03 (3x increase, was 0.05)
      loo_loss_weight: 0.08          # Current: 0.05 → Target: 0.08 (1.6x increase, was 0.10)
    
    # Stability gating (controls whether escalation pauses on instability)
    # When true: escalation pauses if previous epoch had instability events
    # When false: always follow schedule regardless of instability (for ablations)
    require_stability: true
    
    # Per-epoch thresholds (gating checks previous epoch's counters)
    # These are NOT rolling windows - counters reset each epoch
    # UPDATE (Dec 2025): Allow 1-2 NaN batches per epoch (observed in epoch 34 logs)
    # Setting to 0 was too strict - any random NaN paused escalation permanently
    max_grad_explosion_events_per_epoch: 2    # Allow 2 grad explosions (was 0)
    max_lr_backoff_events_per_epoch: 2        # Allow 2 LR backoffs (was 0)
    max_consecutive_nan_streak_per_epoch: 3   # Allow streak of 3 (was 0)
    
    # Recovery after backoff (prevents permanent suppression)
    # If NaN/explosion causes weight reduction, this restores toward target once stable
    recovery_enabled: true
    recovery_step_per_window: 0.05   # +5% of gap per stable window
    
    # Logging
    log_every_epoch: true            # Log escalation state every epoch
    
    # =============================================================
    # LATE-PHASE STABILIZATION (Jan 2026)
    # =============================================================
    # After epoch 50, training is in "precision mode" chasing exact match.
    # We tighten stability gates and optionally decay targets to prevent
    # catastrophic forgetting of already-solved tasks.
    late_phase:
      enabled: true
      start_epoch: 50                # When to switch to late-phase mode
      
      # Stricter stability gates (any instability pauses escalation)
      max_grad_explosion_events: 0   # Zero tolerance in late phase
      max_lr_backoff_events: 0       # Zero tolerance in late phase
      max_attention_collapse_events: 0  # Zero tolerance for collapse
      
      # Target decay (optional - reduces meta-learning pressure late)
      decay_targets: false           # If true, decay toward lower values
      target_decay_factor: 0.8       # Multiply targets by this in late phase
      
      # LR decay policy for late phase
      lr_decay:
        enabled: true
        start_epoch: 50              # Begin LR decay (same as late phase)
        end_epoch: 200               # End of decay schedule
        decay_factor: 0.1            # Final LR = initial * decay_factor
        schedule: cosine             # cosine | linear | step

data:
  train_path: "./data/arc-agi/data/training"
  eval_path: "./data/arc-agi/data/evaluation"
  
  max_grid_size: 30
  
  # CRITICAL: Ignore padding in loss
  ignore_padding_in_loss: true
  
  # ==========================================================================
  # MERGED TRAINING SET (ARC-AGI-1 + ARC-AGI-2) - January 2026
  # ==========================================================================
  # When enabled, uses the merged training set created by:
  #   python scripts/build_merged_training_set.py
  #
  # This provides MORE RULES for training (1000+ unique tasks vs 400)
  # while ensuring NO DATA LEAKAGE to ARC-AGI-1/2 evaluation sets.
  #
  # WHAT IT DOES:
  # - Loads tasks from merged_train_manifest.jsonl instead of train_path
  # - Uses canonical dedup to exclude any task matching eval sets
  # - Provides dev set (merged_dev_manifest.jsonl) for tuning/stopping
  #
  # REQUIREMENTS:
  # 1. Run: python scripts/build_merged_training_set.py
  # 2. Ensure merged_training_path points to output directory
  # 3. Set use_merged_training: true
  #
  # WHEN FALSE (default): Uses existing train_path (ARC-AGI-1 only)
  # WHEN TRUE: Uses merged training set from merged_training_path
  #
  # This is ADDITIVE - does NOT change existing data loading behavior
  # when set to false. The original codebase remains untouched.
  # ==========================================================================
  use_merged_training: false  # Set to true to use ARC-AGI-1 + ARC-AGI-2 merged set
  merged_training_path: "./data/merged_training"  # Output from build_merged_training_set.py
  merged_dev_path: "./data/merged_training"       # Same dir, uses merged_dev_manifest.jsonl
  merged_manifest_validation: true  # Validate cache/buffer manifest hashes match
  
  # ==========================================================================
  # DATA SAMPLING CONFIGURATION
  # ==========================================================================
  # 
  # SIMPLE MENTAL MODEL:
  # 1. max_tasks: How many TASKS to use (STRATIFIED sampling from 400)
  # 2. cache_samples: Whether to use pre-cached augmentations
  # 3. cache_load_percent: How much of cache to load into RAM
  #
  # BUCKETED BATCHING: ALWAYS ON (non-negotiable for memory efficiency)
  # Groups same-sized grids together to minimize padding waste.
  #
  # ==========================================================================
  # VALID CONFIGURATIONS (what actually works)
  # ==========================================================================
  #
  # | Scenario           | max_tasks | cache_samples | cache_load_percent | Sampling           |
  # |--------------------|-----------|---------------|--------------------|--------------------|
  # | Full training      | null      | true          | 100                | All 400K samples   |
  # | Memory-limited     | null      | true          | 6                  | RANDOM 6% (~24K)   |
  # | Stratified test    | 50        | true          | 100                | STRATIFIED 50 tasks|
  # | Quick smoke test   | 25        | false         | (ignored)          | STRATIFIED 25 tasks|
  # | On-the-fly train   | null      | false         | (ignored)          | All 400, infinite  |
  #
  # WHAT'S STRATIFIED vs RANDOM:
  # - max_tasks → STRATIFIED: Covers all 18 strata (grid sizes × shot counts × size changes)
  # - cache_load_percent → RANDOM: Just loads first N% of shuffled chunks (fast but random)
  #
  # INVALID/WASTEFUL COMBINATIONS:
  # - max_tasks=100 + cache_load_percent=6: Loads 24K samples, filters to ~6K → wasteful!
  #   Better: Use max_tasks=100 + cache_load_percent=100 (gets all 100K samples for 100 tasks)
  #
  # RECOMMENDED CONFIGURATIONS:
  # | Use Case          | Config                                      | Expected Samples |
  # |-------------------|---------------------------------------------|------------------|
  # | Quick smoke test  | max_tasks=25, cache_samples=false           | 25 tasks, fresh  |
  # | Dev iteration     | max_tasks=50, cache_samples=true, pct=100   | ~50K samples     |
  # | Full validation   | max_tasks=100, cache_samples=true, pct=100  | ~100K samples    |
  # | Production train  | max_tasks=null, cache_samples=true, pct=100 | 400K samples     |
  # | Memory-constrained| max_tasks=null, cache_samples=true, pct=10  | ~40K samples     |
  #
  # ==========================================================================
  # CACHE AUTO-VALIDATION (Dec 2025)
  # ==========================================================================
  # The code automatically validates cached data before loading:
  # - Checks cache version (must match current code version)
  # - Checks augmentation settings (augment, color_perm, translational, max_size)
  # - If mismatch detected: AUTOMATICALLY deletes old cache and regenerates!
  #
  # This means you DON'T need to manually delete caches when settings change.
  # The code is smart enough to detect stale caches and regenerate them.
  # ==========================================================================
  
  # ==========================================================================
  # CURRENT MODE: DEV ITERATION (change for production)
  # ==========================================================================
  # For production training, change to:
  #   max_tasks: null
  #   cache_load_percent: 100
  # ==========================================================================
  # UPDATED (Dec 2025): Use ALL 400 tasks with 50 samples each = 20K total
  # This improves generalization (more task diversity) while keeping training fast.
  # Previous config: 50 tasks x 1000 = 50K (caused 3% TTA consensus, 50x eval entropy gap)
  # New config: 400 tasks x 50 = 20K (better task diversity for generalization)
  max_tasks: null  # ALL 400 TASKS for better generalization
  samples_per_task: 50  # 50 samples per task (was 1000) for faster iteration
  stratified_seed: 42  # Seed for reproducible stratified sampling
  
  # Data loading configuration
  # NOTE: When cache_samples=true, num_workers is automatically set to 0
  # to avoid duplicating the 400K cache across worker processes.
  # GPU stalls are prevented by CUDAPrefetcher (async CPU→GPU transfer).
  num_workers: 12      # Used when cache_samples=false (on-the-fly augmentation)
  pin_memory: true     # Enables async GPU transfer (always beneficial)
  prefetch_factor: 6   # Used when num_workers > 0
  persistent_workers: true
  
  # ==========================================================================
  # CACHING + AUGMENTATION STRATEGY (TRM-INSPIRED)
  # ==========================================================================
  # TRM uses 1000x pre-generated augmentation per task with:
  #   - 8 dihedral transforms
  #   - Random color permutation (color 0 fixed, 1-9 shuffled)
  #   - Translational offset
  #
  # We do the same with 400K cached samples:
  #   - Each sample has RANDOM augmentation baked in
  #   - color_permutation_prob: 1.0 = EVERY sample color-permuted (like TRM!)
  #   - This naturally balances class distribution over training
  #
  # CACHE AUTO-VALIDATION: Code detects stale caches and auto-regenerates!
  # No need to manually delete caches when changing augmentation settings.
  # Version is tracked INSIDE the cache metadata, not in the filename.
  # ==========================================================================
  
  cache_samples: true
  
  # ==========================================================================
  # num_cached_samples CALCULATION (Jan 2026)
  # ==========================================================================
  # COMPUTED DYNAMICALLY based on task count and samples_per_task:
  #   - use_merged_training=false: 400 tasks × samples_per_task
  #   - use_merged_training=true:  602 tasks × samples_per_task
  #
  # Set to 'auto' to let the code compute it, or specify explicit value.
  # When 'auto', the code uses: detected_tasks × samples_per_task
  #
  # FORMULA: num_cached_samples = task_count × samples_per_task
  #   - Default (400 tasks × 50) = 20,000
  #   - Merged (602 tasks × 50) = 30,100
  # ==========================================================================
  num_cached_samples: auto  # Computed as task_count × samples_per_task
  
  # Cache path includes dataset type for clarity
  # Code will auto-update path based on use_merged_training flag
  cache_path: "./cache/rlan_stable_agi1_400tasks.pkl"  # Updated if merged training enabled
  
  # See max_tasks vs cache_load_percent explanation above!
  # This controls RAM usage by loading a random subset of cached samples.
  # For representative testing, prefer max_tasks (stratified) over this (random).
  cache_load_percent: 100  # DEV ITERATION: Load all samples for stratified tasks
                           # Use 6-20 for quick testing (~20s load)
                           # For production with max_tasks=null, use 100
  
  # ==========================================================================
  # BUCKETED BATCHING - NON-NEGOTIABLE (ALWAYS ENABLED FOR MEMORY EFFICIENCY)
  # ==========================================================================
  # Groups samples by grid size to minimize padding waste.
  # Without this: One 30x30 grid forces ALL 80 batch samples to 30x30 = 4x memory waste!
  # With this: Each batch has similar-sized grids → minimal padding.
  #
  # This applies to ALL configurations:
  # - Full training (400K samples)
  # - Stratified testing (max_tasks=50)
  # - On-the-fly augmentation (cache_samples=false)
  # - Memory-limited runs (cache_load_percent < 100)
  #
  # Batch ORDER is shuffled each epoch to avoid learning bias.
  # ==========================================================================
  bucketed_batching: true  # WARNING: Setting to false wastes memory!
  bucket_boundaries: [10, 15, 20, 25]  # Creates 5 buckets: ≤10, 11-15, 16-20, 21-25, >25
  
  augmentation:
    enabled: true
    rotation: true
    flip: true
    transpose: true
    color_permutation: true
    color_permutation_prob: 0.5  # Reduce early instability; can increase later by resuming
    translational: true
    track_augmentation: true  # REQUIRED for inverse aug at eval

# =============================================================
# EVALUATION - TRM-STYLE WITH INVERSE AUGMENTATION
# =============================================================
# Critical for measuring true generalization:
# 1. Apply augmentations to test inputs
# 2. Get model predictions
# 3. REVERSE augmentations to canonical space
# 4. Compare with original ground truth
# 5. Use voting across predictions for robustness
# =============================================================
evaluation:
  num_guesses: 2
  use_tta: true
  tta_rotations: [0, 90, 180, 270]
  tta_flips: [false, true]
  
  # TRM-style evaluation settings
  use_trm_style_eval: true
  num_augmented_views: 8  # 8 dihedral transforms (D4 group)
  num_color_perms: 4      # 4 color permutations per dihedral = 32 total views
  use_voting: true        # Aggregate predictions
  use_inverse_aug: true   # CRITICAL: Undo augmentations before comparison
  pass_ks: [1, 2, 3]      # Report Pass@K: is answer in top K voted predictions?
  max_eval_tasks: 100     # Number of eval tasks to run TTA on (100 for full eval)

# =============================================================
# GAP MONITORING - EARLY WARNING FOR GENERALIZATION ISSUES
# =============================================================
monitoring:
  enabled: true
  exact_match_warning: 0.10   # Warn if train-eval gap > 10%
  exact_match_critical: 0.20  # Critical if gap > 20%
  entropy_ratio_warning: 2.0  # Warn if eval entropy > 2x train
  entropy_ratio_critical: 5.0 # Critical if > 5x
  stop_value_warning: 0.15    # Warn if stop value gap > 0.15
  stop_value_critical: 0.25   # Critical if > 0.25
  
  # =============================================================
  # P0.3: LoRA NORM GOVERNOR THRESHOLDS (Dec 2025)
  # =============================================================
  # Based on 821b111 failure analysis: LoRA norms reached 62+ before NaN
  # These thresholds trigger warnings and auto-backoff to prevent explosion
  #
  # METRIC FIX (Dec 2025): Now computes MEAN PER-SAMPLE norm, excluding
  # the 'context' auxiliary tensor. Old metric was batch-size dependent
  # and included context vector (L2 ~143 for batch 80, hidden 256).
  #
  # NEW RATIONALE (per-sample mean, actual LoRA deltas only):
  # - With delta_scale=0.005 (warmup start): expect ~0.001-0.05
  # - With delta_scale=0.1 (warmup end): expect ~0.05-0.5
  # - With delta_scale=0.3 (escalation target): expect ~0.1-1.5
  # - Warning zone: > 2.0 (unusually large adaptations)
  # - Critical zone: > 5.0 (likely gradient issues)
  # - Kill zone: > 10.0 (model weights likely corrupted)
  lora_norm_warn: 2.0         # Log warning if mean per-sample LoRA norm > 2.0
  lora_norm_critical: 5.0     # Log critical + consider LR reduction if > 5.0
  lora_norm_kill: 10.0        # Model is exploding - weights corrupted
  
  # =============================================================
  # P0.4: NaN BATCH ABORT THRESHOLD
  # =============================================================
  # If NaN batches exceed this count in an epoch, abort and restart
  # with smaller learning rate or different random seed
  nan_batches_abort: 20       # Abort epoch if > 20 NaN batches (10% of typical 200 batches)
  
  # =============================================================
  # TTA CONSENSUS THRESHOLDS
  # =============================================================
  # Based on 821b111 failure: 9% TTA consensus indicates broken equivariance
  tta_consensus_warning: 0.25  # Warn if winner votes < 25% of total views
  tta_consensus_critical: 0.15 # Critical if < 15% (random would be 3%)
  
  # =============================================================
  # CENTROID DIVERSITY THRESHOLDS
  # =============================================================
  # Based on 821b111 failure: centroid spread 0.19 indicates collapsed attention
  centroid_spread_warning: 2.0   # Warn if spread < 2.0
  centroid_spread_critical: 0.5  # Critical if spread < 0.5 (complete collapse)
  
  # =============================================================
  # ATTENTION COLLAPSE DETECTION (Jan 2026)
  # =============================================================
  # Detects when attention becomes near-uniform (diffuse), which precedes
  # accuracy cliff. Uniform over 30x30 grid ≈ 0.0011; healthy sharp ≈ 0.1+
  attn_max_collapse_threshold: 0.02          # Batch flagged if attn_max_mean < this
  attention_collapse_consecutive_threshold: 2 # Trigger backoff after N consecutive
  
  # =============================================================
  # ATTENTION COLLAPSE BACKOFF POLICY (Jan 2026)
  # =============================================================
  # When collapse detected, actively reduce instability sources
  # (not just pause escalation). This reverses the collapse chain.
  collapse_backoff:
    enabled: true
    cooldown_epochs: 3              # How long to apply backoff
    delta_scale_factor: 0.5         # Multiply HyperLoRA delta_scale by this
    lr_factor: 0.5                  # Multiply LR by this during cooldown
    restore_rate: 0.2               # Restore 20% of gap per stable epoch

logging:
  log_every: 1
  save_every: 10
  eval_every: 10  # Every 10 epochs (was 1) - TTA on 100 tasks is expensive
  keep_last_n: 3
  checkpoint_dir: "checkpoints/rlan_stable_256"  # Explicit 256-dim path
  log_to_file: true
  track_augmentation: true  # Enable to verify augmentation diversity
  memory_debug_batches: 5   # Log detailed memory checkpoints for first N batches per epoch
  
  use_wandb: false
  wandb_project: "rlan-arc"
  wandb_run_name: null

# =============================================================
# INFERENCE SETTINGS (Dec 2025)
# =============================================================
# These settings can be overridden via command-line arguments:
#   --use-best-step, --no-best-step, --num-steps <N>
#
# Example commands:
#   # Use entropy-based best step + 10 solver iterations
#   python evaluate_rlan.py --checkpoint best.pt --use-best-step --num-steps 10
#
#   # Compare 6 vs 10 steps
#   python evaluate_rlan.py --checkpoint best.pt --num-steps 6
#   python evaluate_rlan.py --checkpoint best.pt --num-steps 10
#
inference:
  # Temperature for softmax (lower = sharper predictions)
  # Should match training end temperature for consistency
  temperature: 0.1
  
  # Best-step selection: pick step with lowest entropy (most confident)
  # - false: Always use last step (default, matches training)
  # - true: Use entropy-based selection (may improve accuracy)
  use_best_step_selection: false
  
  # Solver steps override (null = use model's trained default)
  # Can test with more steps than trained (e.g., train=6, infer=10)
  num_steps_override: null
  
  # Test-Time Augmentation (matches training TTA for consistent results)
  # With 8 dihedral x 4 color perms = 32 views, same as evaluate_trm_style()
  use_tta: true
  num_dihedral: 8      # Full D4 group (identity + 3 rotations + 4 reflections)
  num_color_perms: 4   # Color permutations per dihedral (keeps color 0 fixed)
  
  # Batch size for evaluation (can be larger than training)
  batch_size: 32
  
  # =============================================================
  # HYPER-LORA INFERENCE SETTINGS
  # =============================================================
  # LOO Sanity Check: Verify predicted LoRA weights work on support set
  # If accuracy on training pairs is below threshold, fall back to
  # standard forward pass without LoRA (safety mechanism).
  # =============================================================
  loo_sanity_check: true
  loo_threshold: 0.9    # Require 90% accuracy on support set
  
  # =============================================================
  # ACW: AUGMENTED CONFIDENCE WEIGHTING
  # =============================================================
  # Instead of simple voting, weight each augmented prediction by
  # how consistent it is with other augmentations. More consistent
  # predictions get higher weight in the final vote.
  #
  # Consistency = 1 - mean(pairwise_disagreement_with_other_views)
  # =============================================================
  acw_enabled: true
  acw_num_views: 8     # Number of augmented views to consider

  # =============================================================
  # INFERENCE-TIME META-LEARNING FLAGS (Jan 2026)
  # =============================================================
  # These flags control meta-learning modules at eval/inference time.
  # CRITICAL: These are SEPARATE from training staging epochs!
  #
  # At inference, we always WANT meta-learning active (if trained).
  # These settings allow:
  # 1. Explicit control over what's active at eval
  # 2. Graceful handling if buffers are empty/invalid
  # 3. Fallback behavior for partial checkpoints
  #
  # Each module has:
  # - enable: Whether to try activating (True = attempt, False = force off)
  # - require_valid_state: If True, only activate if module has valid state
  #   (e.g., HPM needs non-empty buffers, HyperLoRA needs trained weights)
  # =============================================================
  meta_learning:
    # HyperLoRA: Task-specific weight adaptation
    hyperlora:
      enable: true                # Activate HyperLoRA at inference
      require_trained: true       # Only if hyperlora params have non-zero weights
      fallback_on_failure: true   # Fall back to base model if LoRA fails sanity check
    
    # HPM: Hierarchical Primitive Memory
    hpm:
      enable: true                # Activate HPM at inference
      require_nonempty_buffers: true  # Only if dynamic buffers have entries
      min_buffer_entries: 1       # Minimum entries needed to activate
      use_static_banks: true      # Always use static (learned) banks
      use_dynamic_banks: true     # Use dynamic (instance/procedural) if available
    
    # Solver Cross-Attention: Context injection at each step
    solver_context:
      enable: true                # Activate solver cross-attention at inference
    
    # Cross-Attention Injector: Spatial context injection  
    cross_attention:
      enable: true                # Activate cross-attention injector at inference

hardware:
  device: "cuda"
  seed: 42
  deterministic: false  # Set true for reproducibility (slower)
  check_nan_inf: true   # Enable NaN/Inf detection in training loop

device:
  use_cuda: true
  mixed_precision: true
  dtype: "bfloat16"
  compile: false
