# RLAN Fair Comparison Configuration
# ==================================
# Matches TRM capacity (~10M params) for fair comparison
# 
# TRM Config (from TinyRecursiveModels/config/arch/trm.yaml):
#   - hidden_size: 512
#   - num_heads: 8
#   - L_layers: 2, L_cycles: 6, H_cycles: 3 (total 18 reasoning passes)
#   - expansion: 4 (SwiGLU)
#   - global_batch_size: 768 (requires 8+ A100 GPUs!)
#   - epochs: 100,000
#
# RLAN Advantages over TRM (same parameter count):
#   1. Preserves 2D spatial structure (TRM flattens to 1D)
#   2. ContextEncoder learns task rules (TRM memorizes via puzzle_emb)
#   3. DSC discovers spatial anchors (TRM has no spatial inductive bias)
#   4. MSRE computes relative coordinates (TRM treats positions as tokens)
#   5. LCR provides soft counting (TRM cannot count)
#   6. SPH extracts symbolic predicates (TRM has no symbolic layer)
#   7. Runs on single RTX 3090 (TRM needs GPU cluster)
#   8. 1K epochs with infinite augmentation (TRM needs 100K epochs)
#
# Optimized for RTX 3090 (24GB VRAM), single GPU training

model:
  type: "rlan"
  hidden_dim: 512   # Match TRM's hidden_size=512
  num_colors: 10
  num_classes: 11   # 10 colors + background
  max_grid_size: 30
  max_clues: 5      # Spatial anchors (TRM has none)
  num_predicates: 8 # Symbolic predicates (TRM has none)
  num_solver_steps: 6  # Comparable to TRM's L_cycles=6
  use_act: true  # Enable Adaptive Computation Time
  dropout: 0.1
  
  # Attention heads - match TRM's num_heads=8
  dsc_num_heads: 8
  lcr_num_heads: 8
  
  # MSRE parameters (unique to RLAN)
  msre_encoding_dim: 64
  msre_num_freq: 8
  
  # LCR parameters (unique to RLAN)
  lcr_num_freq: 8

training:
  # TRM trains for 100K epochs on 8+ GPUs
  # RLAN achieves comparable results in 1K epochs on 1 GPU
  # (thanks to infinite augmentation + structured inductive biases)
  max_epochs: 1000
  
  # TRM: global_batch=768 across 8 GPUs = 96 per GPU
  # RLAN: batch=48, grad_accum=4 = effective 192 on single GPU
  # hidden_dim=512 uses ~18GB VRAM with batch=48
  batch_size: 48
  learning_rate: 1.0e-4  # Match TRM's lr=1e-4
  weight_decay: 0.1      # Match TRM's weight_decay=0.1
  gradient_clip: 1.0
  
  # Temperature schedule
  temperature_start: 5.0
  temperature_end: 0.1
  
  # Loss weights - focal loss dominates
  focal_gamma: 2.0
  focal_alpha: 0.25
  lambda_entropy: 0.01
  lambda_sparsity: 0.005
  lambda_predicate: 0.001
  lambda_curriculum: 0.01
  lambda_deep_supervision: 0.3
  
  # Curriculum phases
  pretrain_epochs: 100
  finetune_start_epoch: 800
  
  # Optimizer & Scheduler
  optimizer: "adamw"
  scheduler: "onecycle"
  warmup_epochs: 20
  min_lr: 1.0e-6
  
  # EMA for stable evaluation
  use_ema: true
  ema_decay: 0.999
  
  # Gradient accumulation
  # effective_batch_size = 32 * 4 = 128
  grad_accumulation_steps: 4

data:
  train_path: "./data/arc-agi/data/training"
  eval_path: "./data/arc-agi/data/evaluation"
  
  max_grid_size: 30
  
  # Multi-worker data loading
  num_workers: 8
  pin_memory: true
  prefetch_factor: 4
  persistent_workers: true
  
  # Infinite diversity mode
  cache_samples: false
  
  # Full augmentation pipeline
  augmentation:
    enabled: true
    rotation: true
    flip: true
    transpose: true
    color_permutation: true

evaluation:
  num_guesses: 2
  use_tta: true
  tta_rotations: [0, 90, 180, 270]
  tta_flips: [false, true]

logging:
  log_every: 1
  save_every: 25
  eval_every: 5
  keep_last_n: 5
  checkpoint_dir: "checkpoints/rlan_fair"  # TRM-matched capacity
  log_to_file: true
  
  use_wandb: false
  wandb_project: "rlan-arc"
  wandb_run_name: null

# Hardware settings
hardware:
  device: "cuda"
  seed: 42
  deterministic: false

device:
  use_cuda: true
  mixed_precision: true   # AMP for memory efficiency
  dtype: "bfloat16"       # bfloat16 more stable than float16 (TRM uses this)
  compile: false           # torch.compile disabled for stability
