# SCI-ARC Default Configuration
# ===========================================================================
# PURPOSE: BALANCED config for development and reasonable performance
# Use Case: Day-to-day training, debugging, testing
# NOT optimized for publication - use competitive.yaml for that
#
# Hardware: NVIDIA RTX 3090 (24GB VRAM) with CUDA 12.6
# Training Time: ~12-24 hours
# Parameters: ~7.1M (matching TRM)
# ===========================================================================

# =============================================================================
# MODEL ARCHITECTURE
# =============================================================================
model:
  # Core dimensions (matching TRM's ~7M params)
  hidden_dim: 256               # TRM uses 256
  num_colors: 10
  max_grid_size: 30
  
  # Structural encoder
  num_structure_slots: 8        # Number of transformation prototypes
  se_layers: 3                  # Transformer encoder layers (context_encoder)
  structure_heads: 8            # Attention heads for structure
  use_abstraction: true         # Enable abstraction layer
  
  # Content encoder  
  max_objects: 16               # Maximum number of objects to track
  content_heads: 8              # Attention heads for content
  
  # Causal binding
  binding_heads: 8              # Attention heads for binding
  
  # Recursive refinement (TRM-style)
  H_cycles: 3                   # Outer refinement cycles (TRM uses 3)
  L_cycles: 4                   # Inner refinement cycles (TRM uses 4-6)
  L_layers: 2                   # Layers per L cycle
  
  # Output
  dropout: 0.1

# =============================================================================
# TRAINING
# =============================================================================
training:
  # Optimization
  learning_rate: 3.0e-4
  weight_decay: 0.01
  max_epochs: 100
  warmup_epochs: 5
  grad_clip: 1.0
  grad_accumulation_steps: 1
  
  # Batch size (optimized for RTX 3090 24GB VRAM)
  # 128 fits comfortably in 24GB VRAM (~18GB), avoiding slow shared memory
  # Larger batches overflow VRAM causing PCIe transfers (7s/batch -> 4s/batch)
  batch_size: 128
  eval_batch_size: 128
  
  # Loss weights
  # NOTE: SCL at 1.0 dominates training (97% of loss). Reduce to 0.1 to let task loss drive learning.
  # Task loss ~0.17 with Focal Loss, SCL ~5.5 → total was 97% SCL!
  scl_weight: 0.1               # Structural Contrastive Loss (auxiliary, not primary)
  ortho_weight: 0.01            # Orthogonality constraint
  deep_supervision_weight: 0.3  # Intermediate supervision (reduced)
  
  # ==========================================================================
  # CISL: Content-Invariant Structure Learning (replaces SCL for few-shot learning)
  # ==========================================================================
  # CISL uses 4 losses instead of standard contrastive learning:
  # 1. L_consist: All demos in a task should have same structure embedding
  # 2. L_content_inv: Content-permuted task should have same structure embedding  
  # 3. L_var: Batch variance regularization (prevents collapse to zeros)
  # 4. L_recon: Standard reconstruction loss (already exists as task_loss)
  #
  # CRITICAL: L_variance must be high enough to prevent directional collapse!
  # Previous setting (0.1) was too weak - embeddings collapsed to cosine_sim=1.0
  #
  # Note: Config params use 'cicl_' prefix for backward compatibility
  # Set use_cicl=true to enable CISL (recommended for ARC), false for legacy SCL
  use_cicl: true                # Enable CISL instead of SCL
  cicl_consist_weight: 0.5      # Within-task consistency weight (λ₁)
  cicl_color_inv_weight: 0.5    # Content invariance weight (λ₂, key SCI principle!)
  cicl_variance_weight: 1.0     # Batch variance weight (λ₃, INCREASED to prevent collapse!)
  cicl_target_std: 0.5          # Target std (γ) for variance regularization
  
  # Anti-background-collapse (CRITICAL for ARC's ~85% background grids)
  # NOTE: gamma=2.0 + class_weights reduced loss from 2.98 to 0.17 (too aggressive!)
  # Reduce gamma to 1.0 for gentler down-weighting, keep class_weights for content focus
  focal_gamma: 1.0              # Focal Loss gamma (1.0 = gentler, 2.0 = aggressive)
  use_class_weights: true       # Background weight=0.1, others=1.0 (10x penalty for content errors)
  label_smoothing: 0.1          # Prevents overconfident predictions
  
  # Learning rate schedule
  scheduler_type: cosine        # cosine, onecycle, constant
  min_lr: 1.0e-6
  
  # Mixed precision (REQUIRED for RTX 3090 efficiency)
  use_amp: true
  
  # Curriculum learning
  use_curriculum: true
  curriculum_stages:            # Epoch thresholds
    - 10  # Stage 1: easy tasks (small grids, many examples)
    - 30  # Stage 2: medium tasks
    - 60  # Stage 3: hard tasks (all data)

# =============================================================================
# DATA
# =============================================================================
data:
  # Dataset paths (PRODUCTION: D:\SCI-ARC with data at ./data/arc-agi/data/)
  arc_dir: ./data/arc-agi/data  # Contains training/ and evaluation/ subdirectories
  rearc_dir: null               # Optional: path to RE-ARC
  
  # Data loading
  # Windows: num_workers=0 is fastest with cache_samples=true (all data in memory)
  # Linux: Use 4-8 workers. Windows multiprocessing has high overhead.
  num_workers: 0
  pin_memory: true
  
  # Caching (eliminates data loading stalls)
  # When enabled, pre-generates augmented samples into memory before training
  # Memory usage: ~400 tasks × 8 augmentations × ~1KB = ~3MB (very small)
  cache_samples: true           # Pre-cache all samples in memory
  cache_augmentations: 8        # Number of augmented versions per task
  
  # Augmentation
  augment: true
  augment_rotation: true        # Random 90° rotations
  augment_flip: true            # Random flips
  augment_color: true           # Color permutation
  
  # SCL Transform Family Assignment
  # CRITICAL for SCL learning - determines what becomes "positive pairs"
  # Options:
  #   "task"     - RECOMMENDED: Same task (different augmentations) = positive pairs
  #                This teaches augmentation invariance (core generalization property)
  #   "augment"  - Same augmentation (different tasks) = positive pairs  
  #                This teaches "rotation detection" not "task understanding"
  #   "inferred" - Use detected transform type (only works for simple transforms)
  scl_family_mode: "task"       # Use task-based grouping for augmentation invariance

# =============================================================================
# EVALUATION
# =============================================================================
evaluation:
  num_attempts: 2               # Official ARC allows 2 attempts
  use_voting: true              # Ensemble voting
  temperature: 1.0              # Sampling temperature
  batch_size: 1                 # For evaluation

# =============================================================================
# INFERENCE MODULES (Disabled by default for faster dev iteration)
# =============================================================================
# For competitive evaluation, use configs/competitive.yaml which enables these.
# These modules add overhead but significantly improve accuracy.
#
inference:
  # Component toggles (all disabled for fast dev)
  use_ttt: false                # Test-Time Training
  use_stochastic_sampling: false
  use_augmentation_voting: true # Basic voting still enabled
  use_consistency_verification: false
  
  # TTT parameters (used when use_ttt=true)
  ttt_steps: 10
  ttt_learning_rate: 1.0e-4
  ttt_grad_clip: 1.0
  ttt_modules:
    - grid_encoder
    - structural_encoder
  
  # Sampling parameters
  num_samples: 16
  sampling_temperature: 1.0
  use_mc_dropout: false
  
  # Voting parameters
  voting_num_dihedral: 8
  
  # Output
  top_k: 2

# =============================================================================
# LOGGING & CHECKPOINTS
# =============================================================================
logging:
  log_every: 10                 # Log every N steps
  eval_every: 1                 # Evaluate every N epochs
  save_every: 5                 # Save checkpoint every N epochs (for resume safety)
  keep_last_n: 5                # Keep last N checkpoints
  
  # File logging (for reproducibility and debugging)
  log_to_file: true             # Save all terminal output to log file
  
  # Weights & Biases
  use_wandb: true
  wandb_project: sci-arc
  wandb_run_name: null          # Auto-generated if null
  
  # Directories
  checkpoint_dir: ./checkpoints
  output_dir: ./outputs

# =============================================================================
# HARDWARE (RTX 3090 24GB VRAM, CUDA 12.6)
# =============================================================================
hardware:
  device: cuda                  # Use 'cuda' for GPU, 'cpu' for CPU-only
  seed: 42                      # Fixed seed for reproducibility
  deterministic: true           # REQUIRED for reproducible results (Nature/NeurIPS)
  
  # Memory optimization for 24GB VRAM
  # These are applied automatically when use_amp: true
  # If you run out of memory, reduce batch_size or increase grad_accumulation_steps
