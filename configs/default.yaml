# SCI-ARC Default Configuration
# ===========================================================================
# PURPOSE: BALANCED config for development and reasonable performance
# Use Case: Day-to-day training, debugging, testing
# NOT optimized for publication - use competitive.yaml for that
#
# Hardware: NVIDIA RTX 3090 (24GB VRAM) with CUDA 12.6
# Training Time: ~12-24 hours
# Parameters: ~7.1M (matching TRM)
# ===========================================================================

# =============================================================================
# MODEL ARCHITECTURE
# =============================================================================
model:
  # Core dimensions (matching TRM's ~7M params)
  hidden_dim: 256               # TRM uses 256
  num_colors: 10
  max_grid_size: 30
  
  # Structural encoder
  num_structure_slots: 8        # Number of transformation prototypes
  num_abstraction_layers: 3     # Depth of abstraction hierarchy
  structure_heads: 8            # Attention heads for structure
  
  # Content encoder  
  max_objects: 16               # Maximum number of objects to track
  content_heads: 8              # Attention heads for content
  
  # Causal binding
  binding_heads: 8              # Attention heads for binding
  
  # Recursive refinement (TRM-style)
  H_cycles: 3                   # Outer refinement cycles (TRM uses 3)
  L_cycles: 4                   # Inner refinement cycles (TRM uses 4-6)
  L_layers: 2                   # Layers per L cycle
  
  # Output
  dropout: 0.1

# =============================================================================
# TRAINING
# =============================================================================
training:
  # Optimization
  learning_rate: 3.0e-4
  weight_decay: 0.01
  max_epochs: 100
  warmup_epochs: 5
  grad_clip: 1.0
  grad_accumulation_steps: 1
  
  # Batch size (optimized for RTX 3090 24GB VRAM)
  # 192 uses ~18-19GB VRAM with good SCL performance
  batch_size: 192
  eval_batch_size: 192
  
  # Loss weights
  scl_weight: 1.0               # Structural Contrastive Loss (increased from 0.1)
  ortho_weight: 0.01            # Orthogonality constraint
  deep_supervision_weight: 0.5  # Intermediate supervision
  
  # Learning rate schedule
  scheduler_type: cosine        # cosine, onecycle, constant
  min_lr: 1.0e-6
  
  # Mixed precision (REQUIRED for RTX 3090 efficiency)
  use_amp: true
  
  # Curriculum learning
  use_curriculum: true
  curriculum_stages:            # Epoch thresholds
    - 10  # Stage 1: easy tasks (small grids, many examples)
    - 30  # Stage 2: medium tasks
    - 60  # Stage 3: hard tasks (all data)

# =============================================================================
# DATA
# =============================================================================
data:
  # Dataset paths (PRODUCTION: D:\SCI-ARC with data at ./data/arc-agi/data/)
  arc_dir: ./data/arc-agi/data  # Contains training/ and evaluation/ subdirectories
  rearc_dir: null               # Optional: path to RE-ARC
  
  # Data loading
  # With batch_size=256 and 48 vCPUs, 24 workers keeps GPU fed
  # Rule: 2-4 workers per GPU, but can go higher with many cores
  # Reduce to 16 if you see CPU memory issues
  num_workers: 24
  pin_memory: true
  
  # Caching (eliminates data loading stalls)
  # When enabled, pre-generates augmented samples into memory before training
  # Memory usage: ~400 tasks × 8 augmentations × ~1KB = ~3MB (very small)
  cache_samples: true           # Pre-cache all samples in memory
  cache_augmentations: 8        # Number of augmented versions per task
  
  # Augmentation
  augment: true
  augment_rotation: true        # Random 90° rotations
  augment_flip: true            # Random flips
  augment_color: true           # Color permutation
  
  # SCL Transform Family Assignment
  # CRITICAL for SCL learning: Use augmentation type (dihedral 0-7) as transform_family
  # This ensures samples with same augmentation are positive pairs in contrastive learning
  use_augment_family: true      # Use augmentation type as transform_family for SCL

# =============================================================================
# EVALUATION
# =============================================================================
evaluation:
  num_attempts: 2               # Official ARC allows 2 attempts
  use_voting: true              # Ensemble voting
  temperature: 1.0              # Sampling temperature
  batch_size: 1                 # For evaluation

# =============================================================================
# LOGGING & CHECKPOINTS
# =============================================================================
logging:
  log_every: 10                 # Log every N steps
  eval_every: 1                 # Evaluate every N epochs
  save_every: 5                 # Save checkpoint every N epochs (for resume safety)
  keep_last_n: 5                # Keep last N checkpoints
  
  # File logging (for reproducibility and debugging)
  log_to_file: true             # Save all terminal output to log file
  
  # Weights & Biases
  use_wandb: true
  wandb_project: sci-arc
  wandb_run_name: null          # Auto-generated if null
  
  # Directories
  checkpoint_dir: ./checkpoints
  output_dir: ./outputs

# =============================================================================
# HARDWARE (RTX 3090 24GB VRAM, CUDA 12.6)
# =============================================================================
hardware:
  device: cuda                  # Use 'cuda' for GPU, 'cpu' for CPU-only
  seed: 42                      # Fixed seed for reproducibility
  deterministic: true           # REQUIRED for reproducible results (Nature/NeurIPS)
  
  # Memory optimization for 24GB VRAM
  # These are applied automatically when use_amp: true
  # If you run out of memory, reduce batch_size or increase grad_accumulation_steps
