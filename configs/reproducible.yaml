# SCI-ARC Reproducible Configuration for Publication
# ===========================================================================
# PURPOSE: REPRODUCIBILITY for paper submission (same as default but strict)
# Use Case: When you need bit-exact reproduction for Nature/NeurIPS
# Note: Same architecture as default.yaml, but with strict determinism
#
# For MAXIMUM PERFORMANCE, use competitive.yaml instead!
#
# Hardware: NVIDIA RTX 3090 (24GB VRAM), CUDA 12.6
# ===========================================================================

# =============================================================================
# REPRODUCIBILITY SETTINGS (CRITICAL FOR PUBLICATION)
# =============================================================================
# These settings ensure bit-exact reproducibility across runs.
# Results can be reproduced by anyone with the same hardware/software.

reproducibility:
  seed: 42                      # Master random seed
  deterministic: true           # Use deterministic algorithms
  benchmark: false              # Disable cuDNN auto-tuner (required for reproducibility)
  
  # Software versions for exact reproduction
  # These are logged automatically but listed here for reference
  python_version: "3.10"
  pytorch_version: "2.1.0+cu126"
  cuda_version: "12.6"

# =============================================================================
# MODEL ARCHITECTURE
# =============================================================================
model:
  hidden_dim: 256
  num_colors: 10
  max_grid_size: 30
  num_structure_slots: 8
  num_abstraction_layers: 3
  structure_heads: 8
  max_objects: 16
  content_heads: 8
  binding_heads: 8
  H_cycles: 3
  L_cycles: 4
  L_layers: 2
  dropout: 0.1

# =============================================================================
# TRAINING
# =============================================================================
training:
  learning_rate: 3.0e-4
  weight_decay: 0.01
  max_epochs: 100
  warmup_epochs: 5
  grad_clip: 1.0
  grad_accumulation_steps: 1
  # 128 balances SCL learning with training speed on RTX 3090
  batch_size: 128
  eval_batch_size: 128
  scl_weight: 0.1
  ortho_weight: 0.01
  deep_supervision_weight: 0.5
  scheduler_type: cosine
  min_lr: 1.0e-6
  use_amp: true
  use_curriculum: true
  curriculum_stages:
    - 10
    - 30
    - 60

# =============================================================================
# DATA
# =============================================================================
data:
  # Dataset paths (PRODUCTION: D:\SCI-ARC with data at ./data/arc-agi/data/)
  arc_dir: ./data/arc-agi/data  # Contains training/ and evaluation/ subdirectories
  rearc_dir: null
  # With batch_size=256 and 48 vCPUs, 24 workers keeps GPU fed
  num_workers: 24
  pin_memory: true
  augment: true
  augment_rotation: true
  augment_flip: true
  augment_color: true

# =============================================================================
# EVALUATION
# =============================================================================
evaluation:
  num_attempts: 2
  use_voting: true
  num_dihedral: 8               # All 8 dihedral transforms for voting
  temperature: 1.0
  batch_size: 1

# =============================================================================
# LOGGING
# =============================================================================
logging:
  log_every: 10
  eval_every: 1
  save_every: 5
  keep_last_n: 3
  log_to_file: true             # Save all terminal output to log file
  use_wandb: true
  wandb_project: sci-arc-reproducible
  wandb_run_name: null
  checkpoint_dir: ./checkpoints
  output_dir: ./outputs

# =============================================================================
# HARDWARE
# =============================================================================
hardware:
  device: cuda
  seed: 42
  deterministic: true
