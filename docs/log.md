
  Batch 9/18: loss=0.7882, focal_weighted=0.7713, batch_acc=62.9%, exact=3/310 (1.0%), running_acc=55.5%, lr=5.00e-03
    [TaskTrack] Global_Solved: 2/411, Epoch_Solved: 2, New_Puzzles: 0
    FG: batch=51.1% run50=39.5% | BG: batch=70.2% run50=70.0%
    Per-Color: [0:81% 1:79% 2:46% 3:25% 4:74% 5:74% 6:43% 7:7% 8:64% 9:73%]
    Running50: [0:86% 1:42% 2:32% 3:32% 4:54% 5:37% 6:45% 7:65% 8:53% 9:49%]
  Batch 10/18: loss=0.3457, focal_weighted=0.3287, batch_acc=52.8%, exact=3/350 (0.9%), running_acc=55.2%, lr=5.00e-03
    [TaskTrack] Global_Solved: 2/411, Epoch_Solved: 2, New_Puzzles: 0
    FG: batch=40.6% run50=39.6% | BG: batch=60.6% run50=69.2%
    Per-Color: [0:76% 1:2% 2:46% 3:12% 4:29% 5:62% 6:60% 7:83% 8:72% 9:34%]
    Running50: [0:85% 1:38% 2:33% 3:31% 4:52% 5:39% 6:47% 7:67% 8:54% 9:48%]
  Batch 11/18: loss=0.3634, focal_weighted=0.3403, batch_acc=61.3%, exact=3/390 (0.8%), running_acc=55.8%, lr=5.00e-03
    [TaskTrack] Global_Solved: 2/411, Epoch_Solved: 2, New_Puzzles: 0
    FG: batch=50.3% run50=40.5% | BG: batch=74.6% run50=69.6%
    Per-Color: [0:86% 1:40% 2:22% 3:35% 4:65% 5:64% 6:67% 7:76% 8:80% 9:39%]
    Running50: [0:85% 1:39% 2:32% 3:31% 4:53% 5:41% 6:48% 7:68% 8:56% 9:47%]
  Batch 12/18: loss=0.1003, focal_weighted=0.0865, batch_acc=48.2%, exact=3/430 (0.7%), running_acc=55.2%, lr=5.00e-03
    [TaskTrack] Global_Solved: 2/411, Epoch_Solved: 2, New_Puzzles: 0
    FG: batch=26.1% run50=39.4% | BG: batch=68.0% run50=69.5%
    Per-Color: [0:84% 1:16% 2:19% 3:6% 4:44% 5:59% 6:0% 7:66% 8:28% 9:25%]
    Running50: [0:85% 1:37% 2:31% 3:29% 4:52% 5:43% 6:45% 7:67% 8:54% 9:45%]
  Batch 13/18: loss=0.4071, focal_weighted=0.3864, batch_acc=56.2%, exact=3/470 (0.6%), running_acc=55.2%, lr=5.00e-03
    [TaskTrack] Global_Solved: 2/411, Epoch_Solved: 2, New_Puzzles: 0
    FG: batch=39.3% run50=39.3% | BG: batch=60.9% run50=68.9%
    Per-Color: [0:87% 1:49% 2:62% 3:44% 4:77% 5:67% 6:48% 7:51% 8:41% 9:42%]
    Running50: [0:85% 1:38% 2:34% 3:30% 4:54% 5:45% 6:45% 7:66% 8:53% 9:45%]
  Batch 14/18: loss=0.1160, focal_weighted=0.0913, batch_acc=36.8%, exact=3/482 (0.6%), running_acc=54.0%, lr=5.00e-03
    [TaskTrack] Global_Solved: 2/411, Epoch_Solved: 2, New_Puzzles: 0
    FG: batch=20.7% run50=38.1% | BG: batch=65.8% run50=68.7%
    Per-Color: [0:86% 1:38% 2:77% 3:0% 4:0% 5:98% 6:43% 7:- 8:71% 9:12%]
    Running50: [0:85% 1:38% 2:36% 3:28% 4:50% 5:48% 6:45% 7:66% 8:55% 9:43%]
  Batch 15/18: loss=0.0954, focal_weighted=0.0793, batch_acc=52.2%, exact=3/522 (0.6%), running_acc=53.9%, lr=5.00e-03
    [TaskTrack] Global_Solved: 2/411, Epoch_Solved: 2, New_Puzzles: 0
    FG: batch=26.2% run50=37.4% | BG: batch=85.0% run50=69.7%
    Per-Color: [0:89% 1:14% 2:43% 3:24% 4:8% 5:34% 6:28% 7:78% 8:74% 9:38%]
    Running50: [0:85% 1:36% 2:37% 3:28% 4:48% 5:47% 6:44% 7:67% 8:56% 9:42%]
  Batch 16/18: loss=0.4122, focal_weighted=0.3949, batch_acc=61.2%, exact=3/562 (0.5%), running_acc=54.3%, lr=5.00e-03
    [TaskTrack] Global_Solved: 2/411, Epoch_Solved: 2, New_Puzzles: 0
    FG: batch=49.9% run50=38.1% | BG: batch=63.4% run50=69.3%
    Per-Color: [0:88% 1:49% 2:48% 3:57% 4:83% 5:51% 6:53% 7:81% 8:65% 9:44%]
    Running50: [0:86% 1:37% 2:37% 3:29% 4:50% 5:47% 6:44% 7:68% 8:56% 9:42%]
  Batch 17/18: loss=0.1059, focal_weighted=0.0929, batch_acc=46.3%, exact=3/602 (0.5%), running_acc=53.9%, lr=5.00e-03
    [TaskTrack] Global_Solved: 2/411, Epoch_Solved: 2, New_Puzzles: 0
    FG: batch=17.3% run50=36.9% | BG: batch=78.6% run50=69.8%
    Per-Color: [0:89% 1:9% 2:11% 3:44% 4:62% 5:6% 6:14% 7:5% 8:14% 9:38%]
    Running50: [0:86% 1:35% 2:36% 3:30% 4:50% 5:45% 6:42% 7:64% 8:54% 9:42%]

  [EPOCH MEMORY] Peak: 18322MB alloc, 20988MB reserved (?-0MB from prev epoch)

Epoch 5 Summary:
  Total Loss: 0.2904
  Task Loss (focal): 0.2721
  [Global Task Progress] Unique Solved: 2/411 (0.5%)
    NEW puzzles solved this epoch: 0
  Entropy Loss: 0.8256 (weight=0.01)
  Sparsity Loss: 0.0782 (weight=0.2)
  ART Consistency Loss: 0.0030 (weight=0.02, batches=18)
  ARPS Imitation Loss: 0.0000 (weight=0.1, batches=0)
  Time: 12.5s, LR: 5.00e-03
    Per-module LRs: DSC:5.00e-03, MSRE:5.00e-03, Other:5.00e-04
  Temperature: 0.9772 (lower=sharper attention)
  Samples Processed: 602 (18 batches)
  Dihedral Distribution: [100.0%, 0.0%, 0.0%, 0.0%, 0.0%, 0.0%, 0.0%, 0.0%]
    [!] Non-uniform dihedral distribution (max dev: 87.5%)
  Color Permutation: 0.0% (0/602)
  Translational Aug: 0.0% (0/602), unique offsets: 0
  Aug Quality: OK
  --- Training Diagnostics ---
  Solver Steps: 4 (deep supervision active)
  Per-Step Loss (epoch avg, 1 batches): [1.5312, 1.4766, 1.4531, 1.4375]
    ? Step improvement: 6.1% (later steps better - GOOD!)
  Best-Step Histogram: [s0:3, s1:7, s2:3, s3:6]
  Solver Health: Last step best: 31.6%, Earlier step best: 68.4%
  Avg Step Improvement: 0.3% (step0?stepN)
    ?? SOLVER OVER-ITERATION WARNING: 68% of batches had earlier step as best!
    ?? Consider: (1) enabling best-step selection, (2) reducing num_solver_steps, (3) enabling ACT
  Entropy-Loss Agreement: 27.8% (5/18)
    ?? LOW AGREEMENT: Entropy picks same step as loss only 28% of time!
    ?? Inference best-step selection may not be reliable!
  Grad Norms: DSC=0.0259, StopPred=0.0006, Encoder=0.0723, Solver=0.4382
              ContextEnc=0.0529, MSRE=0.0042
  StopPred Weight Var: 2.82e-02
  Attention: max=0.7841, min=0.000000
    Attention is sharp (good!)
  Stop Prob: 0.004 (approx 4.0 clues active)
  Stop Probs Std: 0.001 (global std across batch�clues)
  Clues Used: mean=3.99, std=0.00, range=[7.0, 7.0]
  Clue-Loss Correlation: +0.000 (weak - per-sample coupling may need tuning)
  Stop Logits: mean=-5.66, std=0.15, range=[-5.9, -5.0]
    [CRITICAL] Stop logits saturated! |mean|=5.7 > 5.0
    Sigmoid gradient ? 0, stop predictor cannot learn!
    Consider adding L2 regularization on stop_logits
    [!] Low variance - clue count not adapting per-task!
  Per-Clue Entropy: [4.92, 0.01, 0.03, 0.17, 0.24, 0.25, 0.28] (mean=0.84, max=6.80)
    Good entropy (0.84) - attention is focused!
  Centroid Spread: 3.81 (higher=more diverse)
  --- Stop Predictor Coupling ---
  Entropy Input to Stop: 0.1241 (normalized, lower=sharper)
  Per-Clue Entropy Input: [0.723, 0.002, 0.005, 0.025, 0.036, 0.037, 0.040]
  Per-Clue Stop Prob: [0.003, 0.004, 0.003, 0.003, 0.003, 0.004, 0.004]
  --- Sparsity Loss Breakdown (Per-Sample Coupled) ---
  Min Clue Penalty: 0.0000 (per-sample avg)
  Per-Sample Clue Penalty (scaled): 0.0000
  Base Pondering: 0.0697 (clues=6.97)
  Entropy Pondering: 0.0087
  --- Gradient Clipping ---
  Grad Norm (before clip): 0.4597
    Gradients within bounds
  --- Per-Class Distribution (Valid Pixels Only) ---
  Padding: 70.0% of grid (ignored in loss)
  Pred %: [45.6, 0.9, 3.6, 9.3, 4.3, 0.7, 1.6, 10.1, 21.2, 2.7]
  Target %: [37.7, 2.9, 6.0, 8.6, 8.2, 2.4, 1.5, 7.3, 20.4, 4.9]
  Per-Class Acc %: [85, 55, 42, 43, 68, 57, 49, 77, 67, 54]
  [WARN] FG color preference: 34% are color 8
  ==================================================
  PER-SAMPLE TRAINING ACCURACY (Epoch 5)
  ==================================================
  ? Mean Accuracy: 53.9%
  ? Exact Match: 3/602 (0.5%)
  ? High Acc (?90%): 55/602 (9.1%)
  FG Accuracy: 36.9%
  BG Accuracy: 69.8%
  Batch Trend: 53.5% ? 50.5% (? 2.9pp)
  Accuracy Distribution: 0-25%:12%, 25-50%:12%, 50-75%:38%, 75-90%:12%, 90-100%:25%
  Running Window (last 18 batches): 53.9% � 7.4%

  --- PER-COLOR ACCURACY (10 classes) ---
  Color:       0     1     2     3     4     5     6     7     8     9
  Acc%:      85%   55%   42%   43%   68%   57%   49%   77%   67%   54%
  Target:  28.9% 10.1%  7.1%  9.1% 12.0%  5.0%  3.0% 11.1% 11.3%  2.4%
  Pred:    38.4%  7.7%  5.1%  5.5% 11.5%  5.0%  2.4% 11.4%  9.8%  3.0%
  [!] Weak colors (<50% acc): 2(Red), 3(Green), 6(Pink)
  ==================================================

  ==================================================
  LEARNING TRAJECTORY (Epoch 5)
  ==================================================
  Stop Prob:   0.004 ? (init=0.27, task-dependent)
  Exp. Clues:  6.97 (latent variable, task-dependent)
  Attn Entropy: 0.84 ? (max=6.8, sharper=better)
  Task Loss:   0.2721 ?
  Train Acc:   53.9% ?
  Exact Match: 0.5% ?
  Best Step:   3 (later=better refinement)
  FG Coverage: 87.3% of target ?
  ==================================================

  ############################################################
  ? TRAINING HEALTH CHECK - Epoch 5
  ############################################################
  ? Attention sharpening (0.12 < 0.7)
  ? Loss decreasing (0.645 ? 0.272)
  ? Accuracy improving (26.2% ? 53.9%)
  ? No NaN/Inf issues
  ? Stop probs uniform (global=0.001, per_clue=0.000) - early epoch OK
  ? Centroids moderately spread (3.8)
  ? Weak confidence-stop coupling (r=0.27)
  ? Color preference (34% one color)
  --------------------------------------------------------
  RESULT: 4/8 checks passed
  STATUS: ? MONITOR
  ? Some concerns but not critical. Watch next few epochs.
  ############################################################
  HPM Instance Buffer: NOT INITIALIZED (check use_hpm config)
  HPM Procedural Buffer: NOT INITIALIZED (check use_hpm and use_hyperlora config)
  Saved checkpoint to checkpoints\rlan_stable_ablation\latest.pt
  [ARCDataset] Augmentation config updated:
    dihedral=False, color_perm=False (prob=0.30), translational=False

Epoch 6/120
----------------------------------------
  [Batch 0] Grid size: 30x30 (batch_max_size=30)
[OUTPUT_EQUIV WARNING] Failed: too many indices for tensor of dimension 4
[ARPS WARNING] Failed: index 7 is out of bounds for dimension 0 with size 7

  [MEMORY] Epoch 5 Batch 0:
    Baseline: alloc=181MB, reserved=20988MB
    01_batch_on_gpu: alloc=189MB (+8MB), reserved=21010MB
    02_after_forward: alloc=14527MB (+14337MB), reserved=21010MB
    03_before_backward: alloc=14541MB (+14MB), reserved=21140MB
    04_after_backward: alloc=303MB (-14238MB), reserved=21140MB
    PEAK: alloc=15949MB, reserved=21140MB / 24576MB (86.0%)

    [BREAKDOWN] Largest increase at '02_after_forward' (+14337MB):
      Model params: 51.2MB
      Model grads:  0.0MB
      Optimizer:    102.3MB (estimate (Adam-like: AdamW))
      Batch data:   4.9MB
      Activations:  60.1MB
      Output tensors (top 5):
        features: 35.2MB
        support_features: 20.0MB
        all_logits: 2.7MB
        attention_maps: 1.0MB
        logits: 0.7MB
      Module params (top 5):
        solver: 28.2MB
        context_encoder: 15.0MB
        context_injector: 3.0MB
        dsc: 2.5MB
        structure_projector: 0.5MB
      Active modules: solver_context, cross_attention
      Inactive modules: hyperlora, hpm, loo, equivariance

  [MEMORY] First forward pass with staged modules active:
  [MEMORY] Batch 0: alloc=303MB, reserved=21140MB, headroom=3436MB
  Batch 0/18: loss=0.0954, focal_weighted=0.0725, batch_acc=43.9%, exact=0/40 (0.0%), running_acc=43.9%, lr=5.00e-03
    [TaskTrack] Global_Solved: 2/411, Epoch_Solved: 0, New_Puzzles: 0
    FG: batch=24.6% run50=24.6% | BG: batch=76.9% run50=76.9%
    Per-Color: [0:88% 1:6% 2:4% 3:30% 4:6% 5:70% 6:23% 7:75% 8:20% 9:59%]
    Running50: [0:88% 1:6% 2:4% 3:30% 4:6% 5:70% 6:23% 7:75% 8:20% 9:59%]
    Solver: [1.742, 1.727, 1.719, 1.727] ? best=2
  [Batch 1] Grid size: 30x30 (batch_max_size=30)
[OUTPUT_EQUIV WARNING] Failed: too many indices for tensor of dimension 4


