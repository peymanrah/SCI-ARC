<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RLAN: Recursive Latent Attractor Networks for AGI-ARC Reasoning</title>
    
    <!-- KaTeX for LaTeX rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    
    <!-- Plotly for charts -->
    <script src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>
    
    <!-- Custom styles -->
    <link rel="stylesheet" href="css/style.css">
</head>
<body>
    <!-- Presenter Mode Toggle -->
    <div id="presenter-controls">
        <button id="presenter-toggle" title="Toggle Presenter Mode">
            <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                <rect x="2" y="3" width="20" height="14" rx="2" ry="2"></rect>
                <line x1="8" y1="21" x2="16" y2="21"></line>
                <line x1="12" y1="17" x2="12" y2="21"></line>
            </svg>
        </button>
        <div id="presenter-panel" class="hidden">
            <div class="presenter-header">
                <h3>Presenter Mode</h3>
                <select id="talk-duration">
                    <option value="30s">30-second pitch</option>
                    <option value="2min">2-minute overview</option>
                    <option value="5min" selected>5-minute deep dive</option>
                </select>
            </div>
            <div id="speaker-notes"></div>
            <div id="progress-indicator">
                <div id="progress-bar"></div>
                <span id="progress-text">Section 1 of 8</span>
            </div>
            <div id="pacing-guide"></div>
        </div>
    </div>

    <!-- Navigation -->
    <nav id="story-nav">
        <div class="nav-logo">RLAN</div>
        <ul class="nav-links">
            <li><a href="#hero" class="active">Intro</a></li>
            <li><a href="#arc-problem">ARC</a></li>
            <li><a href="#architecture">Architecture</a></li>
            <li><a href="#visual-walkthrough">Walkthrough</a></li>
            <li><a href="#dsc-coordinates">DSC</a></li>
            <li><a href="#solver">Solver</a></li>
            <li><a href="#training">Training</a></li>
            <li><a href="#parameter-efficiency">Params</a></li>
            <li><a href="#comparison">vs Tiny Recursive Model</a></li>
            <li><a href="#future">Future</a></li>
        </ul>
    </nav>

    <!-- Main Content -->
    <main id="story-container">
        
        <!-- ========== SECTION 1: HERO ========== -->
        <section id="hero" class="story-section" data-section="1">
            <div class="section-content hero-content">
                <div class="hero-text">
                    <div class="paper-title-header centered-title">
                        <h1 class="paper-main-title single-line">Recursive Latent Attractor Networks (RLAN)</h1>
                        <p class="paper-subtitle-full">A Unified Architecture for Solving Abstract Reasoning via Dynamic Coordinate Re-projection</p>
                        <p class="paper-agi-emphasis">üéØ Solving the AGI-ARC Challenge: The Ultimate Test of Machine Intelligence</p>
                    </div>
                    <p class="tagline">A neural architecture that <em>thinks iteratively</em> about 2D spatial puzzles</p>
                    
                    <div class="hero-stats">
                        <div class="stat-card">
                            <span class="stat-value">55%</span>
                            <span class="stat-label">Exact Match on ARC-AGI-1</span>
                        </div>
                        <div class="stat-card">
                            <span class="stat-value">~8M</span>
                            <span class="stat-label">Parameters</span>
                        </div>
                        <div class="stat-card">
                            <span class="stat-value">50 Epochs</span>
                            <span class="stat-label">1 GPU Training</span>
                        </div>
                    </div>
                    
                    <a href="#arc-problem" class="cta-button">Explore the Demo ‚Üí</a>
                </div>
                <div class="hero-visual" id="hero-grid-container">
                    <!-- Animated ARC grid will render here -->
                </div>
            </div>
            
            <div class="takeaway">
                <strong>Takeaway:</strong> RLAN learns to solve visual reasoning puzzles by thinking in relative coordinates and iteratively refining predictions‚Äîlike a human reasoning step by step.
            </div>
            
            <!-- Speaker notes (hidden unless presenter mode) -->
            <div class="speaker-notes" data-duration-30s="RLAN solves AGI-ARC visual puzzles through relative coordinates and iterative refinement‚Äîachieving 55% accuracy with just 8M parameters." data-duration-2min="RLAN‚ÄîRecursive Latent Attractor Networks‚Äîis a unified architecture for solving the ARC-AGI benchmark. Unlike transformers that process sequences with trillions of parameters, RLAN maintains 2D structure, thinks in relative coordinates, and iteratively refines answers. We achieve 55% exact match with just ~8M parameters‚Äî220,000√ó more efficient than GPT-4." data-duration-5min="Welcome to RLAN‚ÄîRecursive Latent Attractor Networks: A Unified Architecture for Solving Abstract Reasoning via Dynamic Coordinate Re-projection. This is our approach to the AGI-ARC challenge‚Äîoften called the 'IQ test for AI'. While GPT-4 with 1.76 trillion parameters scores around 5% on ARC, RLAN achieves 55% exact match with just 8 million parameters. The key insight is coordinate re-projection: instead of memorizing absolute positions, RLAN finds 'clue anchors' and reasons in relative space‚Äîjust like humans do."></div>
        </section>

        <!-- ========== SECTION 1.5: HOW RLAN THINKS ========== -->
        <section id="how-rlan-thinks" class="story-section" data-section="1.5">
            <div class="section-header">
                <h2>How RLAN "Thinks"</h2>
                <p class="section-subtitle">The intuition behind recursive spatial reasoning</p>
            </div>
            
            <div class="section-content">
                <div class="thinking-explainer">
                    <!-- Human Analogy -->
                    <div class="human-analogy-container">
                        <h3>üß† How a Human Solves ARC</h3>
                        <p>Imagine asking a human to solve an ARC puzzle. They don't memorize every pixel. Instead, they do three specific things:</p>
                        
                        <div class="human-steps">
                            <div class="human-step" id="human-step-1">
                                <div class="step-icon">üëÄ</div>
                                <div class="step-content">
                                    <h4>1. Look at the Examples</h4>
                                    <p><em>"Oh, in every example, the <span class="color-red">Red</span> square moves to the <span class="color-blue">Blue</span> square."</em></p>
                                    <span class="step-label">Rule Extraction</span>
                                </div>
                            </div>
                            <div class="human-step" id="human-step-2">
                                <div class="step-icon">üéØ</div>
                                <div class="step-content">
                                    <h4>2. Find the Objects</h4>
                                    <p><em>They scan the test grid to find the <span class="color-red">Red</span> square.</em></p>
                                    <span class="step-label">Saliency / Attention</span>
                                </div>
                            </div>
                            <div class="human-step" id="human-step-3">
                                <div class="step-icon">‚û°Ô∏è</div>
                                <div class="step-content">
                                    <h4>3. Apply the Rule</h4>
                                    <p><em>They mentally move it <strong>relative</strong> to its starting position.</em></p>
                                    <span class="step-label">Relative Reasoning</span>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <!-- The Problem with Standard NNs -->
                    <div class="problem-container">
                        <h3>‚ùå Why Standard Neural Networks Fail</h3>
                        <div class="problem-visual">
                            <div class="nn-problem-grid">
                                <div class="grid-scenario">
                                    <div class="scenario-label">Training: Object at (10,10)</div>
                                    <div class="mini-grid training-grid" id="nn-train-grid">
                                        <!-- 5x5 grid visualization -->
                                        <div class="grid-cell"></div><div class="grid-cell"></div><div class="grid-cell"></div><div class="grid-cell"></div><div class="grid-cell"></div>
                                        <div class="grid-cell"></div><div class="grid-cell red-cell">‚ñ†</div><div class="grid-cell"></div><div class="grid-cell"></div><div class="grid-cell"></div>
                                        <div class="grid-cell"></div><div class="grid-cell"></div><div class="grid-cell"></div><div class="grid-cell"></div><div class="grid-cell"></div>
                                        <div class="grid-cell"></div><div class="grid-cell"></div><div class="grid-cell"></div><div class="grid-cell"></div><div class="grid-cell"></div>
                                        <div class="grid-cell"></div><div class="grid-cell"></div><div class="grid-cell"></div><div class="grid-cell"></div><div class="grid-cell"></div>
                                    </div>
                                    <div class="scenario-note">NN memorizes: "Pixel (1,1) is Red"</div>
                                </div>
                                <div class="arrow-separator">‚Üí</div>
                                <div class="grid-scenario">
                                    <div class="scenario-label">Test: Object at (20,20)</div>
                                    <div class="mini-grid test-grid" id="nn-test-grid">
                                        <div class="grid-cell"></div><div class="grid-cell"></div><div class="grid-cell"></div><div class="grid-cell"></div><div class="grid-cell"></div>
                                        <div class="grid-cell"></div><div class="grid-cell"></div><div class="grid-cell"></div><div class="grid-cell"></div><div class="grid-cell"></div>
                                        <div class="grid-cell"></div><div class="grid-cell"></div><div class="grid-cell"></div><div class="grid-cell"></div><div class="grid-cell"></div>
                                        <div class="grid-cell"></div><div class="grid-cell"></div><div class="grid-cell"></div><div class="grid-cell red-cell">‚ñ†</div><div class="grid-cell"></div>
                                        <div class="grid-cell"></div><div class="grid-cell"></div><div class="grid-cell"></div><div class="grid-cell"></div><div class="grid-cell"></div>
                                    </div>
                                    <div class="scenario-note failure">‚ùå NN fails: "Where's (1,1)?"</div>
                                </div>
                            </div>
                        </div>
                        <p class="problem-explanation">Standard CNNs and Transformers memorize <strong>absolute positions</strong>. If an object moves, they're lost.</p>
                    </div>
                    
                    <!-- RLAN's Solution -->
                    <div class="solution-container">
                        <h3>‚úÖ RLAN's Solution: Think in Relative Coordinates</h3>
                        <p>RLAN doesn't care <em>where</em> an object is absolutely; it only cares where things are <strong>relative to each other</strong>.</p>
                        
                        <div class="rlan-modules-visual">
                            <div class="module-flow-horizontal">
                                <!-- Context Encoder -->
                                <div class="flow-module-compact" id="flow-context">
                                    <div class="module-header">
                                        <span class="module-icon">üìñ</span>
                                        <h4>Context Encoder</h4>
                                    </div>
                                    <p class="module-nickname">"The Rule Book"</p>
                                    <div class="module-io">
                                        <div class="io-row input"><span class="io-label">IN</span> Training pairs (In‚ÜíOut)</div>
                                        <div class="io-row action"><span class="io-label">DO</span> Compare input vs output</div>
                                        <div class="io-row output"><span class="io-label">OUT</span> Task Vector œÑ</div>
                                    </div>
                                </div>
                                
                                <div class="flow-arrow-h">‚Üí</div>
                                
                                <!-- Dynamic Spatial Conditioning -->
                                <div class="flow-module-compact" id="flow-dsc">
                                    <div class="module-header">
                                        <span class="module-icon">üëÅÔ∏è</span>
                                        <h4>Dynamic Spatial Conditioning</h4>
                                    </div>
                                    <p class="module-nickname">"The Eye"</p>
                                    <div class="module-io">
                                        <div class="io-row input"><span class="io-label">IN</span> Test grid + Task Vector œÑ</div>
                                        <div class="io-row action"><span class="io-label">DO</span> "Where should I look?"</div>
                                        <div class="io-row output"><span class="io-label">OUT</span> Attractor $(c_x, c_y)$</div>
                                    </div>
                                </div>
                                
                                <div class="flow-arrow-h">‚Üí</div>
                                
                                <!-- Multi-Scale Relative Encoding -->
                                <div class="flow-module-compact highlight-module" id="flow-msre">
                                    <div class="module-header">
                                        <span class="module-icon">üß≠</span>
                                        <h4>Multi-Scale Relative Encoding</h4>
                                    </div>
                                    <p class="module-nickname">"The Compass" ‚≠ê</p>
                                    <div class="module-io">
                                        <div class="io-row input"><span class="io-label">IN</span> Grid + Attractor $(c_x, c_y)$</div>
                                        <div class="io-row action"><span class="io-label">DO</span> $\Delta = (x - c_x, y - c_y)$</div>
                                        <div class="io-row output"><span class="io-label">OUT</span> Position-invariant features</div>
                                    </div>
                                </div>
                                
                                <div class="flow-arrow-h">‚Üí</div>
                                
                                <!-- Recursive Solver -->
                                <div class="flow-module-compact" id="flow-solver">
                                    <div class="module-header">
                                        <span class="module-icon">üé®</span>
                                        <h4>Recursive Solver</h4>
                                    </div>
                                    <p class="module-nickname">"The Painter"</p>
                                    <div class="module-io">
                                        <div class="io-row input"><span class="io-label">IN</span> Relative coords + œÑ</div>
                                        <div class="io-row action"><span class="io-label">DO</span> Iteratively paint output</div>
                                        <div class="io-row output"><span class="io-label">OUT</span> Final prediction grid</div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <!-- Why It Generalizes -->
                    <div class="generalization-container">
                        <h3>üöÄ Why RLAN Generalizes</h3>
                        <div class="generalization-reasons">
                            <div class="reason-card">
                                <div class="reason-icon">üìê</div>
                                <h4>Manifold Flattening</h4>
                                <p>By switching to <strong>Relative Coordinates</strong>, RLAN "flattens" the problem space. The task "Move Object 1 step right" looks <em>mathematically identical</em> regardless of where the object starts.</p>
                            </div>
                            <div class="reason-card">
                                <div class="reason-icon">üîÄ</div>
                                <h4>Separation of Concerns</h4>
                                <p><strong>DSC (Perception):</strong> Handles the messy pixel work<br>
                                <strong>Solver (Reasoning):</strong> Handles abstract logic<br>
                                The Solver learns "Copy" once and applies it to <em>any</em> object DSC points at.</p>
                            </div>
                            <div class="reason-card">
                                <div class="reason-icon">üéØ</div>
                                <h4>Dynamic Attractors</h4>
                                <p>DSC creates a <strong>new coordinate system</strong> for every sample. It "re-centers" the world around relevant objects, ignoring noise.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="takeaway">
                <strong>Takeaway:</strong> RLAN works because it stops treating grids as fixed images and starts treating them as collections of <strong>anchors</strong> and <strong>relative movements</strong>. Its mathematics <em>force</em> it to ignore absolute positions and focus on relationships.
            </div>
            
            <div class="speaker-notes" data-duration-30s="RLAN mimics human reasoning: extract rules, find objects, apply relative logic." data-duration-2min="RLAN mimics how humans solve puzzles. First, the Context Encoder extracts the rule from examples. Then, DSC finds the relevant objects. Finally, MSRE converts everything to relative coordinates‚Äîmaking 'Red at (10,10)' identical to 'Red at (20,20)'. This forces the model to learn general rules, not memorize positions." data-duration-5min="RLAN is designed to think like humans. Step 1: The Context Encoder watches the training examples and extracts the rule‚Äîlike 'move Red to Blue'. Step 2: The Dynamic Saliency Controller scans the test grid and places 'attractors' on key objects. Step 3: The Multi-Scale Relative Encoding module transforms all coordinates to be relative to these attractors. This is the key insight: a Red square at (10,10) looks mathematically identical to one at (20,20) because both are at position (0,0) relative to their own attractor. The model literally cannot distinguish absolute positions‚Äîit's forced to learn the general rule. Finally, the Recursive Solver paints the output, using cross-attention to look back at the examples for guidance."></div>
        </section>

        <!-- ========== SECTION 2: THE ARC PROBLEM ========== -->
        <section id="arc-problem" class="story-section" data-section="2">
            <div class="section-header">
                <h2>The ARC Challenge</h2>
                <p class="section-subtitle">Abstract Reasoning Corpus: The IQ test for AI</p>
            </div>
            
            <div class="section-content">
                <div class="arc-explainer">
                    <div class="explainer-text">
                        <h3>What is ARC?</h3>
                        <p>Each ARC task presents a few input-output grid pairs as <strong>training examples</strong>, then asks the model to predict the output for a new <strong>test input</strong>.</p>
                        
                        <div class="key-challenges">
                            <h4>Why is it hard?</h4>
                            <ul>
                                <li><strong>Few-shot learning:</strong> Only 2-5 examples per task</li>
                                <li><strong>Novel rules:</strong> Each task has a unique transformation</li>
                                <li><strong>Spatial reasoning:</strong> Patterns, symmetry, counting, topology</li>
                                <li><strong>No memorization:</strong> Test tasks differ from training</li>
                            </ul>
                        </div>
                        
                        <div class="intuition-box">
                            <div class="intuition-icon">üí°</div>
                            <div class="intuition-content">
                                <strong>Intuition:</strong> Think of ARC as a visual IQ test. You see a pattern transformation demonstrated, then must apply that same rule to a new input‚Äîwithout being told what the rule is.
                            </div>
                        </div>
                    </div>
                    
                    <div class="arc-demo-container">
                        <div class="puzzle-selector">
                            <label>Select Puzzle:</label>
                            <select id="puzzle-select">
                                <option value="0">Pattern Fill</option>
                                <option value="1">Color Mapping</option>
                                <option value="2">Object Counting</option>
                            </select>
                        </div>
                        
                        <div id="arc-sandbox">
                            <div class="example-pairs" id="example-pairs">
                                <!-- Training examples render here -->
                            </div>
                            <div class="test-area">
                                <div class="test-input-container">
                                    <h4>Test Input</h4>
                                    <div id="test-input-grid"></div>
                                </div>
                                <div class="test-output-container">
                                    <h4>Your Prediction</h4>
                                    <div id="test-output-grid" class="hidden-solution"></div>
                                    <button id="show-solution-btn" class="action-btn">Show Solution</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="takeaway">
                <strong>Takeaway:</strong> ARC requires genuine reasoning‚Äîpattern discovery from minimal examples‚Äînot pattern matching from massive datasets.
            </div>
            
            <div class="speaker-notes" data-duration-30s="ARC is an IQ test for AI: infer rules from 2-5 examples, apply to new inputs." data-duration-2min="ARC presents visual puzzles where you must infer the transformation rule from just 2-5 examples. Unlike ImageNet or language models, you can't memorize‚Äîeach test task is novel. This requires genuine abstraction and reasoning, which is why LLMs struggle despite their scale." data-duration-5min="The Abstract Reasoning Corpus is fundamentally different from typical ML benchmarks. Each task shows 2-5 input-output pairs demonstrating some transformation‚Äîmaybe 'fill the enclosed region' or 'count objects and create that many dots'. The test asks you to apply this rule to a new input. The catch? Each task has a UNIQUE rule, and test tasks differ from training tasks. You cannot memorize your way to success. This is why GPT-4 scores around 5% while humans score 85%. It requires the kind of fluid reasoning that current AI lacks."></div>
        </section>

        <!-- ========== SECTION 3: RLAN ARCHITECTURE ========== -->
        <section id="architecture" class="story-section" data-section="3">
            <div class="section-header">
                <h2>RLAN Architecture</h2>
                <p class="section-subtitle">Six modules working together for spatial reasoning</p>
            </div>
            
            <div class="section-content">
                <div class="architecture-overview">
                    <div id="architecture-diagram" class="arch-diagram">
                        <!-- SVG architecture diagram renders here -->
                    </div>
                    
                    <div class="module-cards">
                        <!-- Context Encoder -->
                        <div class="module-card" data-module="encoder">
                            <div class="module-header">
                                <div class="module-icon">üì•</div>
                                <h4>Context Encoder</h4>
                            </div>
                            <p class="module-summary">Encodes training examples into a unified context using cross-attention and FiLM conditioning.</p>
                            <button class="expand-btn">Deep Dive ‚ñº</button>
                            <div class="module-details hidden">
                                <div class="equation-box">
                                    <p class="equation-label">Pair difference encoding:</p>
                                    <div class="katex-equation">$$F_{pair}^{(k)} = \text{Conv}\left([F_{in}^{(k)}; F_{out}^{(k)}; F_{out}^{(k)} - F_{in}^{(k)}]\right)$$</div>
                                    <div class="variable-definitions">
                                        <p><strong>Variables:</strong></p>
                                        <ul>
                                            <li>$F_{in}^{(k)}$ = encoded input features for training pair $k$</li>
                                            <li>$F_{out}^{(k)}$ = encoded output features for training pair $k$</li>
                                            <li>$F_{out}^{(k)} - F_{in}^{(k)}$ = explicit difference (what changed)</li>
                                            <li>$[\cdot ; \cdot]$ = concatenation along channel dimension</li>
                                        </ul>
                                    </div>
                                </div>
                                <div class="equation-box">
                                    <p class="equation-label">Cross-attention aggregation:</p>
                                    <div class="katex-equation">$$\mathbf{c} = \frac{1}{N_q} \sum_{i=1}^{N_q} \text{CrossAttn}(Q_i, K=Z, V=Z)$$</div>
                                    <div class="variable-definitions">
                                        <p><strong>Variables:</strong></p>
                                        <ul>
                                            <li>$Q_i$ = learnable query vectors ($N_q=4$ queries)</li>
                                            <li>$Z = [z^{(1)}, ..., z^{(K)}]$ = stacked context from $K$ training pairs</li>
                                            <li>$\mathbf{c} \in \mathbb{R}^D$ = unified context vector</li>
                                        </ul>
                                    </div>
                                </div>
                                <p><strong>Purpose:</strong> Captures the common transformation pattern demonstrated across all training pairs.</p>
                            </div>
                        </div>
                        
                        <!-- DSC -->
                        <div class="module-card" data-module="dsc">
                            <div class="module-header">
                                <div class="module-icon">üéØ</div>
                                <h4>Dynamic Saliency Controller (DSC)</h4>
                            </div>
                            <p class="module-summary">Iteratively discovers "clue anchors"‚Äîwhere to focus attention for solving the task.</p>
                            <button class="expand-btn">Deep Dive ‚ñº</button>
                            <div class="module-details hidden">
                                <div class="equation-box">
                                    <p class="equation-label">Attention map computation:</p>
                                    <div class="katex-equation">$$M_t = \text{Softmax}\left(\frac{\text{UNet}(F, H_{t-1})}{\tau}\right)$$</div>
                                    <div class="variable-definitions">
                                        <p><strong>Variables:</strong></p>
                                        <ul>
                                            <li>$M_t \in [0,1]^{H \times W}$ = attention map at step $t$ (probability distribution)</li>
                                            <li>$F \in \mathbb{R}^{H \times W \times D}$ = grid features from encoder</li>
                                            <li>$H_{t-1} \in \mathbb{R}^D$ = hidden state from previous step</li>
                                            <li>$\tau$ = temperature (controls sharpness, anneals from 5.0 to 0.1)</li>
                                        </ul>
                                    </div>
                                </div>
                                <div class="equation-box">
                                    <p class="equation-label">Centroid extraction (differentiable):</p>
                                    <div class="katex-equation">$$\mu_y^t = \sum_{i,j} M_t^{(i,j)} \cdot i, \quad \mu_x^t = \sum_{i,j} M_t^{(i,j)} \cdot j$$</div>
                                    <div class="variable-definitions">
                                        <p><strong>Variables:</strong></p>
                                        <ul>
                                            <li>$\mu_y^t, \mu_x^t$ = center of mass of attention (the "clue" location)</li>
                                            <li>$(i, j)$ = grid coordinates</li>
                                        </ul>
                                    </div>
                                </div>
                                <p><strong>Intuition:</strong> Like a human's eye automatically finding key objects in a scene.</p>
                            </div>
                        </div>
                        
                        <!-- MSRE -->
                        <div class="module-card" data-module="msre">
                            <div class="module-header">
                                <div class="module-icon">üìç</div>
                                <h4>Multi-Scale Relative Encoding (MSRE)</h4>
                            </div>
                            <p class="module-summary">Transforms absolute coordinates to anchor-relative, providing translation and scale invariance.</p>
                            <button class="expand-btn">Deep Dive ‚ñº</button>
                            <div class="module-details hidden">
                                <div class="equation-box">
                                    <p class="equation-label">Absolute relative coordinates:</p>
                                    <div class="katex-equation">$$P_{abs}^t(i, j) = [i - \mu_y, j - \mu_x]$$</div>
                                    <div class="variable-definitions">
                                        <p><strong>Variables:</strong></p>
                                        <ul>
                                            <li>$P_{abs}^t$ = pixel-distance from anchor (in pixels)</li>
                                            <li>$(i, j)$ = current cell position</li>
                                            <li>$(\mu_y, \mu_x)$ = clue anchor centroid from DSC</li>
                                        </ul>
                                    </div>
                                </div>
                                <div class="equation-box">
                                    <p class="equation-label">Normalized relative coordinates:</p>
                                    <div class="katex-equation">$$P_{norm}^t(i, j) = \left[\frac{i - \mu_y}{\max(H, W)}, \frac{j - \mu_x}{\max(H, W)}\right]$$</div>
                                    <div class="variable-definitions">
                                        <p><strong>Variables:</strong></p>
                                        <ul>
                                            <li>$P_{norm}^t$ = scale-invariant position (fraction of grid size)</li>
                                            <li>$H, W$ = grid height and width</li>
                                        </ul>
                                    </div>
                                </div>
                                <div class="equation-box">
                                    <p class="equation-label">Log-polar coordinates (for rotation):</p>
                                    <div class="katex-equation">$$r = \log(\sqrt{(i-\mu_y)^2 + (j-\mu_x)^2} + 1), \quad \phi = \arctan2(j-\mu_x, i-\mu_y)$$</div>
                                    <div class="variable-definitions">
                                        <p><strong>Variables:</strong></p>
                                        <ul>
                                            <li>$r$ = log-radius (distance from anchor)</li>
                                            <li>$\phi$ = angle from anchor (for rotational patterns)</li>
                                        </ul>
                                    </div>
                                </div>
                                <p><strong>Key Insight:</strong> "Move to anchor" works regardless of where the anchor appears!</p>
                            </div>
                        </div>
                        
                        <!-- LCR -->
                        <div class="module-card" data-module="lcr">
                            <div class="module-header">
                                <div class="module-icon">üî¢</div>
                                <h4>Latent Counting Registers (LCR)</h4>
                            </div>
                            <p class="module-summary">Enables non-spatial numerical reasoning‚Äîcounting colors within attended regions.</p>
                            <button class="expand-btn">Deep Dive ‚ñº</button>
                            <div class="module-details hidden">
                                <div class="equation-box">
                                    <p class="equation-label">Soft color counting:</p>
                                    <div class="katex-equation">$$\mathbf{c}_t = \sum_{i,j} M_t^{(i,j)} \cdot \text{OneHot}(X_{i,j}) \in \mathbb{R}^{C}$$</div>
                                    <div class="variable-definitions">
                                        <p><strong>Variables:</strong></p>
                                        <ul>
                                            <li>$\mathbf{c}_t \in \mathbb{R}^{10}$ = color histogram for clue $t$</li>
                                            <li>$M_t^{(i,j)}$ = attention weight at position $(i,j)$</li>
                                            <li>$X_{i,j}$ = color value (0-9) at position $(i,j)$</li>
                                            <li>$C = 10$ = number of color classes</li>
                                        </ul>
                                    </div>
                                </div>
                                <p><strong>Purpose:</strong> Enables tasks like "fill with majority color" or "output size equals object count".</p>
                                <p><strong>Broadcast:</strong> Counts are spatially broadcast so every pixel knows global color statistics.</p>
                            </div>
                        </div>
                        
                        <!-- SPH -->
                        <div class="module-card" data-module="sph">
                            <div class="module-header">
                                <div class="module-icon">üìä</div>
                                <h4>Symbolic Predicate Heads (SPH)</h4>
                            </div>
                            <p class="module-summary">Detects high-level properties (symmetry, object count) for conditional IF-THEN logic.</p>
                            <button class="expand-btn">Deep Dive ‚ñº</button>
                            <div class="module-details hidden">
                                <div class="equation-box">
                                    <p class="equation-label">Predicate computation:</p>
                                    <div class="katex-equation">$$p_k = \sigma\left(\text{MLP}_k\left(\text{GlobalPool}(F_\theta(X))\right)\right) \in [0, 1]$$</div>
                                    <div class="variable-definitions">
                                        <p><strong>Variables:</strong></p>
                                        <ul>
                                            <li>$p_k$ = predicate $k$ (soft boolean, 0=false, 1=true)</li>
                                            <li>$\sigma$ = sigmoid function</li>
                                            <li>$\text{GlobalPool}$ = average + max pooling over spatial dimensions</li>
                                            <li>$F_\theta(X)$ = encoder features</li>
                                            <li>$N_p = 8$ predicates learned end-to-end</li>
                                        </ul>
                                    </div>
                                </div>
                                <div class="equation-box">
                                    <p class="equation-label">Predicate gating:</p>
                                    <div class="katex-equation">$$H'_t = H_t \odot \sigma(\text{MLP}_{gate}(\mathbf{p}))$$</div>
                                    <div class="variable-definitions">
                                        <p><strong>Variables:</strong></p>
                                        <ul>
                                            <li>$\mathbf{p} = [p_1, ..., p_k]$ = predicate vector</li>
                                            <li>$\odot$ = element-wise multiplication (gates hidden state)</li>
                                        </ul>
                                    </div>
                                </div>
                                <p><strong>Example:</strong> If $p_1 \approx 0.95$ (symmetric), gate activates "vertical flip" neurons.</p>
                            </div>
                        </div>
                        
                        <!-- Recursive Solver -->
                        <div class="module-card highlight" data-module="solver">
                            <div class="module-header">
                                <div class="module-icon">üîÑ</div>
                                <h4>Recursive Solver</h4>
                            </div>
                            <p class="module-summary">The heart of RLAN‚Äîiteratively refines predictions using ConvGRU until convergence.</p>
                            <button class="expand-btn">Deep Dive ‚ñº</button>
                            <div class="module-details hidden">
                                <div class="equation-box">
                                    <p class="equation-label">Feature assembly (solver input):</p>
                                    <div class="katex-equation">$$\hat{X}_s = \text{Concat}\left(X, \{P^t\}_{t=1}^{N_{clues}}, C_{broadcast}, H_{s-1}\right)$$</div>
                                    <div class="variable-definitions">
                                        <p><strong>Variables:</strong></p>
                                        <ul>
                                            <li>$X$ = original grid (10 color channels)</li>
                                            <li>$\{P^t\}$ = relative coordinates from each clue ($6 \times N_{clues}$ channels)</li>
                                            <li>$C_{broadcast}$ = color counts broadcast spatially</li>
                                            <li>$H_{s-1}$ = hidden state from previous iteration</li>
                                        </ul>
                                    </div>
                                </div>
                                <div class="equation-box">
                                    <p class="equation-label">ConvGRU recurrence:</p>
                                    <div class="katex-equation">$$H_s = \text{ConvGRU}(\hat{X}_s, H_{s-1})$$</div>
                                    <div class="katex-equation">$$\text{logits}_s = \text{clamp}(\text{OutputHead}(H_s), -50, 50)$$</div>
                                    <div class="variable-definitions">
                                        <p><strong>Variables:</strong></p>
                                        <ul>
                                            <li>$H_s$ = hidden state at iteration $s$</li>
                                            <li>$\text{logits}_s \in \mathbb{R}^{H \times W \times 10}$ = per-pixel color predictions</li>
                                            <li>Clamp prevents numerical instability</li>
                                            <li>$S = 6$ typical iterations; halting is learned</li>
                                        </ul>
                                    </div>
                                </div>
                                <p><strong>Key insight:</strong> Each iteration corrects errors from the previous‚Äîlike sketching then refining.</p>
                            </div>
                        </div>
                        
                        <!-- HyperLoRA (NEW - Meta-Learning) -->
                        <div class="module-card" data-module="hyperlora">
                            <div class="module-header">
                                <div class="module-icon">üß¨</div>
                                <h4>HyperLoRA (Meta-Learning)</h4>
                            </div>
                            <p class="module-summary">Predicts task-specific weight adaptations from context‚Äîenabling fast adaptation without gradient updates at inference.</p>
                            <button class="expand-btn">Deep Dive ‚ñº</button>
                            <div class="module-details hidden">
                                <div class="equation-box">
                                    <p class="equation-label">Context pooling for HyperNet input:</p>
                                    <div class="katex-equation">$$\mathbf{z}_{struct} = \text{StructProjector}\left(\text{AvgPool}(F_{support})\right)$$</div>
                                    <div class="variable-definitions">
                                        <p><strong>Variables:</strong></p>
                                        <ul>
                                            <li>$F_{support}$ = encoded features from all training pairs</li>
                                            <li>$\mathbf{z}_{struct}$ = structure latent (captures the transformation rule)</li>
                                        </ul>
                                    </div>
                                </div>
                                <div class="equation-box">
                                    <p class="equation-label">LoRA weight prediction:</p>
                                    <div class="katex-equation">$$\Delta W_A, \Delta W_B = \text{HyperNet}(\mathbf{z}_{struct})$$</div>
                                    <div class="katex-equation">$$W' = W + \alpha \cdot \Delta W_A \cdot \Delta W_B^T$$</div>
                                    <div class="variable-definitions">
                                        <p><strong>Variables:</strong></p>
                                        <ul>
                                            <li>$\Delta W_A \in \mathbb{R}^{d \times r}$, $\Delta W_B \in \mathbb{R}^{d \times r}$ = low-rank adapters (rank $r=8$)</li>
                                            <li>$W$ = original GRU/output head weights</li>
                                            <li>$W'$ = task-adapted weights</li>
                                            <li>$\alpha$ = scaling factor (default 1.0)</li>
                                        </ul>
                                    </div>
                                </div>
                                <p><strong>Why this matters:</strong> Unlike fine-tuning (needs gradients), HyperLoRA adapts weights <em>instantly</em> from context‚Äîenabling true few-shot generalization.</p>
                            </div>
                        </div>
                        
                        <!-- LOO Training (NEW - Meta-Learning Training) -->
                        <div class="module-card" data-module="loo">
                            <div class="module-header">
                                <div class="module-icon">üéì</div>
                                <h4>LOO Training (Leave-One-Out)</h4>
                            </div>
                            <p class="module-summary">Trains HyperLoRA to generalize by predicting held-out pairs from remaining context.</p>
                            <button class="expand-btn">Deep Dive ‚ñº</button>
                            <div class="module-details hidden">
                                <div class="equation-box">
                                    <p class="equation-label">Leave-One-Out loss:</p>
                                    <div class="katex-equation">$$\mathcal{L}_{LOO} = \frac{1}{N} \sum_{k=1}^{N} \mathcal{L}_{CE}\left(\hat{Y}^{(-k)}, Y^{(k)}\right)$$</div>
                                    <div class="variable-definitions">
                                        <p><strong>Variables:</strong></p>
                                        <ul>
                                            <li>$N$ = number of training pairs</li>
                                            <li>$\hat{Y}^{(-k)}$ = prediction using LoRA from pairs $\{1,...,N\} \setminus \{k\}$</li>
                                            <li>$Y^{(k)}$ = ground truth output for held-out pair $k$</li>
                                        </ul>
                                    </div>
                                </div>
                                <p><strong>Intuition:</strong> If HyperLoRA can predict pair 3 using only pairs 1 and 2, it learned the <em>general rule</em>, not memorized specific pixels.</p>
                                <p><strong>Training signal:</strong> "Learn weights that transfer to unseen examples"</p>
                            </div>
                        </div>
                        
                        <!-- TTA + ACW (NEW - Inference Robustness) -->
                        <div class="module-card" data-module="tta">
                            <div class="module-header">
                                <div class="module-icon">üîÑ</div>
                                <h4>TTA + ACW (Inference Robustness)</h4>
                            </div>
                            <p class="module-summary">Test-Time Augmentation with Augmented Confidence Weighting for robust predictions.</p>
                            <button class="expand-btn">Deep Dive ‚ñº</button>
                            <div class="module-details hidden">
                                <div class="equation-box">
                                    <p class="equation-label">Test-Time Augmentation:</p>
                                    <div class="katex-equation">$$\hat{Y} = \text{Vote}\left(\{T^{-1}(\text{Model}(T(X)))\}_{T \in \mathcal{T}}\right)$$</div>
                                    <div class="variable-definitions">
                                        <p><strong>Variables:</strong></p>
                                        <ul>
                                            <li>$\mathcal{T}$ = set of 32 augmentations (8 dihedral √ó 4 color permutations)</li>
                                            <li>$T$ = augmentation, $T^{-1}$ = inverse augmentation</li>
                                            <li>$\text{Vote}$ = majority voting across predictions</li>
                                        </ul>
                                    </div>
                                </div>
                                <div class="equation-box">
                                    <p class="equation-label">Augmented Confidence Weighting (ACW):</p>
                                    <div class="katex-equation">$$w_i = 1 - \frac{1}{|\mathcal{T}|} \sum_{j \neq i} \mathbb{1}[\hat{Y}_i \neq \hat{Y}_j]$$</div>
                                    <div class="katex-equation">$$\hat{Y}_{final} = \arg\max_c \sum_{i} w_i \cdot \mathbb{1}[\hat{Y}_i = c]$$</div>
                                    <div class="variable-definitions">
                                        <p><strong>Variables:</strong></p>
                                        <ul>
                                            <li>$w_i$ = weight for view $i$ (higher if consistent with others)</li>
                                            <li>More consistent predictions get higher voting weight</li>
                                        </ul>
                                    </div>
                                </div>
                                <p><strong>Why this works:</strong> Correct predictions are usually consistent across augmentations; wrong ones vary randomly.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="takeaway">
                <strong>Takeaway:</strong> RLAN's modular architecture combines spatial reasoning (DSC, MSRE, Solver) with meta-learning (HyperLoRA, LOO) and robust inference (TTA, ACW) for true few-shot generalization.
            </div>
            
            <!-- PSEUDOCODE SECTION -->
            <div class="pseudocode-section">
                <h3>üìã RLAN Algorithm Pseudocode</h3>
                <div class="pseudocode-container">
                    <pre class="pseudocode"><code><span class="keyword">function</span> <span class="function">RLAN_Forward</span>(train_pairs, test_input):
    <span class="comment"># Step 1: Context Encoding (before DSC and MSRE)</span>
    context = <span class="function">ContextEncoder</span>(train_pairs)  <span class="comment"># c ‚àà ‚Ñù^D</span>
    
    <span class="comment"># Step 2: Grid Encoding</span>
    F = <span class="function">GridEncoder</span>(test_input)  <span class="comment"># F ‚àà ‚Ñù^(H√óW√óD)</span>
    F = <span class="function">FiLM</span>(F, context)  <span class="comment"># condition on context</span>
    
    <span class="comment"># Step 3: HyperLoRA - Meta-Learning Weight Adaptation (NEW!)</span>
    <span class="keyword">if</span> use_hyperlora:
        z_struct = <span class="function">StructProjector</span>(<span class="function">AvgPool</span>(F_support))  <span class="comment"># structure latent</span>
        ŒîW_A, ŒîW_B = <span class="function">HyperNet</span>(z_struct)  <span class="comment"># predict LoRA weights</span>
        <span class="comment"># Apply LoRA to GRU and output head: W' = W + ŒîW_A @ ŒîW_B.T</span>
    
    <span class="comment"># Step 4: Dynamic Saliency Controller (find clue anchors)</span>
    clues = []
    H_dsc = zeros(D)
    <span class="keyword">for</span> t <span class="keyword">in</span> 1..N_clues:
        M_t = <span class="function">Softmax</span>(<span class="function">UNet</span>(F, H_dsc) / œÑ)  <span class="comment"># attention map</span>
        Œº_t = <span class="function">Centroid</span>(M_t)  <span class="comment"># (Œº_y, Œº_x) anchor location</span>
        clues.append((M_t, Œº_t))
        H_dsc = <span class="function">GRU</span>(H_dsc, <span class="function">Pool</span>(M_t ‚äô F))
    
    <span class="comment"># Step 5: Multi-Scale Relative Encoding</span>
    P_rel = []
    <span class="keyword">for</span> (M_t, Œº_t) <span class="keyword">in</span> clues:
        P_abs = <span class="function">RelativeCoords</span>(Œº_t)  <span class="comment"># P_abs(i,j) = [i-Œº_y, j-Œº_x]</span>
        P_norm = P_abs / max(H, W)  <span class="comment"># scale-invariant</span>
        P_polar = <span class="function">LogPolar</span>(Œº_t)  <span class="comment"># (r, œÜ) for rotation</span>
        P_rel.append(<span class="function">Concat</span>(P_abs, P_norm, P_polar))
    
    <span class="comment"># Step 6: Latent Counting Registers</span>
    C = []
    <span class="keyword">for</span> (M_t, _) <span class="keyword">in</span> clues:
        c_t = Œ£_{i,j} M_t[i,j] ¬∑ <span class="function">OneHot</span>(X[i,j])  <span class="comment"># color histogram</span>
        C.append(c_t)
    C_broadcast = <span class="function">SpatialBroadcast</span>(C)  <span class="comment"># ‚Ñù^(H√óW√ó10)</span>
    
    <span class="comment"># Step 7: Symbolic Predicate Heads</span>
    p = <span class="function">SPH</span>(<span class="function">GlobalPool</span>(F))  <span class="comment"># p_k ‚àà [0,1] for K predicates</span>
    g = <span class="function">SizePredictor</span>(p)  <span class="comment"># output grid size</span>
    
    <span class="comment"># Step 8: Recursive Solver (iterative refinement with task-adapted weights)</span>
    H = zeros(H_out, W_out, D)
    <span class="keyword">for</span> s <span class="keyword">in</span> 1..S_max:
        X_hat = <span class="function">Concat</span>(X, P_rel, C_broadcast, H)
        H = <span class="function">ConvGRU</span>(X_hat, H)  <span class="comment"># uses adapted weights W' if HyperLoRA active</span>
        logits = <span class="function">clamp</span>(<span class="function">OutputHead</span>(H), -50, 50)
        <span class="keyword">if</span> <span class="function">HaltingCondition</span>(H):
            <span class="keyword">break</span>
    
    <span class="keyword">return</span> <span class="function">Stablemax</span>(logits)  <span class="comment"># final grid prediction</span>

<span class="comment"># ===========================================</span>
<span class="comment"># TRAINING: How HyperLoRA learns to generalize</span>
<span class="comment"># ===========================================</span>
<span class="keyword">function</span> <span class="function">RLAN_Train_Step</span>(batch):
    <span class="comment"># Standard task loss</span>
    pred = <span class="function">RLAN_Forward</span>(batch.train_pairs, batch.test_input)
    L_task = <span class="function">CrossEntropy</span>(pred, batch.test_output)
    
    <span class="comment"># LOO Loss: Leave-One-Out meta-learning (trains HyperLoRA to generalize)</span>
    L_loo = 0
    <span class="keyword">for</span> k <span class="keyword">in</span> 1..N:  <span class="comment"># for each training pair</span>
        pairs_minus_k = train_pairs <span class="keyword">without</span> pair[k]  <span class="comment"># hold out pair k</span>
        z_struct_k = <span class="function">StructProjector</span>(<span class="function">Encode</span>(pairs_minus_k))
        ŒîW = <span class="function">HyperNet</span>(z_struct_k)  <span class="comment"># predict weights from N-1 pairs</span>
        pred_k = <span class="function">SolveWithLoRA</span>(pair[k].input, ŒîW)  <span class="comment"># predict held-out</span>
        L_loo += <span class="function">CrossEntropy</span>(pred_k, pair[k].output)
    L_loo = L_loo / N
    
    <span class="comment"># Total loss with orthogonality regularization</span>
    z_content = <span class="function">ContentProjector</span>(test_features)
    L_ortho = |z_struct ¬∑ z_content|¬≤  <span class="comment"># structure ‚ä• content</span>
    
    <span class="keyword">return</span> L_task + Œª_loo * L_loo + Œª_ortho * L_ortho

<span class="comment"># ===========================================</span>
<span class="comment"># INFERENCE: TTA + ACW for robust predictions</span>
<span class="comment"># ===========================================</span>
<span class="keyword">function</span> <span class="function">RLAN_Inference</span>(train_pairs, test_input):
    predictions = []
    
    <span class="comment"># Test-Time Augmentation: 8 dihedral √ó 4 color perms = 32 views</span>
    <span class="keyword">for</span> T <span class="keyword">in</span> Dihedral8 √ó ColorPerms4:
        aug_pairs = <span class="function">ApplyAug</span>(train_pairs, T)
        aug_test = <span class="function">ApplyAug</span>(test_input, T)
        pred = <span class="function">RLAN_Forward</span>(aug_pairs, aug_test)
        pred = <span class="function">InverseAug</span>(pred, T)  <span class="comment"># undo augmentation</span>
        predictions.append(pred)
    
    <span class="comment"># ACW: Augmented Confidence Weighting</span>
    <span class="keyword">for</span> i <span class="keyword">in</span> predictions:
        consistency[i] = 1 - <span class="function">MeanDisagreement</span>(pred[i], all_others)
    
    <span class="keyword">return</span> <span class="function">WeightedVote</span>(predictions, consistency)</code></pre>
                </div>
                
                <div class="formula-reference">
                    <h4>Key Formulas</h4>
                    <div class="formula-grid">
                        <div class="formula-item">
                            <span class="formula-name">DSC Attention:</span>
                            <span class="katex-inline">$M_t = \text{Softmax}(\text{UNet}(F, H_{t-1})/\tau)$</span>
                        </div>
                        <div class="formula-item">
                            <span class="formula-name">Relative Coords:</span>
                            <span class="katex-inline">$P_{rel}(i,j) = [i - \mu_y, j - \mu_x]$</span>
                        </div>
                        <div class="formula-item">
                            <span class="formula-name">Color Count:</span>
                            <span class="katex-inline">$\mathbf{c}_t = \sum_{i,j} M_t^{(i,j)} \cdot \text{OneHot}(X_{i,j})$</span>
                        </div>
                        <div class="formula-item">
                            <span class="formula-name">Solver:</span>
                            <span class="katex-inline">$H_{s} = \text{ConvGRU}(\hat{X}_s, H_{s-1})$</span>
                        </div>
                        <div class="formula-item">
                            <span class="formula-name">HyperLoRA:</span>
                            <span class="katex-inline">$W' = W + \alpha \Delta W_A \Delta W_B^T$</span>
                        </div>
                        <div class="formula-item">
                            <span class="formula-name">LOO Loss:</span>
                            <span class="katex-inline">$\mathcal{L}_{LOO} = \frac{1}{N}\sum_k \mathcal{L}(\hat{Y}^{(-k)}, Y^{(k)})$</span>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="speaker-notes" data-duration-30s="Nine modules: encode, HyperLoRA adapts weights, DSC focuses, MSRE relativizes, count, size, solve, TTA+ACW at inference." data-duration-2min="RLAN has nine key modules. The Context Encoder builds task representations. HyperLoRA predicts task-specific weight adaptations. DSC learns where to focus attention. MSRE converts to relative coordinates. LCR enables counting. SPH detects predicates. The Recursive Solver iteratively refines predictions. During training, LOO loss teaches HyperLoRA to generalize. At inference, TTA with ACW aggregates predictions across augmentations." data-duration-5min="Let me walk through the enhanced architecture. The Context Encoder processes training examples into a task representation. NEW: HyperLoRA takes this and predicts task-specific LoRA weight deltas‚Äîadapting the GRU and output head per task without gradient updates. DSC finds attention hotspots. MSRE converts to relative coordinates for translation invariance. LCR counts colors. SPH detects predicates. The Recursive Solver uses task-adapted weights to iteratively refine. During training, LOO loss holds out pairs to teach HyperLoRA true generalization. At inference, we run 32 augmented views (TTA) and use ACW to weight-vote based on prediction consistency."></div>
        </section>

        <!-- ========== NEW SECTION: VISUAL WALKTHROUGH ========== -->
        <section id="visual-walkthrough" class="story-section" data-section="3.5">
            <div class="section-header">
                <h2>Visual Walkthrough: RLAN in Action</h2>
                <p class="section-subtitle">Watch how each module processes a real ARC example</p>
            </div>
            
            <div class="section-content">
                <!-- EXAMPLE -->
                <div class="example-walkthrough" id="walkthrough-example">
                    <div class="example-header">
                        <h3>Object Movement: "Move to the Marker"</h3>
                    </div>
                    
                    <div class="example-puzzle">
                        <div class="puzzle-grids">
                            <div class="grid-panel">
                                <h4>Input</h4>
                                <div class="arc-grid-viz" id="easy-input">
                                    <!-- 4x4 grid: Grey square at (0,0), Red marker at (3,3) -->
                                </div>
                            </div>
                            <div class="arrow-viz">‚Üí</div>
                            <div class="grid-panel">
                                <h4>Output</h4>
                                <div class="arc-grid-viz" id="easy-output">
                                    <!-- Grey square moved to (3,3) -->
                                </div>
                            </div>
                        </div>
                        <p class="puzzle-description"><strong>Task:</strong> Move the grey square to wherever the red pixel is.</p>
                    </div>
                    
                    <div class="module-flow">
                        <div class="flow-step">
                            <div class="step-number">1</div>
                            <div class="step-content">
                                <h4>üéØ DSC: Find the Clue</h4>
                                <div class="step-viz" id="easy-dsc-viz">
                                    <div class="attention-overlay">
                                        <!-- Attention heatmap showing focus on red pixel -->
                                    </div>
                                </div>
                                <p>Attention map $M_1$ focuses on the <strong>red pixel</strong> at position (3,3).</p>
                                <p class="equation-inline">Centroid: $\mu_1 = (3, 3)$</p>
                            </div>
                        </div>
                        
                        <div class="flow-step">
                            <div class="step-number">2</div>
                            <div class="step-content">
                                <h4>üìç MSRE: Re-project Coordinates</h4>
                                <div class="step-viz" id="easy-msre-viz">
                                    <div class="coordinate-transform">
                                        <!-- Before/After coordinate visualization -->
                                    </div>
                                </div>
                                <p>Every pixel now has coordinates <strong>relative to the red pixel</strong>:</p>
                                <ul class="transform-list">
                                    <li>Grey square at (0,0) ‚Üí Relative: <strong>(-3, -3)</strong></li>
                                    <li>Red marker at (3,3) ‚Üí Relative: <strong>(0, 0)</strong> (the anchor)</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="flow-step">
                            <div class="step-number">3</div>
                            <div class="step-content">
                                <h4>üîÑ Solver: Learn the Rule</h4>
                                <p>The rule in relative space: <strong>"Copy what's at (-3,-3) to position (0,0)"</strong></p>
                                <p class="key-insight">‚ú® This works <em>anywhere</em> the red pixel appears‚Äîthe same relative rule applies!</p>
                            </div>
                        </div>
                    </div>
                    
                    <div class="insight-box">
                        <div class="insight-icon">üí°</div>
                        <div class="insight-content">
                            <strong>Why This Matters:</strong> A traditional CNN would memorize "move from (0,0) to (3,3)". Move the red pixel, and it fails completely. RLAN learns "move TO the anchor"‚Äîa position-invariant rule.
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="takeaway">
                <strong>Takeaway:</strong> RLAN uses clue anchors and relative coordinates to learn position-invariant rules from minimal examples.
            </div>
            
            <div class="speaker-notes" data-duration-30s="DSC finds anchor, MSRE re-projects to relative coords, solver learns invariant rule." data-duration-2min="Move object to marker‚ÄîDSC finds the red pixel as anchor, MSRE re-projects everything relative to it, solver learns 'place at (0,0) relative to anchor'. This position-invariant rule works anywhere the anchor appears." data-duration-5min="This example shows RLAN's core power. DSC finds one clue (the red pixel), MSRE converts all coords to be relative to this anchor, and the solver learns a position-invariant rule. A traditional CNN would memorize absolute positions, but RLAN learns the abstract rule 'move TO the anchor' which generalizes to any position."></div>
        </section>

        <!-- ========== SECTION 4: DSC & COORDINATES ========== -->
        <section id="dsc-coordinates" class="story-section" data-section="4">
            <div class="section-header">
                <h2>DSC & Relative Coordinates</h2>
                <p class="section-subtitle">How RLAN achieves translation invariance</p>
            </div>
            
            <div class="section-content">
                <div class="coordinate-demo">
                    <div class="demo-explanation">
                        <h3>The Problem with Absolute Coordinates</h3>
                        <p>If a pattern appears at position (2,3) in training but (5,7) in test, absolute encodings fail. The model has never seen this exact position!</p>
                        
                        <div class="intuition-box">
                            <div class="intuition-icon">üí°</div>
                            <div class="intuition-content">
                                <strong>Intuition:</strong> Imagine giving directions. "Go to latitude 40.7128¬∞" is brittle. "Go 2 blocks north from here" is robust‚Äîit works from any starting point.
                            </div>
                        </div>
                        
                        <h3>The Solution: Anchor-Relative Encoding</h3>
                        <ol class="solution-steps">
                            <li><strong>DSC finds anchors:</strong> Attention highlights salient regions</li>
                            <li><strong>MSRE re-centers:</strong> All positions measured relative to anchor</li>
                            <li><strong>Translation invariance:</strong> Same pattern = same relative encoding, anywhere on grid</li>
                        </ol>
                    </div>
                    
                    <div class="coordinate-visualization">
                        <div class="viz-controls">
                            <button id="toggle-coords" class="action-btn">Toggle: Absolute ‚Üî Relative</button>
                            <span id="coord-mode-label">Mode: Absolute</span>
                        </div>
                        
                        <div class="viz-container">
                            <div class="grid-with-coords" id="coord-demo-grid">
                                <!-- Grid with coordinate overlays -->
                            </div>
                            <div class="anchor-indicator">
                                <div class="anchor-legend">
                                    <span class="anchor-dot"></span> Anchor Point (DSC attention peak)
                                </div>
                            </div>
                        </div>
                        
                        <div class="heatmap-section">
                            <h4>DSC Attention Heatmap</h4>
                            <p class="proxy-note">‚ö†Ô∏è Proxy visualization (heuristic-based). See docs for logging real attention maps.</p>
                            <div id="dsc-heatmap">
                                <!-- Heatmap overlay renders here -->
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="takeaway">
                <strong>Takeaway:</strong> DSC + MSRE gives RLAN translation invariance‚Äîpatterns are recognized regardless of where they appear on the grid.
            </div>
            
            <div class="speaker-notes" data-duration-30s="Relative coordinates from attention anchors give translation invariance." data-duration-2min="Standard neural nets use absolute positions‚Äîcell (2,3) is encoded differently from (5,7). But if the same pattern moves, we want the same representation! DSC finds attention anchors‚Äîsalient points like corners or edges. MSRE then measures all positions relative to these anchors. Same pattern = same encoding, regardless of absolute position." data-duration-5min="This is one of RLAN's key innovations. Consider a simple pattern that appears at (2,3) in training but (5,7) in test. With absolute position encoding, the model has literally never seen these positions paired with this pattern. It fails. RLAN solves this with anchor-relative encoding. DSC uses attention to find salient anchor points‚Äîthink edges, corners, color boundaries. MSRE then re-expresses all positions as offsets from these anchors. Now the pattern 'red cell 2 steps right of anchor' encodes identically regardless of absolute position. This is true translation invariance, and it's why RLAN generalizes where others fail."></div>
        </section>

        <!-- ========== SECTION 5: RECURSIVE SOLVER ========== -->
        <section id="solver" class="story-section" data-section="5">
            <div class="section-header">
                <h2>Recursive Solver</h2>
                <p class="section-subtitle">Watch RLAN think step-by-step</p>
            </div>
            
            <div class="section-content">
                <div class="solver-demo">
                    <div class="solver-explanation">
                        <h3>Iterative Refinement</h3>
                        <p>Unlike single-pass models, RLAN's solver <em>iterates</em>. Each step refines the previous prediction, correcting errors progressively.</p>
                        
                        <div class="equation-box">
                            <p class="equation-label">ConvGRU recurrence:</p>
                            <div class="katex-equation">$$h^{(t+1)} = (1 - z^{(t)}) \odot h^{(t)} + z^{(t)} \odot \tilde{h}^{(t)}$$</div>
                        </div>
                        
                        <div class="intuition-box">
                            <div class="intuition-icon">üí°</div>
                            <div class="intuition-content">
                                <strong>Intuition:</strong> Like an artist: first sketch the rough shape, then add details, then fix mistakes. Each pass builds on the last.
                            </div>
                        </div>
                        
                        <div class="metrics-comparison">
                            <h4>Solver Improvement Evidence</h4>
                            <table class="evidence-table">
                                <tr>
                                    <th>Metric</th>
                                    <th>Epoch 1</th>
                                    <th>Epoch 3</th>
                                    <th>Change</th>
                                </tr>
                                <tr>
                                    <td>Solver Improvement</td>
                                    <td>-0.6%</td>
                                    <td>+84.9%</td>
                                    <td class="positive">+85.5%</td>
                                </tr>
                                <tr>
                                    <td>Attn Entropy</td>
                                    <td>3.07</td>
                                    <td>0.06</td>
                                    <td class="positive">50√ó sharper</td>
                                </tr>
                            </table>
                        </div>
                    </div>
                    
                    <div class="solver-stepper">
                        <h4>Step Through Solver Iterations</h4>
                        <div class="stepper-controls">
                            <button id="step-prev" class="step-btn" disabled>‚Üê Prev</button>
                            <span id="step-counter">Iteration 0 / 5</span>
                            <button id="step-next" class="step-btn">Next ‚Üí</button>
                            <button id="step-play" class="step-btn play-btn">‚ñ∂ Auto-Play</button>
                        </div>
                        
                        <div class="stepper-visualization">
                            <div class="iteration-grid-container">
                                <div id="iteration-grid">
                                    <!-- Current iteration grid renders here -->
                                </div>
                                <div class="iteration-info">
                                    <div class="confidence-meter">
                                        <label>Confidence:</label>
                                        <div class="meter-bar">
                                            <div id="confidence-fill" class="meter-fill"></div>
                                        </div>
                                        <span id="confidence-value">0%</span>
                                    </div>
                                    <div class="halt-probability">
                                        <label>Halt Prob:</label>
                                        <span id="halt-prob-value">0.05</span>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="iteration-timeline">
                                <div id="timeline-track">
                                    <!-- Timeline markers render here -->
                                </div>
                            </div>
                        </div>
                        
                        <p class="proxy-note">‚ö†Ô∏è Proxy visualization showing simulated refinement. See docs for logging real iteration states.</p>
                    </div>
                </div>
            </div>
            
            <div class="takeaway">
                <strong>Takeaway:</strong> Iterative refinement is crucial‚Äîby Epoch 3, iterations improve output by 84.9%, with attention sharpening 50√ó.
            </div>
            
            <div class="speaker-notes" data-duration-30s="Solver iterates 3-8 times, improving output by 85% through refinement." data-duration-2min="The Recursive Solver is RLAN's core innovation. Instead of a single forward pass, it iterates using a ConvGRU. At Epoch 1, iterations actually hurt (-0.6%). By Epoch 3, they provide +84.9% improvement. The model learns to use iterations productively. Attention entropy drops from 3.07 to 0.06‚Äî50 times sharper‚Äîshowing the model learns precise focus." data-duration-5min="Let's watch the solver in action. It starts with a rough prediction‚Äîoften mostly background or a simple pattern. Each iteration refines this: first establishing the overall structure, then filling details, then correcting errors. The ConvGRU maintains hidden state across iterations, building on previous work. Here's the remarkable thing: at Epoch 1, iterations actually made things WORSE‚Äînegative 0.6% improvement. The model hadn't learned to use them yet. By Epoch 3, iterations provide +84.9% improvement over the initial guess. The attention entropy metric is telling: it dropped from 3.07 to 0.06‚Äî50 times sharper. The model learned to focus precisely rather than spreading attention diffusely."></div>
        </section>

        <!-- ========== SECTION 6: TRAINING DASHBOARD ========== -->
        <section id="training" class="story-section" data-section="6">
            <div class="section-header">
                <h2>Training Evidence</h2>
                <p class="section-subtitle">Metrics from 50-epoch training run on single GPU</p>
            </div>
            
            <div class="section-content">
                <div class="training-dashboard">
                    <div class="evidence-callout">
                        <h3>üéØ Key Claims with Evidence (50 Epochs)</h3>
                        <div class="evidence-cards">
                            <div class="evidence-card">
                                <div class="evidence-number">98%</div>
                                <div class="evidence-label">Cell Accuracy</div>
                                <div class="evidence-detail">Up from 66.5% at Epoch 1</div>
                            </div>
                            <div class="evidence-card">
                                <div class="evidence-number">0%‚Üí55%</div>
                                <div class="evidence-label">Exact Match</div>
                                <div class="evidence-detail">Achieved at Epoch 50</div>
                            </div>
                            <div class="evidence-card">
                                <div class="evidence-number">+116%</div>
                                <div class="evidence-label">Solver Improvement</div>
                                <div class="evidence-detail">Iterations now help significantly</div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="charts-grid">
                        <div class="chart-container">
                            <h4>Loss vs Epoch</h4>
                            <div id="loss-chart" class="chart-area"></div>
                        </div>
                        <div class="chart-container">
                            <h4>Accuracy vs Epoch</h4>
                            <div id="accuracy-chart" class="chart-area"></div>
                        </div>
                        <div class="chart-container">
                            <h4>Solver Improvement</h4>
                            <div id="solver-chart" class="chart-area"></div>
                        </div>
                        <div class="chart-container">
                            <h4>Attention Entropy</h4>
                            <div id="entropy-chart" class="chart-area"></div>
                        </div>
                    </div>
                    
                    <div class="training-details collapsible">
                        <button class="collapsible-toggle">
                            <span>Training Configuration Details</span>
                            <span class="toggle-icon">‚ñº</span>
                        </button>
                        <div class="collapsible-content hidden">
                            <table class="config-table">
                                <tr><td>Hardware</td><td>1√ó RTX 3090 (24GB)</td></tr>
                                <tr><td>Batch Size</td><td>32</td></tr>
                                <tr><td>Learning Rate</td><td>1e-4 (warmup + cosine)</td></tr>
                                <tr><td>Solver Iterations</td><td>5 max</td></tr>
                                <tr><td>Total Epochs</td><td>50</td></tr>
                                <tr><td>Training Time</td><td>~8 hours for full 50 epochs</td></tr>
                                <tr><td>Loss Function</td><td>StablemaxCrossEntropy + MSE</td></tr>
                            </table>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="takeaway">
                <strong>Takeaway:</strong> RLAN learns efficiently‚Äîreaching 55% exact match with only 8M parameters. Full 50-epoch training on consumer hardware achieves 98% cell accuracy.
            </div>
            
            <div class="speaker-notes" data-duration-30s="98% cell accuracy, 55% exact match, 116% solver gain‚Äîafter 50 epochs on RTX 3090." data-duration-2min="The training evidence is compelling. Over 50 epochs on a single RTX 3090, we go from 66% to 98% cell accuracy, 0% to 55% exact match. Critically, solver improvement reaches 116%‚Äîthe model learns to leverage iteration. Attention entropy drops 1500√ó, showing extreme precision." data-duration-5min="Let me walk through our training evidence. We trained on a single RTX 3090‚Äîno massive compute cluster needed. Epoch 1 shows 66.5% cell accuracy, near 0% exact match‚Äîand solver iterations actually hurt at -0.6%. By epoch 50, we're at 98% cell accuracy, 55% exact match, and solver improvement is +116%. The model learned to USE the iterations productively. Attention entropy drops from 3.07 to 0.002‚Äî1500√ó sharper. This is the architecture learning to reason precisely, not just memorize."></div>
        </section>

        <!-- ========== SECTION 7: TINY RECURSIVE MODEL COMPARISON ========== -->
        <section id="comparison" class="story-section" data-section="7">
            <div class="section-header">
                <h2>RLAN vs Transformers</h2>
                <p class="section-subtitle">A fundamentally different approach to spatial reasoning</p>
            </div>
            
            <div class="section-content">
                <div class="comparison-container">
                    <div class="architecture-comparison" id="arch-comparison">
                        <!-- Interactive SVG comparison renders here -->
                    </div>
                    
                    <div class="comparison-controls">
                        <button id="highlight-representation" class="compare-btn active">Representation</button>
                        <button id="highlight-attention" class="compare-btn">Attention</button>
                        <button id="highlight-compute" class="compare-btn">Compute</button>
                    </div>
                    
                    <div class="comparison-table-container">
                        <table class="comparison-table">
                            <thead>
                                <tr>
                                    <th>Aspect</th>
                                    <th class="trm-col">Tiny Recursive Model (Transformer)</th>
                                    <th class="rlan-col">RLAN</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr data-aspect="representation">
                                    <td><strong>Grid Representation</strong></td>
                                    <td class="trm-col">Flattened 1D sequence</td>
                                    <td class="rlan-col">Native 2D throughout</td>
                                </tr>
                                <tr data-aspect="representation">
                                    <td><strong>Position Encoding</strong></td>
                                    <td class="trm-col">Absolute (learned/sinusoidal)</td>
                                    <td class="rlan-col">Anchor-relative (DSC+MSRE)</td>
                                </tr>
                                <tr data-aspect="attention">
                                    <td><strong>Attention Type</strong></td>
                                    <td class="trm-col">Global self-attention</td>
                                    <td class="rlan-col">Spatial + learned anchors</td>
                                </tr>
                                <tr data-aspect="attention">
                                    <td><strong>Inductive Bias</strong></td>
                                    <td class="trm-col">Minimal (sequence-agnostic)</td>
                                    <td class="rlan-col">Strong spatial priors</td>
                                </tr>
                                <tr data-aspect="compute">
                                    <td><strong>Refinement</strong></td>
                                    <td class="trm-col">Single forward pass</td>
                                    <td class="rlan-col">Iterative (3-8 steps)</td>
                                </tr>
                                <tr data-aspect="compute">
                                    <td><strong>Training Compute</strong></td>
                                    <td class="trm-col">8√ó A100, 100K epochs</td>
                                    <td class="rlan-col">1√ó RTX 3090, 50 epochs</td>
                                </tr>
                                <tr data-aspect="compute">
                                    <td><strong>Memory Scaling</strong></td>
                                    <td class="trm-col">O(n¬≤) for n cells</td>
                                    <td class="rlan-col">O(n) local operations</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    
                    <div class="failure-modes">
                        <h4>Where Each Struggles</h4>
                        <div class="failure-grid">
                            <div class="failure-card trm">
                                <h5>Tiny Recursive Model (Transformer) Limitations</h5>
                                <ul>
                                    <li>Loses 2D structure in flattening</li>
                                    <li>No explicit counting mechanism</li>
                                    <li>Struggles with novel grid sizes</li>
                                    <li>High compute requirements</li>
                                </ul>
                            </div>
                            <div class="failure-card rlan">
                                <h5>RLAN Limitations</h5>
                                <ul>
                                    <li>Limited sequence/language understanding</li>
                                    <li>Fixed max grid size (30√ó30)</li>
                                    <li>No pre-trained knowledge</li>
                                    <li>Fewer parameters, less capacity</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="takeaway">
                <strong>Takeaway:</strong> RLAN trades generality for spatial specialization‚Äî100√ó less compute, native 2D structure, iterative refinement, but limited language understanding.
            </div>
            
            <div class="speaker-notes" data-duration-30s="RLAN: 2D native, iterative, 100√ó less compute. Tiny Recursive Model: general but loses spatial structure." data-duration-2min="The key difference is representation. Transformers flatten grids to sequences‚Äîa 10√ó10 grid becomes 100 tokens‚Äîlosing explicit 2D structure. RLAN maintains native 2D throughout with Conv operations. Transformers use absolute positions; RLAN uses anchor-relative. Transformers do one forward pass; RLAN iterates. Training cost differs massively: Tiny Recursive Model needs 8√óA100, RLAN runs on 1√óRTX3090." data-duration-5min="Let me contrast the architectures. Transformers are general-purpose: they flatten the grid into a sequence, apply self-attention, decode. This works for many domains but loses explicit 2D structure. A cell's 4-neighbors become arbitrary positions in a sequence. RLAN takes the opposite approach: strong spatial inductive bias. Convolutions preserve 2D structure. DSC+MSRE provide translation invariance through relative encoding. The ConvGRU solver iterates rather than single-pass. The compute difference is striking: transformer papers often cite 8√óA100 for 100K epochs. We use 1√óRTX3090 for ~1K epochs. That's roughly 100√ó less compute. The tradeoff? RLAN is specialized‚Äîit won't understand language or do general reasoning. But for spatial puzzles, that specialization pays off."></div>
        </section>

        <!-- ========== SECTION 7.5: PARAMETER EFFICIENCY ========== -->
        <section id="parameter-efficiency" class="story-section" data-section="7.5">
            <div class="section-header">
                <h2>Parameter Efficiency: RLAN vs GPT</h2>
                <p class="section-subtitle">RLAN achieves 55% on ARC-AGI-1 with ONLY 8M parameters and 50 epochs with one GPU. GPT-4 achieves 5% with 1.76 trillion parameters and many days of training across several GPUs. The right architecture beats brute-force scale.</p>
            </div>
            
            <div class="section-content">
                <div class="parameter-comparison">
                    <!-- Visual Parameter Scale -->
                    <div class="param-scale-viz">
                        <h3>Parameter Count Comparison</h3>
                        <div class="param-bars">
                            <div class="param-bar-item rlan-bar">
                                <div class="bar-label">
                                    <span class="model-name">RLAN</span>
                                    <span class="param-count">~8 Million</span>
                                </div>
                                <div class="bar-container">
                                    <div class="bar-fill rlan" style="width: 0.0005%;"></div>
                                </div>
                                <span class="bar-note">8M params</span>
                            </div>
                            <div class="param-bar-item gpt3-bar">
                                <div class="bar-label">
                                    <span class="model-name">GPT-3.5</span>
                                    <span class="param-count">175 Billion</span>
                                </div>
                                <div class="bar-container">
                                    <div class="bar-fill gpt3" style="width: 10%;"></div>
                                </div>
                                <span class="bar-note">175B params (21,875√ó larger)</span>
                            </div>
                            <div class="param-bar-item gpt4-bar">
                                <div class="bar-label">
                                    <span class="model-name">GPT-4</span>
                                    <span class="param-count">~1.76 Trillion</span>
                                </div>
                                <div class="bar-container">
                                    <div class="bar-fill gpt4" style="width: 100%;"></div>
                                </div>
                                <span class="bar-note">1.76T params (220,000√ó larger)</span>
                            </div>
                        </div>
                    </div>
                    
                    <!-- ARC-AGI Performance -->
                    <div class="performance-comparison">
                        <h3>ARC-AGI Performance vs Parameters</h3>
                        <table class="perf-table">
                            <thead>
                                <tr>
                                    <th>Model</th>
                                    <th>Parameters</th>
                                    <th>ARC-AGI Score</th>
                                    <th>Efficiency</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr class="highlight-row">
                                    <td><strong>RLAN*</strong></td>
                                    <td>~8M</td>
                                    <td><strong>55%</strong></td>
                                    <td>6.9% per million params</td>
                                </tr>
                                <tr class="sota-row">
                                    <td>Tiny Recursive Model (TRM)‚Ä†</td>
                                    <td>~10M</td>
                                    <td>45%</td>
                                    <td>4.5% per million params</td>
                                </tr>
                                <tr>
                                    <td>GPT-3.5</td>
                                    <td>175B</td>
                                    <td>~3%</td>
                                    <td>0.00002% per million params</td>
                                </tr>
                                <tr>
                                    <td>GPT-4</td>
                                    <td>~1.76T</td>
                                    <td>~5%</td>
                                    <td>0.000003% per million params</td>
                                </tr>
                                <tr>
                                    <td>Claude 3</td>
                                    <td>Unknown</td>
                                    <td>~4%</td>
                                    <td>-</td>
                                </tr>
                                <tr>
                                    <td>Human Average</td>
                                    <td>~86B neurons</td>
                                    <td>~85%</td>
                                    <td>-</td>
                                </tr>
                                <tr class="table-footer">
                                    <td colspan="4" style="font-size: 0.85em; text-align: left; padding-top: 12px; border-top: 1px solid rgba(255,255,255,0.2);">
                                        * <strong>New Small Model State-of-the-Art</strong><br>
                                        ‚Ä† Previous Small Model State-of-the-Art
                                    </td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    
                    <!-- RLAN Component Breakdown -->
                    <div class="rlan-breakdown">
                        <h3>RLAN Parameter Breakdown (~8M Total)</h3>
                        <div class="component-bars">
                            <div class="component-item">
                                <div class="component-label">
                                    <span>Context Encoder</span>
                                    <span>~0.8M</span>
                                </div>
                                <div class="component-bar" style="width: 10%;"></div>
                            </div>
                            <div class="component-item">
                                <div class="component-label">
                                    <span>Grid Encoder</span>
                                    <span>~0.8M</span>
                                </div>
                                <div class="component-bar" style="width: 10%;"></div>
                            </div>
                            <div class="component-item">
                                <div class="component-label">
                                    <span>DSC (per step)</span>
                                    <span>~0.8M</span>
                                </div>
                                <div class="component-bar" style="width: 10%;"></div>
                            </div>
                            <div class="component-item">
                                <div class="component-label">
                                    <span>MSRE + LCR</span>
                                    <span>~0.1M</span>
                                </div>
                                <div class="component-bar" style="width: 1.25%;"></div>
                            </div>
                            <div class="component-item">
                                <div class="component-label">
                                    <span>SPH</span>
                                    <span>~0.1M</span>
                                </div>
                                <div class="component-bar" style="width: 1.25%;"></div>
                            </div>
                            <div class="component-item highlight">
                                <div class="component-label">
                                    <span>Recursive Solver</span>
                                    <span>~4.5M</span>
                                </div>
                                <div class="component-bar solver" style="width: 56.25%;"></div>
                            </div>
                        </div>
                        <p class="breakdown-note">The Recursive Solver uses the majority of parameters‚Äîit's where the iterative reasoning happens.</p>
                    </div>
                    
                    <!-- Key Insight -->
                    <div class="insight-box efficiency">
                        <div class="insight-icon">üéØ</div>
                        <div class="insight-content">
                            <strong>The Efficiency Paradox:</strong> GPT-4 has 220,000√ó more parameters than RLAN, yet scores 10√ó <em>worse</em> on ARC-AGI. This demonstrates that abstract reasoning isn't about scale‚Äîit's about the right inductive biases. RLAN's architecture is specifically designed for spatial reasoning, while LLMs are optimized for language patterns.
                        </div>
                    </div>
                    
                    <!-- Why Parameters Don't Equal Intelligence -->
                    <div class="why-section">
                        <h3>Why More Parameters ‚â† Better Reasoning</h3>
                        <div class="reason-cards">
                            <div class="reason-card">
                                <div class="reason-icon">üß©</div>
                                <h4>Inductive Bias Matters</h4>
                                <p>RLAN's architecture encodes spatial priors: 2D convolutions, relative coordinates, iterative refinement. LLMs treat everything as sequences.</p>
                            </div>
                            <div class="reason-card">
                                <div class="reason-icon">üìê</div>
                                <h4>Coordinate Re-projection</h4>
                                <p>RLAN's DSC+MSRE provides translation invariance automatically. LLMs must learn this from millions of examples.</p>
                            </div>
                            <div class="reason-card">
                                <div class="reason-icon">üî¢</div>
                                <h4>Explicit Counting</h4>
                                <p>LCR provides differentiable counting. LLMs must simulate counting through token prediction‚Äîprone to errors.</p>
                            </div>
                            <div class="reason-card">
                                <div class="reason-icon">üîÑ</div>
                                <h4>Iterative Refinement</h4>
                                <p>RLAN iterates to correct errors. LLMs generate tokens left-to-right without backtracking.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="takeaway">
                <strong>Takeaway:</strong> RLAN achieves 55% on ARC-AGI with 8M parameters. GPT-4 achieves 5% with 1.76 trillion. The right architecture beats brute-force scale.
            </div>
            
            <div class="speaker-notes" data-duration-30s="8M params, 55% accuracy vs GPT-4's 1.76T params, 5% accuracy. Architecture beats scale." data-duration-2min="Let's talk numbers. RLAN has ~8 million parameters. GPT-4 has ~1.76 trillion‚Äî220,000 times larger. Yet on ARC-AGI, RLAN scores 55% while GPT-4 scores around 5%. That's 10√ó better performance with 220,000√ó fewer parameters. The key is inductive bias: RLAN is built FOR spatial reasoning. LLMs are built for language." data-duration-5min="This comparison is striking. RLAN: 8 million parameters, 55% on ARC-AGI. GPT-4: 1.76 trillion parameters, approximately 5% on ARC-AGI. That's a 10√ó performance advantage with 220,000√ó fewer parameters. How? Architecture. RLAN encodes the right priors: 2D convolutions preserve spatial structure, DSC+MSRE provide translation invariance, LCR enables counting, the solver iterates to correct errors. LLMs are general-purpose sequence models‚Äîthey must learn spatial reasoning from scratch, competing with their language training. The efficiency metric is remarkable: RLAN gets 6.9% accuracy per million parameters. GPT-4 gets 0.000003%. That's over 2 million times more efficient per parameter for this task. This isn't about dismissing LLMs‚Äîit's about using the right tool for the job."></div>
        </section>

        <!-- ========== SECTION 8: LLM INTEGRATION ========== -->
        <section id="future" class="story-section" data-section="8">
            <div class="section-header">
                <h2>Future: RLAN + LLMs</h2>
                <p class="section-subtitle">Integrating spatial reasoning into large language models</p>
            </div>
            
            <div class="section-content">
                <div class="future-vision">
                    <div class="integration-intro">
                        <h3>The Opportunity</h3>
                        <p>LLMs excel at language and knowledge but struggle with spatial reasoning. RLAN excels at spatial reasoning but lacks language. <strong>Combine them.</strong></p>
                        
                        <div class="intuition-box">
                            <div class="intuition-icon">üí°</div>
                            <div class="intuition-content">
                                <strong>Intuition:</strong> Like adding a calculator to a language model‚ÄîRLAN becomes a "spatial reasoning coprocessor" that the LLM can invoke.
                            </div>
                        </div>
                    </div>
                    
                    <div class="integration-options">
                        <h3>Integration Strategies</h3>
                        
                        <div class="option-cards">
                            <div class="option-card">
                                <h4>üîß Option A: External Tool</h4>
                                <p>RLAN as a callable tool in agent frameworks (like code interpreter)</p>
                                <div class="pros-cons">
                                    <span class="pro">+ No LLM retraining</span>
                                    <span class="pro">+ Modular</span>
                                    <span class="con">- API overhead</span>
                                </div>
                            </div>
                            
                            <div class="option-card">
                                <h4>üîå Option B: Spatial Adapter</h4>
                                <p>RLAN as a frozen adapter layer that processes grid inputs</p>
                                <div class="pros-cons">
                                    <span class="pro">+ Tight integration</span>
                                    <span class="pro">+ Differentiable</span>
                                    <span class="con">- Needs fine-tuning</span>
                                </div>
                            </div>
                            
                            <div class="option-card recommended">
                                <h4>üéØ Option C: Refinement Head</h4>
                                <p>RLAN replaces transformer decoder for spatial outputs</p>
                                <div class="pros-cons">
                                    <span class="pro">+ Best of both</span>
                                    <span class="pro">+ Iterative decode</span>
                                    <span class="con">- Complex training</span>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="api-section">
                        <h3>Hook API Design</h3>
                        <div class="code-block">
                            <pre><code class="language-python"># RLAN as a spatial reasoning tool for LLMs
class RLANSpatialTool:
    """
    Input Contract:
    - grid: List[List[int]] ‚Äî 2D grid with values 0-9
    - context: List[Tuple[grid, grid]] ‚Äî training examples
    
    Output Contract:
    - prediction: List[List[int]] ‚Äî predicted output grid
    - confidence: float ‚Äî model confidence (0-1)
    - iterations: int ‚Äî solver steps used
    """
    
    def __call__(self, test_input: Grid, 
                 context: List[Example]) -> SolverResult:
        # Encode context examples
        ctx = self.encoder(context)
        
        # Run iterative solver
        h = self.initial_state(test_input)
        for t in range(self.max_iter):
            h = self.solver.step(h, ctx)
            if self.should_halt(h):
                break
        
        return SolverResult(
            prediction=self.decode(h),
            confidence=self.get_confidence(h),
            iterations=t + 1
        )</code></pre>
                        </div>
                    </div>
                    
                    <div class="integration-diagram">
                        <h3>Integration Architecture</h3>
                        <div id="llm-integration-diagram">
                            <!-- SVG diagram renders here -->
                        </div>
                    </div>
                    
                    <div class="training-strategy">
                        <h3>Training Strategy</h3>
                        <ol class="strategy-steps">
                            <li><strong>Phase 1:</strong> Pre-train RLAN on ARC (done)</li>
                            <li><strong>Phase 2:</strong> Freeze RLAN, train projection layers</li>
                            <li><strong>Phase 3:</strong> Multi-task: language + spatial jointly</li>
                            <li><strong>Phase 4:</strong> Distillation: RLAN teaches spatial reasoning patterns to LLM</li>
                        </ol>
                    </div>
                </div>
            </div>
            
            <div class="takeaway">
                <strong>Takeaway:</strong> RLAN can augment LLMs as a spatial reasoning coprocessor‚Äîtool-based integration is simplest, adapter-based is most powerful.
            </div>
            
            <div class="speaker-notes" data-duration-30s="RLAN as spatial coprocessor for LLMs: tool, adapter, or refinement head." data-duration-2min="The future is integration. LLMs have knowledge and language; RLAN has spatial reasoning. Three options: (A) RLAN as external tool‚Äîeasy, modular, no retraining. (B) RLAN as frozen adapter‚Äîtighter integration, differentiable. (C) RLAN as refinement head‚Äîreplaces decoder for spatial outputs. The training pipeline: freeze RLAN, train projections, then multi-task, then distill." data-duration-5min="Here's the exciting future direction. LLMs are powerful but struggle with spatial reasoning‚ÄîGPT-4 scores 5% on ARC. RLAN excels at spatial but has no language. The solution is integration. Option A is simplest: RLAN as an external tool, like how agents call code interpreters. The LLM describes the problem, calls RLAN, interprets results. No LLM retraining needed. Option B is tighter: RLAN as a frozen adapter that processes grid inputs before the LLM. Differentiable, but needs fine-tuning. Option C is most ambitious: RLAN replaces the transformer decoder for spatial outputs, providing iterative refinement. The API is clean: pass a grid and context examples, get a prediction with confidence. Training would proceed in phases: pre-train RLAN (done), freeze and train projections, then multi-task on mixed data, finally distill spatial reasoning patterns into the LLM itself."></div>
        </section>

    </main>

    <!-- Footer -->
    <footer id="demo-footer">
        <div class="footer-content">
            <p>RLAN: Recurrent Latent-Action Network</p>
            <p class="footer-note">Interactive Demo | Research Prototype</p>
            <p class="offline-note">üì° Uses CDN for KaTeX & Plotly. For offline use, download libraries locally.</p>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="js/components/GridRenderer.js"></script>
    <script src="js/components/HeatmapOverlay.js"></script>
    <script src="js/components/CoordinateToggle.js"></script>
    <script src="js/components/SolverStepper.js"></script>
    <script src="js/components/ChartPanel.js"></script>
    <script src="js/components/ArchitectureViz.js"></script>
    <script src="js/components/PresenterMode.js"></script>
    <script src="js/app.js"></script>
</body>
</html>
